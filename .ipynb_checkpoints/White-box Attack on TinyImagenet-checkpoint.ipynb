{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# White-box Attack on TinyImagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#sys.path.insert(0, '..')\n",
    "# import torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aoezkan/dl_proj/adversarial-attacks-pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/courses/deep_learning/jupyter/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aoezkan/dl_proj\n"
     ]
    }
   ],
   "source": [
    "%cd adversarial-attacks-pytorch\n",
    "import torchattacks\n",
    "import robustbench\n",
    "from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model, clean_accuracy\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.metrics import topk_acc, real_acc, AverageMeter\n",
    "from utils.parsers import get_training_parser\n",
    "from utils.optimizer import get_optimizer, get_scheduler, OPTIMIZERS_DICT, SCHEDULERS\n",
    "\n",
    "from models.networks import get_model, get_model_bypass\n",
    "from data_utils.data_stats import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1527e811ee50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchattacks import PGD\n",
    "from utils_attack import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 11.71488768 GB\n",
      "Allocated Memory: 0.0 GB\n",
      "Cached Memory: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Assuming a single-GPU setup\n",
    "    device_properties = torch.cuda.get_device_properties(device)\n",
    "    \n",
    "    total_memory = device_properties.total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory / 1e9} GB\")\n",
    "    print(f\"Allocated Memory: {allocated_memory / 1e9} GB\")\n",
    "    print(f\"Cached Memory: {cached_memory / 1e9} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'tinyimagenet'                 # One of cifar10, cifar100, stl10, imagenet or imagenet21\n",
    "architecture = 'B_12-Wi_1024'\n",
    "data_resolution = 64                # Resolution of data as it is stored\n",
    "crop_resolution = 64                # Resolution of fine-tuned model (64 for all models we provide)\n",
    "num_classes = CLASS_DICT[dataset]\n",
    "eval_batch_size = 1024\n",
    "checkpoint = 'B_12-Wi_1024_res_64_in21k_tinyimagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from utils.download import download, default_checkpoints\n",
    "\n",
    "\n",
    "NORMS = {\n",
    "    'layer': nn.LayerNorm,\n",
    "    'batch': nn.BatchNorm1d,\n",
    "    'none': nn.Identity\n",
    "}\n",
    "\n",
    "ACT = {\n",
    "    'gelu': nn.GELU(),\n",
    "    'relu': nn.ReLU()\n",
    "}\n",
    "\n",
    "\n",
    "class StandardMLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, widths):\n",
    "        super(StandardMLP, self).__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.widths = widths\n",
    "        self.linear_in = nn.Linear(self.dim_in, self.widths[0])\n",
    "        self.linear_out = nn.Linear(self.widths[-1], self.dim_out)\n",
    "        self.layers = []\n",
    "        self.layer_norms = []\n",
    "        for i in range(len(self.widths) - 1):\n",
    "            self.layers.append(nn.Linear(self.widths[i], self.widths[i + 1]))\n",
    "            self.layer_norms.append(nn.LayerNorm(widths[i + 1]))\n",
    "\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        self.layernorms = nn.ModuleList(self.layer_norms)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.linear_in(x)\n",
    "        for layer, norm in zip(self.layers, self.layer_norms):\n",
    "            z = norm(z)\n",
    "            z = nn.GELU()(z)\n",
    "            z = layer(z)\n",
    "\n",
    "        out = self.linear_out(z)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleneckMLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, block_dims, norm='layer', checkpoint=None, name=None, bypass=False):\n",
    "        super(BottleneckMLP, self).__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.block_dims = block_dims\n",
    "        self.norm = NORMS[norm]\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "        self.name = name\n",
    "        self.linear_in = nn.Linear(self.dim_in, self.block_dims[0][1])\n",
    "        self.linear_out = nn.Linear(self.block_dims[-1][1], self.dim_out)\n",
    "        blocks = []\n",
    "        layernorms = []\n",
    "\n",
    "        for block_dim in self.block_dims:\n",
    "            wide, thin = block_dim\n",
    "            blocks.append(BottleneckBlock(thin=thin, wide=wide))\n",
    "            layernorms.append(self.norm(thin))\n",
    "\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.layernorms = nn.ModuleList(layernorms)\n",
    "\n",
    "        if self.checkpoint is not None:\n",
    "            if not bypass:\n",
    "                self.load(self.checkpoint)\n",
    "            else:\n",
    "                self.load_bypass(self.checkpoint)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_in(x)\n",
    "\n",
    "        for block, norm in zip(self.blocks, self.layernorms):\n",
    "            x = x + block(norm(x))\n",
    "\n",
    "        out = self.linear_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def load(self, name, checkpoint_path='./checkpoints/'):\n",
    "        #if name == True:\n",
    "            # This simply assumes Imagenet21 pre-trained weights at the latest epoch available, no fine-tuning\n",
    "        #    name = default_checkpoints[self.name]\n",
    "        #elif name in ['cifar10', 'cifar100', 'imagenet']:\n",
    "            # This loads the optimal fine-tuned weights for that dataset\n",
    "        #    name = default_checkpoints[self.name + '_' + name]\n",
    "        #else:\n",
    "            # This assumes a full path, e.g. also specifying which epoch etc\n",
    "        #    name = self.name + '_' + name\n",
    "        name = self.name + '_' + name\n",
    "        name = default_checkpoints[name]\n",
    "        weight_path, config_path = download(name, checkpoint_path)\n",
    "\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "\n",
    "        params = {\n",
    "            k: v\n",
    "            for k, v in torch.load(weight_path, map_location ='cpu').items()\n",
    "        }\n",
    "\n",
    "        # Load pre-trained parameters\n",
    "        print('Load_state output', self.load_state_dict(params, strict=False))\n",
    "\n",
    "    def load_bypass(self, name, checkpoint_path='./checkpoints/'):\n",
    "        #if name == True:\n",
    "            # This simply assumes Imagenet21 pre-trained weights at the latest epoch available, no fine-tuning\n",
    "        #    name = default_checkpoints[self.name]\n",
    "        #elif name in ['cifar10', 'cifar100', 'imagenet']:\n",
    "            # This loads the optimal fine-tuned weights for that dataset\n",
    "        #    name = default_checkpoints[self.name + '_' + name]\n",
    "        #else:\n",
    "            # This assumes a full path, e.g. also specifying which epoch etc\n",
    "        #    name = self.name + '_' + name\n",
    "        weight_path = checkpoint_path + name\n",
    "        \n",
    "        print(\"bypass\")\n",
    "\n",
    "        params = {\n",
    "            k: v\n",
    "            for k, v in torch.load(weight_path, map_location ='cpu').items()\n",
    "        }\n",
    "\n",
    "        # Load pre-trained parameters\n",
    "        print('Load_state output', self.load_state_dict(params, strict=True))\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, thin, wide, act=nn.GELU()):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(thin, wide), act, nn.Linear(wide, thin)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def B_12_Wi_1024(dim_in, dim_out, checkpoint=None, bypass=False):\n",
    "    block_dims = [[4 * 1024, 1024] for _ in range(12)]\n",
    "    return BottleneckMLP(dim_in=dim_in, dim_out=dim_out, norm='layer', block_dims=block_dims, checkpoint=checkpoint,\n",
    "                         name='B_' + str(len(block_dims)) + '-Wi_' + str(block_dims[0][1]) + '_res_' + str(int(np.sqrt(dim_in/3))), bypass=bypass)\n",
    "\n",
    "\n",
    "def B_12_Wi_512(dim_in, dim_out, checkpoint=None, bypass=False):\n",
    "    block_dims = [[4 * 512, 512] for _ in range(12)]\n",
    "    return BottleneckMLP(dim_in=dim_in, dim_out=dim_out, norm='layer', block_dims=block_dims, checkpoint=checkpoint,\n",
    "                         name='B_' + str(len(block_dims)) + '-Wi_' + str(block_dims[0][1]) + '_res_' + str(int(np.sqrt(dim_in/3))), bypass=bypass)\n",
    "\n",
    "\n",
    "def B_6_Wi_1024(dim_in, dim_out, checkpoint=None, bypass=False):\n",
    "    block_dims = [[4 * 1024, 1024] for _ in range(6)]\n",
    "    return BottleneckMLP(dim_in=dim_in, dim_out=dim_out, norm='layer', block_dims=block_dims, checkpoint=checkpoint,\n",
    "                         name='B_' + str(len(block_dims)) + '-Wi_' + str(block_dims[0][1]) + '_res_' + str(int(np.sqrt(dim_in/3))), bypass=bypass)\n",
    "\n",
    "\n",
    "def B_6_Wi_512(dim_in, dim_out, checkpoint=None, bypass=False):\n",
    "    block_dims = [[4 * 512, 512] for _ in range(6)]\n",
    "    return BottleneckMLP(dim_in=dim_in, dim_out=dim_out, norm='layer', block_dims=block_dims, checkpoint=checkpoint,\n",
    "                         name='B_' + str(len(block_dims)) + '-Wi_' + str(block_dims[0][1]) + '_res_' + str(int(np.sqrt(dim_in/3))), bypass=bypass)\n",
    "\n",
    "\n",
    "model_list = {\n",
    "    'B_12-Wi_1024': B_12_Wi_1024,\n",
    "    'B_12-Wi_512': B_12_Wi_512,\n",
    "    'B_6-Wi_1024': B_6_Wi_1024,\n",
    "    'B_6-Wi_512': B_6_Wi_512\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(architecture, checkpoint, resolution, num_classes):\n",
    "    return model_list[architecture](dim_in=resolution**2 * 3, dim_out=num_classes, checkpoint=checkpoint)\n",
    "\n",
    "def get_model_bypass(architecture, checkpoint, resolution, num_classes):\n",
    "    return model_list[architecture](dim_in=resolution**2 * 3, dim_out=num_classes, checkpoint=checkpoint, bypass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Wrapper(nn.Module):\n",
    "    def __init__(self, architecture, resolution, num_classes, checkpoint):\n",
    "        super(MLP_Wrapper, self).__init__()\n",
    "        self.model = get_model_bypass(architecture=architecture, resolution=crop_resolution, num_classes=num_classes, checkpoint=checkpoint)\n",
    "        self.resize = transforms.Resize((resolution, resolution))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resize(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bypass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BottleneckMLP:\n\tMissing key(s) in state_dict: \"linear_in.weight\", \"linear_in.bias\", \"linear_out.weight\", \"linear_out.bias\", \"blocks.0.block.0.weight\", \"blocks.0.block.0.bias\", \"blocks.0.block.2.weight\", \"blocks.0.block.2.bias\", \"blocks.1.block.0.weight\", \"blocks.1.block.0.bias\", \"blocks.1.block.2.weight\", \"blocks.1.block.2.bias\", \"blocks.2.block.0.weight\", \"blocks.2.block.0.bias\", \"blocks.2.block.2.weight\", \"blocks.2.block.2.bias\", \"blocks.3.block.0.weight\", \"blocks.3.block.0.bias\", \"blocks.3.block.2.weight\", \"blocks.3.block.2.bias\", \"blocks.4.block.0.weight\", \"blocks.4.block.0.bias\", \"blocks.4.block.2.weight\", \"blocks.4.block.2.bias\", \"blocks.5.block.0.weight\", \"blocks.5.block.0.bias\", \"blocks.5.block.2.weight\", \"blocks.5.block.2.bias\", \"blocks.6.block.0.weight\", \"blocks.6.block.0.bias\", \"blocks.6.block.2.weight\", \"blocks.6.block.2.bias\", \"blocks.7.block.0.weight\", \"blocks.7.block.0.bias\", \"blocks.7.block.2.weight\", \"blocks.7.block.2.bias\", \"blocks.8.block.0.weight\", \"blocks.8.block.0.bias\", \"blocks.8.block.2.weight\", \"blocks.8.block.2.bias\", \"blocks.9.block.0.weight\", \"blocks.9.block.0.bias\", \"blocks.9.block.2.weight\", \"blocks.9.block.2.bias\", \"blocks.10.block.0.weight\", \"blocks.10.block.0.bias\", \"blocks.10.block.2.weight\", \"blocks.10.block.2.bias\", \"blocks.11.block.0.weight\", \"blocks.11.block.0.bias\", \"blocks.11.block.2.weight\", \"blocks.11.block.2.bias\", \"layernorms.0.weight\", \"layernorms.0.bias\", \"layernorms.1.weight\", \"layernorms.1.bias\", \"layernorms.2.weight\", \"layernorms.2.bias\", \"layernorms.3.weight\", \"layernorms.3.bias\", \"layernorms.4.weight\", \"layernorms.4.bias\", \"layernorms.5.weight\", \"layernorms.5.bias\", \"layernorms.6.weight\", \"layernorms.6.bias\", \"layernorms.7.weight\", \"layernorms.7.bias\", \"layernorms.8.weight\", \"layernorms.8.bias\", \"layernorms.9.weight\", \"layernorms.9.bias\", \"layernorms.10.weight\", \"layernorms.10.bias\", \"layernorms.11.weight\", \"layernorms.11.bias\". \n\tUnexpected key(s) in state_dict: \"module.linear_in.weight\", \"module.linear_in.bias\", \"module.linear_out.weight\", \"module.linear_out.bias\", \"module.blocks.0.block.0.weight\", \"module.blocks.0.block.0.bias\", \"module.blocks.0.block.2.weight\", \"module.blocks.0.block.2.bias\", \"module.blocks.1.block.0.weight\", \"module.blocks.1.block.0.bias\", \"module.blocks.1.block.2.weight\", \"module.blocks.1.block.2.bias\", \"module.blocks.2.block.0.weight\", \"module.blocks.2.block.0.bias\", \"module.blocks.2.block.2.weight\", \"module.blocks.2.block.2.bias\", \"module.blocks.3.block.0.weight\", \"module.blocks.3.block.0.bias\", \"module.blocks.3.block.2.weight\", \"module.blocks.3.block.2.bias\", \"module.blocks.4.block.0.weight\", \"module.blocks.4.block.0.bias\", \"module.blocks.4.block.2.weight\", \"module.blocks.4.block.2.bias\", \"module.blocks.5.block.0.weight\", \"module.blocks.5.block.0.bias\", \"module.blocks.5.block.2.weight\", \"module.blocks.5.block.2.bias\", \"module.blocks.6.block.0.weight\", \"module.blocks.6.block.0.bias\", \"module.blocks.6.block.2.weight\", \"module.blocks.6.block.2.bias\", \"module.blocks.7.block.0.weight\", \"module.blocks.7.block.0.bias\", \"module.blocks.7.block.2.weight\", \"module.blocks.7.block.2.bias\", \"module.blocks.8.block.0.weight\", \"module.blocks.8.block.0.bias\", \"module.blocks.8.block.2.weight\", \"module.blocks.8.block.2.bias\", \"module.blocks.9.block.0.weight\", \"module.blocks.9.block.0.bias\", \"module.blocks.9.block.2.weight\", \"module.blocks.9.block.2.bias\", \"module.blocks.10.block.0.weight\", \"module.blocks.10.block.0.bias\", \"module.blocks.10.block.2.weight\", \"module.blocks.10.block.2.bias\", \"module.blocks.11.block.0.weight\", \"module.blocks.11.block.0.bias\", \"module.blocks.11.block.2.weight\", \"module.blocks.11.block.2.bias\", \"module.layernorms.0.weight\", \"module.layernorms.0.bias\", \"module.layernorms.1.weight\", \"module.layernorms.1.bias\", \"module.layernorms.2.weight\", \"module.layernorms.2.bias\", \"module.layernorms.3.weight\", \"module.layernorms.3.bias\", \"module.layernorms.4.weight\", \"module.layernorms.4.bias\", \"module.layernorms.5.weight\", \"module.layernorms.5.bias\", \"module.layernorms.6.weight\", \"module.layernorms.6.bias\", \"module.layernorms.7.weight\", \"module.layernorms.7.bias\", \"module.layernorms.8.weight\", \"module.layernorms.8.bias\", \"module.layernorms.9.weight\", \"module.layernorms.9.bias\", \"module.layernorms.10.weight\", \"module.layernorms.10.bias\", \"module.layernorms.11.weight\", \"module.layernorms.11.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_mlp \u001b[38;5;241m=\u001b[39m \u001b[43mMLP_Wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model_mlp \u001b[38;5;241m=\u001b[39m model_mlp\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m model_mlp\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[72], line 4\u001b[0m, in \u001b[0;36mMLP_Wrapper.__init__\u001b[0;34m(self, architecture, resolution, num_classes, checkpoint)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, architecture, resolution, num_classes, checkpoint):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(MLP_Wrapper, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_bypass\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marchitecture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrop_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((resolution, resolution))\n",
      "Cell \u001b[0;32mIn[71], line 188\u001b[0m, in \u001b[0;36mget_model_bypass\u001b[0;34m(architecture, checkpoint, resolution, num_classes)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_bypass\u001b[39m(architecture, checkpoint, resolution, num_classes):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbypass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 154\u001b[0m, in \u001b[0;36mB_12_Wi_1024\u001b[0;34m(dim_in, dim_out, checkpoint, bypass)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mB_12_Wi_1024\u001b[39m(dim_in, dim_out, checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bypass\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    153\u001b[0m     block_dims \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m)]\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBottleneckMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mB_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mblock_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wi_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mblock_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_res_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_in\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbypass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbypass\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 78\u001b[0m, in \u001b[0;36mBottleneckMLP.__init__\u001b[0;34m(self, dim_in, dim_out, block_dims, norm, checkpoint, name, bypass)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_bypass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 135\u001b[0m, in \u001b[0;36mBottleneckMLP.load_bypass\u001b[0;34m(self, name, checkpoint_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    130\u001b[0m     k: v\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(weight_path, map_location \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    132\u001b[0m }\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Load pre-trained parameters\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoad_state output\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ex4/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BottleneckMLP:\n\tMissing key(s) in state_dict: \"linear_in.weight\", \"linear_in.bias\", \"linear_out.weight\", \"linear_out.bias\", \"blocks.0.block.0.weight\", \"blocks.0.block.0.bias\", \"blocks.0.block.2.weight\", \"blocks.0.block.2.bias\", \"blocks.1.block.0.weight\", \"blocks.1.block.0.bias\", \"blocks.1.block.2.weight\", \"blocks.1.block.2.bias\", \"blocks.2.block.0.weight\", \"blocks.2.block.0.bias\", \"blocks.2.block.2.weight\", \"blocks.2.block.2.bias\", \"blocks.3.block.0.weight\", \"blocks.3.block.0.bias\", \"blocks.3.block.2.weight\", \"blocks.3.block.2.bias\", \"blocks.4.block.0.weight\", \"blocks.4.block.0.bias\", \"blocks.4.block.2.weight\", \"blocks.4.block.2.bias\", \"blocks.5.block.0.weight\", \"blocks.5.block.0.bias\", \"blocks.5.block.2.weight\", \"blocks.5.block.2.bias\", \"blocks.6.block.0.weight\", \"blocks.6.block.0.bias\", \"blocks.6.block.2.weight\", \"blocks.6.block.2.bias\", \"blocks.7.block.0.weight\", \"blocks.7.block.0.bias\", \"blocks.7.block.2.weight\", \"blocks.7.block.2.bias\", \"blocks.8.block.0.weight\", \"blocks.8.block.0.bias\", \"blocks.8.block.2.weight\", \"blocks.8.block.2.bias\", \"blocks.9.block.0.weight\", \"blocks.9.block.0.bias\", \"blocks.9.block.2.weight\", \"blocks.9.block.2.bias\", \"blocks.10.block.0.weight\", \"blocks.10.block.0.bias\", \"blocks.10.block.2.weight\", \"blocks.10.block.2.bias\", \"blocks.11.block.0.weight\", \"blocks.11.block.0.bias\", \"blocks.11.block.2.weight\", \"blocks.11.block.2.bias\", \"layernorms.0.weight\", \"layernorms.0.bias\", \"layernorms.1.weight\", \"layernorms.1.bias\", \"layernorms.2.weight\", \"layernorms.2.bias\", \"layernorms.3.weight\", \"layernorms.3.bias\", \"layernorms.4.weight\", \"layernorms.4.bias\", \"layernorms.5.weight\", \"layernorms.5.bias\", \"layernorms.6.weight\", \"layernorms.6.bias\", \"layernorms.7.weight\", \"layernorms.7.bias\", \"layernorms.8.weight\", \"layernorms.8.bias\", \"layernorms.9.weight\", \"layernorms.9.bias\", \"layernorms.10.weight\", \"layernorms.10.bias\", \"layernorms.11.weight\", \"layernorms.11.bias\". \n\tUnexpected key(s) in state_dict: \"module.linear_in.weight\", \"module.linear_in.bias\", \"module.linear_out.weight\", \"module.linear_out.bias\", \"module.blocks.0.block.0.weight\", \"module.blocks.0.block.0.bias\", \"module.blocks.0.block.2.weight\", \"module.blocks.0.block.2.bias\", \"module.blocks.1.block.0.weight\", \"module.blocks.1.block.0.bias\", \"module.blocks.1.block.2.weight\", \"module.blocks.1.block.2.bias\", \"module.blocks.2.block.0.weight\", \"module.blocks.2.block.0.bias\", \"module.blocks.2.block.2.weight\", \"module.blocks.2.block.2.bias\", \"module.blocks.3.block.0.weight\", \"module.blocks.3.block.0.bias\", \"module.blocks.3.block.2.weight\", \"module.blocks.3.block.2.bias\", \"module.blocks.4.block.0.weight\", \"module.blocks.4.block.0.bias\", \"module.blocks.4.block.2.weight\", \"module.blocks.4.block.2.bias\", \"module.blocks.5.block.0.weight\", \"module.blocks.5.block.0.bias\", \"module.blocks.5.block.2.weight\", \"module.blocks.5.block.2.bias\", \"module.blocks.6.block.0.weight\", \"module.blocks.6.block.0.bias\", \"module.blocks.6.block.2.weight\", \"module.blocks.6.block.2.bias\", \"module.blocks.7.block.0.weight\", \"module.blocks.7.block.0.bias\", \"module.blocks.7.block.2.weight\", \"module.blocks.7.block.2.bias\", \"module.blocks.8.block.0.weight\", \"module.blocks.8.block.0.bias\", \"module.blocks.8.block.2.weight\", \"module.blocks.8.block.2.bias\", \"module.blocks.9.block.0.weight\", \"module.blocks.9.block.0.bias\", \"module.blocks.9.block.2.weight\", \"module.blocks.9.block.2.bias\", \"module.blocks.10.block.0.weight\", \"module.blocks.10.block.0.bias\", \"module.blocks.10.block.2.weight\", \"module.blocks.10.block.2.bias\", \"module.blocks.11.block.0.weight\", \"module.blocks.11.block.0.bias\", \"module.blocks.11.block.2.weight\", \"module.blocks.11.block.2.bias\", \"module.layernorms.0.weight\", \"module.layernorms.0.bias\", \"module.layernorms.1.weight\", \"module.layernorms.1.bias\", \"module.layernorms.2.weight\", \"module.layernorms.2.bias\", \"module.layernorms.3.weight\", \"module.layernorms.3.bias\", \"module.layernorms.4.weight\", \"module.layernorms.4.bias\", \"module.layernorms.5.weight\", \"module.layernorms.5.bias\", \"module.layernorms.6.weight\", \"module.layernorms.6.bias\", \"module.layernorms.7.weight\", \"module.layernorms.7.bias\", \"module.layernorms.8.weight\", \"module.layernorms.8.bias\", \"module.layernorms.9.weight\", \"module.layernorms.9.bias\", \"module.layernorms.10.weight\", \"module.layernorms.10.bias\", \"module.layernorms.11.weight\", \"module.layernorms.11.bias\". "
     ]
    }
   ],
   "source": [
    "model_mlp = MLP_Wrapper(architecture, crop_resolution, num_classes, checkpoint)\n",
    "\n",
    "model_mlp = model_mlp.to(device)\n",
    "model_mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B_12-Wi_1024_res_64_in21k_tinyimagenet.t7'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B_12-Wi_1024_res_64_in21k_tinyimagenet.t7'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B_12-Wi_1024'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Wrapper(nn.Module):\n",
    "    def __init__(self, model, input_size):\n",
    "        super(Model_Wrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.resize = transforms.Resize(input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resize(x)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Wrapper(nn.Module):\n",
    "    def __init__(self, model, input_size):\n",
    "        super(MLP_Wrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.resize = transforms.Resize(input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights already downloaded\n",
      "Load_state output <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BottleneckMLP(\n",
       "    (linear_in): Linear(in_features=12288, out_features=1024, bias=True)\n",
       "    (linear_out): Linear(in_features=1024, out_features=200, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x BottleneckBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorms): ModuleList(\n",
       "      (0-11): 12 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'cifar10'                 # One of cifar10, cifar100, stl10, imagenet or imagenet21\n",
    "architecture = 'B_12-Wi_1024'\n",
    "data_resolution = 32                # Resolution of data as it is stored\n",
    "crop_resolution = 64                # Resolution of fine-tuned model (64 for all models we provide)\n",
    "num_classes = CLASS_DICT[dataset]\n",
    "eval_batch_size = 1024\n",
    "checkpoint = 'in21k_imagenet'\n",
    "\n",
    "# bypass ing \n",
    "num_classes = 1000\n",
    "input_size = 64\n",
    "model_mlp = get_model(architecture=architecture, resolution=input_size, num_classes=num_classes, checkpoint=checkpoint)\n",
    "model_mlp.linear_out = nn.Linear(1024, 200)\n",
    "\n",
    "model_full_path = os.path.join(\"checkpoints\", \"mlp_b12_wi1024_imagenet_bs128_tinyimagenet.t7\")\n",
    "checkpoint = torch.load(model_full_path, map_location=torch.device(device))\n",
    "\n",
    "model_mlp = torch.nn.DataParallel(model_mlp)\n",
    "model_mlp.load_state_dict(checkpoint['model'])\n",
    "model_mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_Wrapper(\n",
       "  (model): DataParallel(\n",
       "    (module): BottleneckMLP(\n",
       "      (linear_in): Linear(in_features=12288, out_features=1024, bias=True)\n",
       "      (linear_out): Linear(in_features=1024, out_features=200, bias=True)\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x BottleneckBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorms): ModuleList(\n",
       "        (0-11): 12 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (resize): Resize(size=64, interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp_wrap = MLP_Wrapper(model_mlp, input_size=input_size)\n",
    "\n",
    "model_mlp_wrap = model_mlp_wrap.to(device)\n",
    "model_mlp_wrap.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cifar10_models.resnet import *\n",
    "model_resnet = resnet18()\n",
    "model_resnet.conv1 = nn.Conv2d(out_channels=64, in_channels=3, kernel_size=7, bias=False)\n",
    "model_resnet.fc = nn.Linear(out_features=200, in_features=512)\n",
    "\n",
    "model_full_path = os.path.join(\"checkpoints\", \"ResNet18_TinyImageNet.t7\")\n",
    "checkpoint = torch.load(model_full_path, map_location=torch.device(device))\n",
    "\n",
    "model_resnet = torch.nn.DataParallel(model_resnet)\n",
    "model_resnet.load_state_dict(checkpoint['model'])\n",
    "model_resnet = model_resnet.to(device)\n",
    "model_resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on CUDA (GPU)\n"
     ]
    }
   ],
   "source": [
    "if next(model_resnet.parameters()).is_cuda:\n",
    "    print(\"Model is on CUDA (GPU)\")\n",
    "else:\n",
    "    print(\"Model is on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_Wrapper(\n",
       "  (model): DataParallel(\n",
       "    (module): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (resize): Resize(size=64, interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resnet_wrap = Model_Wrapper(model_resnet, input_size=input_size)\n",
    "\n",
    "model_resnet_wrap = model_resnet_wrap.to(device)\n",
    "model_resnet_wrap.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aoezkan\n",
      "/home/aoezkan/pytorch-image-models\n",
      "/home/aoezkan\n",
      "/home/aoezkan/dl_proj\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "%cd pytorch-image-models\n",
    "import timm\n",
    "%cd ..\n",
    "%cd dl_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): VisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (patch_drop): Identity()\n",
       "    (norm_pre): Identity()\n",
       "    (blocks): Sequential(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (8): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (9): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (10): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (11): Block(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (q_norm): Identity()\n",
       "          (k_norm): Identity()\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "    (fc_norm): Identity()\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Linear(in_features=192, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_vit = timm.create_model(\"vit_tiny_patch16_224\", pretrained=True)\n",
    "model_vit.head = nn.Linear(model_vit.head.in_features, 200) # tinyimagnet\n",
    "\n",
    "\n",
    "model_full_path = os.path.join(\"checkpoints\", \"vit_tiny_patch16_224_tinyimagenet.t7\")\n",
    "checkpoint = torch.load(model_full_path, map_location=torch.device(device))\n",
    "\n",
    "model_vit = torch.nn.DataParallel(model_vit)\n",
    "model_vit.load_state_dict(checkpoint['model'])\n",
    "model_vit = model_vit.to(device)\n",
    "model_vit.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model_Wrapper(\n",
       "  (model): DataParallel(\n",
       "    (module): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (patch_drop): Identity()\n",
       "      (norm_pre): Identity()\n",
       "      (blocks): Sequential(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (3): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (q_norm): Identity()\n",
       "            (k_norm): Identity()\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): Identity()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): Identity()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (fc_norm): Identity()\n",
       "      (head_drop): Dropout(p=0.0, inplace=False)\n",
       "      (head): Linear(in_features=192, out_features=200, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (resize): Resize(size=224, interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 224\n",
    "model_vit_wrap = Model_Wrapper(model_vit, input_size=input_size)\n",
    "\n",
    "model_vit_wrap = model_vit_wrap.to(device)\n",
    "model_vit_wrap.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ..\n",
    "# %cd datasets/src/datasets\n",
    "# from datasets import load_dataset, get_dataset_split_names\n",
    "# %cd ../../..\n",
    "# %cd dl_proj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# def collate_fn_train(batch):\n",
    "#     return ((torch.stack([transform_train(Image.fromarray(cv2.cvtColor(np.array(x),cv2.COLOR_GRAY2RGB))) if np.array(x).ndim == 2 else transform_train(x) for x,y in batch])), torch.tensor([x['label'] for x in batch]))\n",
    "      \n",
    "    \n",
    "input_size = 64\n",
    "bs = 100\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "# Set the path to the TinyImageNet dataset\n",
    "data_dir = '../tiny-imagenet-200/'\n",
    "\n",
    "\n",
    "# Define data transforms (you can customize these as needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_size, input_size)),  # Resize images to a consistent size\n",
    "    transforms.ToTensor(),         # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalize the image data\n",
    "])\n",
    "\n",
    "\n",
    "def collate_fn_test(batch):\n",
    "    inputs = []\n",
    "    for x,y in batch:\n",
    "        if np.array(x).ndim == 2: # if grayscale\n",
    "            xx = transform(Image.fromarray(cv2.cvtColor(np.array(x),cv2.COLOR_GRAY2RGB)))\n",
    "        else: # if rgb\n",
    "            xx = transform(x)\n",
    "        inputs.append(xx)\n",
    "        \n",
    "    inputs = torch.stack(inputs)\n",
    "    labels = torch.tensor([y for x,y in batch])\n",
    "    return (inputs, labels)\n",
    "\n",
    "# def collate_fn_test(batch):\n",
    "    # return ((torch.stack([transform(Image.fromarray(cv2.cvtColor(np.array(x['image']),cv2.COLOR_GRAY2RGB))) if np.array(x['image']).ndim == 2 else transform(x['image']) for x in batch])), torch.tensor([x['label'] for x in batch]))\n",
    "      \n",
    "# Create an ImageFolder dataset for training\n",
    "# train_dataset = ImageFolder(os.path.join(data_dir, 'train'), transform=transform)\n",
    "\n",
    "# Create a DataLoader for training data\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# Create an ImageFolder dataset for validation\n",
    "val_dataset = ImageFolder(os.path.join(data_dir, 'val'), transform=transform)\n",
    "\n",
    "# Create a DataLoader for validation data\n",
    "val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=1)\n",
    "\n",
    "# Optionally, create a DataLoader for the test set\n",
    "test_dataset = ImageFolder(os.path.join(data_dir, 'test'), transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_orig = ImageFolder(os.path.join(data_dir, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=64x64>, 0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_orig[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.8550,  1.7694,  1.6324,  ...,  2.0092,  2.0092,  2.0434],\n",
       "          [ 2.0605,  1.9064,  1.5810,  ...,  2.0092,  2.0263,  2.0434],\n",
       "          [ 0.7419,  2.2147,  0.8104,  ...,  2.0263,  2.0434,  2.0434],\n",
       "          ...,\n",
       "          [ 1.4612,  1.4783,  1.4783,  ...,  1.4954,  1.4954,  1.4954],\n",
       "          [ 1.4612,  1.4612,  1.4783,  ...,  1.4954,  1.4783,  1.4783],\n",
       "          [ 1.4612,  1.4612,  1.4612,  ...,  1.4783,  1.4783,  1.4783]],\n",
       " \n",
       "         [[ 2.0609,  2.0784,  2.1485,  ...,  2.1134,  2.1134,  2.1485],\n",
       "          [ 1.9034,  1.8508,  1.6933,  ...,  2.1134,  2.1310,  2.1485],\n",
       "          [-0.2325,  1.3782,  0.1352,  ...,  2.1310,  2.1485,  2.1485],\n",
       "          ...,\n",
       "          [ 1.5532,  1.5707,  1.5707,  ...,  1.5882,  1.5882,  1.5882],\n",
       "          [ 1.5532,  1.5532,  1.5707,  ...,  1.5882,  1.5707,  1.5707],\n",
       "          [ 1.5532,  1.5532,  1.5532,  ...,  1.5707,  1.5707,  1.5707]],\n",
       " \n",
       "         [[ 1.9080,  1.8905,  1.8905,  ...,  2.1171,  2.1171,  2.1520],\n",
       "          [ 1.8731,  1.7685,  1.5768,  ...,  2.1171,  2.1346,  2.1520],\n",
       "          [-0.0092,  1.5245,  0.2522,  ...,  2.1346,  2.1520,  2.1520],\n",
       "          ...,\n",
       "          [ 1.5594,  1.5768,  1.5768,  ...,  1.5942,  1.5942,  1.5942],\n",
       "          [ 1.5594,  1.5594,  1.5768,  ...,  1.5942,  1.5768,  1.5768],\n",
       "          [ 1.5594,  1.5594,  1.5594,  ...,  1.5768,  1.5768,  1.5768]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtestset\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testset' is not defined"
     ]
    }
   ],
   "source": [
    "len(testset['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/courses/deep_learning/jupyter/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0163, -0.3998, -2.1935,  ..., -0.0682,  2.7073, -3.9640],\n",
       "        [ 0.2181, -1.0537, -0.5756,  ..., -1.5039, -0.5743, -1.0677],\n",
       "        [-1.6491, -4.6537,  2.9332,  ..., -3.2640,  4.2584, -4.4190],\n",
       "        ...,\n",
       "        [-1.3811,  4.2886,  0.1140,  ...,  2.8328,  4.5107,  1.2314],\n",
       "        [-6.6187, -2.2151, -4.1474,  ...,  1.8303, -3.5188, -8.0871],\n",
       "        [-1.3849, -3.9730,  0.1594,  ..., -1.2767,  2.7958, -5.8903]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vit_wrap(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BottleneckMLP(\n",
       "    (linear_in): Linear(in_features=12288, out_features=1024, bias=True)\n",
       "    (linear_out): Linear(in_features=1024, out_features=200, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x BottleneckBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorms): ModuleList(\n",
       "      (0-11): 12 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 64, 64])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6965, -0.7137, -0.6794,  ..., -0.1657, -0.1999, -0.1999],\n",
       "         [-0.6965, -0.6965, -0.6623,  ..., -0.1486, -0.1828, -0.1999],\n",
       "         [-0.6794, -0.6794, -0.6452,  ..., -0.1314, -0.1486, -0.1657],\n",
       "         ...,\n",
       "         [-0.5253, -0.5082, -0.4911,  ..., -0.2171, -0.2513, -0.2684],\n",
       "         [-0.5253, -0.5082, -0.4911,  ..., -0.1999, -0.2171, -0.2513],\n",
       "         [-0.5253, -0.5253, -0.4911,  ..., -0.1828, -0.1828, -0.1999]],\n",
       "\n",
       "        [[-0.4951, -0.5126, -0.5301,  ..., -0.0399, -0.0749, -0.0749],\n",
       "         [-0.4951, -0.4951, -0.5126,  ..., -0.0224, -0.0574, -0.0749],\n",
       "         [-0.4776, -0.4776, -0.4951,  ..., -0.0049, -0.0224, -0.0399],\n",
       "         ...,\n",
       "         [-0.4076, -0.3901, -0.3725,  ..., -0.0924, -0.1275, -0.1450],\n",
       "         [-0.4076, -0.3901, -0.3725,  ..., -0.0749, -0.0924, -0.1275],\n",
       "         [-0.4076, -0.4076, -0.3725,  ..., -0.0574, -0.0574, -0.0749]],\n",
       "\n",
       "        [[-0.3753, -0.3927, -0.3578,  ...,  0.1825,  0.1476,  0.1476],\n",
       "         [-0.3753, -0.3753, -0.3404,  ...,  0.1999,  0.1651,  0.1476],\n",
       "         [-0.3578, -0.3578, -0.3230,  ...,  0.2173,  0.1999,  0.1825],\n",
       "         ...,\n",
       "         [-0.2184, -0.2010, -0.1835,  ...,  0.0953,  0.0605,  0.0431],\n",
       "         [-0.2184, -0.2010, -0.1835,  ...,  0.1128,  0.0953,  0.0605],\n",
       "         [-0.2184, -0.2184, -0.1835,  ...,  0.1302,  0.1302,  0.1128]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5364, -2.3948,  1.2522,  ...,  1.5902,  0.7864, -0.3174],\n",
       "        [-2.7206, -6.2073, -1.6139,  ..., -3.2325,  0.0199,  2.4659],\n",
       "        [-1.2435, -1.0192, -0.1396,  ..., -2.3569, -0.9291,  0.3972],\n",
       "        ...,\n",
       "        [-2.1531, -1.4292, -1.3181,  ...,  4.1561,  0.8508,  2.7130],\n",
       "        [-1.8335, -3.8028, -1.1931,  ...,  0.9529, -1.2413, -2.9834],\n",
       "        [-1.3705, -3.7400, -1.1680,  ..., -1.5639, -1.6479, -0.8802]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp_wrap(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12288/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3, 64, 64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.insert(0, '..')\n",
    "import robustbench\n",
    "from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model, clean_accuracy\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data loaded]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/cluster/courses/deep_learning/jupyter/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "100%|| 100/100 [00:42<00:00,  2.36it/s]\n"
     ]
    }
   ],
   "source": [
    "print('[Data loaded]')\n",
    "\n",
    "acc_mlp_list = []\n",
    "acc_cnn_list = []\n",
    "acc_vit_list = []\n",
    "\n",
    "for batch in tqdm(test_loader):\n",
    "    images, labels = batch\n",
    "    acc_mlp = clean_accuracy(model_mlp_wrap, images.to(device), labels.to(device))\n",
    "    acc_mlp_list.append(acc_mlp)\n",
    "\n",
    "    acc_cnn = clean_accuracy(model_resnet, images.to(device), labels.to(device))\n",
    "    acc_cnn_list.append(acc_cnn)\n",
    "\n",
    "    acc_vit = clean_accuracy(model_vit_wrap, images.to(device), labels.to(device))\n",
    "    acc_vit_list.append(acc_vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP loaded]\n",
      "MLP Acc: 0.36 %\n",
      "[CNN loaded]\n",
      "CNN Acc: 0.14 %\n",
      "[ViT loaded]\n",
      "ViT Acc: 0.41 %\n"
     ]
    }
   ],
   "source": [
    "acc_mlp = np.array(acc_mlp_list)\n",
    "acc = acc_mlp.mean()\n",
    "print('[MLP loaded]')\n",
    "print('MLP Acc: %2.2f %%'%(acc*100))\n",
    "\n",
    "acc_cnn = np.array(acc_cnn_list)\n",
    "acc = acc_cnn.mean()\n",
    "print('[CNN loaded]')\n",
    "print('CNN Acc: %2.2f %%'%(acc*100))\n",
    "\n",
    "acc_vit = np.array(acc_vit_list)\n",
    "acc = acc_vit.mean()\n",
    "print('[ViT loaded]')\n",
    "print('ViT Acc: %2.2f %%'%(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_adv_attacks(model1, model2, atk_model1, images, labels, iterations=10, device='cpu'):\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    successful_adv_attacks_model1 = []\n",
    "    successful_adv_attacks_model2 = []\n",
    "\n",
    "    for it in range(iterations):\n",
    "        \n",
    "        adv_images = atk_model1(images, labels)\n",
    "        predictions = get_pred(model1, adv_images, device)\n",
    "        predictions = predictions.cuda()\n",
    "        \n",
    "        seccessful_adv_images = adv_images[predictions != labels]\n",
    "        true_labels_of_succ_adv_images = labels[predictions != labels]\n",
    "        successful_adv_attacks_model1.append(seccessful_adv_images.shape[0])\n",
    "\n",
    "\n",
    "        predictions = get_pred(model2, seccessful_adv_images, device)\n",
    "        predictions = predictions.cuda()\n",
    "        seccessful_adv_images = seccessful_adv_images[predictions != true_labels_of_succ_adv_images]\n",
    "        successful_adv_attacks_model2.append(seccessful_adv_images.shape[0])\n",
    "\n",
    "    return successful_adv_attacks_model1, successful_adv_attacks_model2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_exp = 20\n",
    "\n",
    "atk_mlp = PGD(model_mlp, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "# atk_mlp.set_normalization_used(mean=mean, std=std)\n",
    "\n",
    "\n",
    "l1_list_mlp = []\n",
    "l2_list_mlp = []\n",
    "# 100 batches\n",
    "for i,batch in enumerate(test_loader):\n",
    "    print(\"batch:\", i)\n",
    "    ll1_list = []\n",
    "    ll2_list = []\n",
    "    # N_exp times\n",
    "    for i in range(5):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        l1, l2 = compare_adv_attacks(model_mlp, model_resnet, atk_mlp, images, labels, 1, device)\n",
    "        ll1_list.append(l1)\n",
    "        ll2_list.append(l2)\n",
    "    \n",
    "    l1_list_mlp.append(ll1_list)\n",
    "    l2_list_mlp.append(ll2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(mlp_adv,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on mlp Tested on resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_adv = np.array(l1_list_mlp)\n",
    "mlp_adv = np.squeeze(mlp_adv,2)\n",
    "mlp_adv = mlp_adv.sum(axis=0)\n",
    "\n",
    "mlp_adv_mean = mlp_adv.mean(axis=0)\n",
    "mlp_adv_std = mlp_adv.std(axis=0)\n",
    "print('mlp_adv_mean: ', mlp_adv_mean)\n",
    "print('mlp_adv_std: ', mlp_adv_std)\n",
    "\n",
    "cnn_adv = np.array(l2_list_mlp) \n",
    "cnn_adv = np.squeeze(cnn_adv,2)\n",
    "cnn_adv = cnn_adv.sum(axis=0)\n",
    "\n",
    "cnn_adv_mean = cnn_adv.mean(axis=0)\n",
    "cnn_adv_std = cnn_adv.std(axis=0)\n",
    "print('cnn_adv_mean: ', cnn_adv_mean)\n",
    "print('cnn_adv_std: ', cnn_adv_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ratio: \", (cnn_adv_mean/mlp_adv_mean)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_exp = 20\n",
    "\n",
    "atk_cnn = PGD(model_resnet, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "# atk_cnn.set_normalization_used(mean=mean, std=std)\n",
    "\n",
    "\n",
    "l1_list_cnn = []\n",
    "l2_list_cnn = []\n",
    "# 100 batches\n",
    "for i,batch in enumerate(test_loader):\n",
    "    print(\"batch:\", i)\n",
    "    ll1_list = []\n",
    "    ll2_list = []\n",
    "    # N_exp times\n",
    "    for i in range(5):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        l1, l2 = compare_adv_attacks(model_resnet, model_mlp, atk_cnn, images, labels, 1, device)\n",
    "        ll1_list.append(l1)\n",
    "        ll2_list.append(l2)\n",
    "    \n",
    "    l1_list_cnn.append(ll1_list)\n",
    "    l2_list_cnn.append(ll2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on resnet Tested on mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_adv = np.array(l1_list_cnn)\n",
    "cnn_adv = np.squeeze(cnn_adv,2)\n",
    "cnn_adv = cnn_adv.sum(axis=0)\n",
    "\n",
    "cnn_adv_mean = cnn_adv.mean(axis=0)\n",
    "cnn_adv_std = cnn_adv.std(axis=0)\n",
    "print('cnn_adv_mean: ', cnn_adv_mean)\n",
    "print('cnn_adv_std: ', cnn_adv_std)\n",
    "\n",
    "mlp_adv = np.array(l2_list_cnn) \n",
    "mlp_adv = np.squeeze(mlp_adv,2)\n",
    "mlp_adv = mlp_adv.sum(axis=0)\n",
    "\n",
    "mlp_adv_mean = mlp_adv.mean(axis=0)\n",
    "mlp_adv_std = mlp_adv.std(axis=0)\n",
    "print('mlp_adv_mean: ', mlp_adv_mean)\n",
    "print('mlp_adv_std: ', mlp_adv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"ratio: \", (mlp_adv_mean/cnn_adv_mean)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD: Inside normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((crop_resolution, crop_resolution)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "dataset = datasets.CIFAR10(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transform,\n",
    "                           download=True)\n",
    "\n",
    "test_loader = DataLoader(dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_exp = 20\n",
    "\n",
    "atk_mlp = PGD(model_mlp, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "atk_mlp.set_normalization_used(mean=mean, std=std)\n",
    "\n",
    "\n",
    "l1_list_mlp = []\n",
    "l2_list_mlp = []\n",
    "# 100 batches\n",
    "for i,batch in enumerate(test_loader):\n",
    "    print(\"batch:\", i)\n",
    "    ll1_list = []\n",
    "    ll2_list = []\n",
    "    # N_exp times\n",
    "    for i in range(5):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        l1, l2 = compare_adv_attacks(model_mlp, model_resnet, atk_mlp, images, labels, 1, device)\n",
    "        ll1_list.append(l1)\n",
    "        ll2_list.append(l2)\n",
    "    \n",
    "    l1_list_mlp.append(ll1_list)\n",
    "    l2_list_mlp.append(ll2_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on cnn Tested on mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_adv = np.array(l1_list_mlp)\n",
    "mlp_adv = np.squeeze(mlp_adv,2)\n",
    "mlp_adv = mlp_adv.sum(axis=0)\n",
    "\n",
    "mlp_adv_mean = mlp_adv.mean(axis=0)\n",
    "mlp_adv_std = mlp_adv.std(axis=0)\n",
    "print('mlp_adv_mean: ', mlp_adv_mean)\n",
    "print('mlp_adv_std: ', mlp_adv_std)\n",
    "\n",
    "cnn_adv = np.array(l2_list_mlp) \n",
    "cnn_adv = np.squeeze(cnn_adv,2)\n",
    "cnn_adv = cnn_adv.sum(axis=0)\n",
    "\n",
    "cnn_adv_mean = cnn_adv.mean(axis=0)\n",
    "cnn_adv_std = cnn_adv.std(axis=0)\n",
    "print('cnn_adv_mean: ', cnn_adv_mean)\n",
    "print('cnn_adv_std: ', cnn_adv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ratio: \", (cnn_adv_mean/mlp_adv_mean)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_exp = 20\n",
    "\n",
    "atk_cnn = PGD(model_resnet, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "atk_cnn.set_normalization_used(mean=mean, std=std)\n",
    "\n",
    "\n",
    "l1_list_cnn = []\n",
    "l2_list_cnn = []\n",
    "# 100 batches\n",
    "for i,batch in enumerate(test_loader):\n",
    "    print(\"batch:\", i)\n",
    "    ll1_list = []\n",
    "    ll2_list = []\n",
    "    # N_exp times\n",
    "    for i in range(5):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        l1, l2 = compare_adv_attacks(model_resnet, model_mlp, atk_cnn, images, labels, 1, device)\n",
    "        ll1_list.append(l1)\n",
    "        ll2_list.append(l2)\n",
    "    \n",
    "    l1_list_cnn.append(ll1_list)\n",
    "    l2_list_cnn.append(ll2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on cnn Tested on mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_adv = np.array(l1_list_cnn)\n",
    "cnn_adv = np.squeeze(cnn_adv,2)\n",
    "cnn_adv = cnn_adv.sum(axis=0)\n",
    "\n",
    "cnn_adv_mean = cnn_adv.mean(axis=0)\n",
    "cnn_adv_std = cnn_adv.std(axis=0)\n",
    "print('cnn_adv_mean: ', cnn_adv_mean)\n",
    "print('cnn_adv_std: ', cnn_adv_std)\n",
    "\n",
    "mlp_adv = np.array(l2_list_cnn) \n",
    "mlp_adv = np.squeeze(mlp_adv,2)\n",
    "mlp_adv = mlp_adv.sum(axis=0)\n",
    "\n",
    "mlp_adv_mean = mlp_adv.mean(axis=0)\n",
    "mlp_adv_std = mlp_adv.std(axis=0)\n",
    "print('mlp_adv_mean: ', mlp_adv_mean)\n",
    "print('mlp_adv_std: ', mlp_adv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ratio: \", (mlp_adv_mean/cnn_adv_mean)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model_mlp\n",
    "model2 = model_resnet\n",
    "atk_model1 = atk\n",
    "iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "successful_adv_attacks_model1 = []\n",
    "successful_adv_attacks_model2 = []\n",
    "\n",
    "for it in range(iterations):\n",
    "    print(\"Iteration:\", it)\n",
    "    adv_images = atk_model1(images, labels)\n",
    "    predictions = get_pred(model1, adv_images, device)\n",
    "\n",
    "    seccessful_adv_images = adv_images[predictions.cuda() != labels]\n",
    "    true_labels_of_succ_adv_images = labels[predictions != labels]\n",
    "    successful_adv_attacks_model1.append(seccessful_adv_images.shape[0])\n",
    "\n",
    "    predictions = get_pred(model2, seccessful_adv_images, device)\n",
    "    seccessful_adv_images = seccessful_adv_images[predictions != true_labels_of_succ_adv_images]\n",
    "    successful_adv_attacks_model2.append(seccessful_adv_images.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Assuming a single-GPU setup\n",
    "    device_properties = torch.cuda.get_device_properties(device)\n",
    "    \n",
    "    total_memory = device_properties.total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory / 1e9} GB\")\n",
    "    print(f\"Allocated Memory: {allocated_memory / 1e9} GB\")\n",
    "    print(f\"Cached Memory: {cached_memory / 1e9} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_list = []\n",
    "l2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "atk = PGD(model_mlp, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "atk.set_normalization_used(mean=mean, std=std)\n",
    "l1, l2 = compare_adv_attcks(model_mlp, model_resnet, atk, images, labels, N_exp, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = PGD(model_resnet, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "atk.set_normalization_used(mean=mean, std=std)\n",
    "l11, l22 = compare_adv_attcks(model_resnet, model_mlp, atk, images, labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l11, l22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pre = get_pred(model_resnet, adv_images[idx:idx+1], device)\n",
    "imshow(adv_images[idx:idx+1], title=\"True:%d, Pre:%d\"%(labels[idx], pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pre = get_pred(model, adv_images1[idx:idx+1].flatten(1), device)\n",
    "imshow(adv_images1[idx:idx+1], title=\"True:%d, Pre:%d\"%(labels[idx], pre))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ceb8aea646a0c712ed5db194d127de24ece80f87032283552cbe7de982c3798"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
