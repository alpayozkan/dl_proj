{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# White-box Attack on TinyImagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#sys.path.insert(0, '..')\n",
    "# import torchattacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alpayozkan/opt/anaconda3/envs/ex4/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alpayozkan/eth/dl_proj/adversarial-attacks-pytorch\n",
      "/Users/alpayozkan/eth/dl_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alpayozkan/opt/anaconda3/envs/ex4/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd adversarial-attacks-pytorch\n",
    "import torchattacks\n",
    "import robustbench\n",
    "from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model, clean_accuracy\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.metrics import topk_acc, real_acc, AverageMeter\n",
    "from utils.parsers import get_training_parser\n",
    "from utils.optimizer import get_optimizer, get_scheduler, OPTIMIZERS_DICT, SCHEDULERS\n",
    "\n",
    "from models.networks import get_model, get_model_bypass\n",
    "from data_utils.data_stats import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x110baabd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchattacks import PGD\n",
    "from utils_attack import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Assuming a single-GPU setup\n",
    "    device_properties = torch.cuda.get_device_properties(device)\n",
    "    \n",
    "    total_memory = device_properties.total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory / 1e9} GB\")\n",
    "    print(f\"Allocated Memory: {allocated_memory / 1e9} GB\")\n",
    "    print(f\"Cached Memory: {cached_memory / 1e9} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'tinyimagenet'                 # One of cifar10, cifar100, stl10, imagenet or imagenet21\n",
    "architecture = 'B_12-Wi_1024'\n",
    "data_resolution = 64                # Resolution of data as it is stored\n",
    "crop_resolution = 64                # Resolution of fine-tuned model (64 for all models we provide)\n",
    "num_classes = CLASS_DICT[dataset]\n",
    "eval_batch_size = 1024\n",
    "checkpoint = 'B_12-Wi_1024_res_64_in21k_tinyimagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from utils.download import download, default_checkpoints\n",
    "\n",
    "\n",
    "NORMS = {\n",
    "    'layer': nn.LayerNorm,\n",
    "    'batch': nn.BatchNorm1d,\n",
    "    'none': nn.Identity\n",
    "}\n",
    "\n",
    "ACT = {\n",
    "    'gelu': nn.GELU(),\n",
    "    'relu': nn.ReLU()\n",
    "}\n",
    "\n",
    "\n",
    "class StandardMLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, widths):\n",
    "        super(StandardMLP, self).__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.widths = widths\n",
    "        self.linear_in = nn.Linear(self.dim_in, self.widths[0])\n",
    "        self.linear_out = nn.Linear(self.widths[-1], self.dim_out)\n",
    "        self.layers = []\n",
    "        self.layer_norms = []\n",
    "        for i in range(len(self.widths) - 1):\n",
    "            self.layers.append(nn.Linear(self.widths[i], self.widths[i + 1]))\n",
    "            self.layer_norms.append(nn.LayerNorm(widths[i + 1]))\n",
    "\n",
    "        self.layers = nn.ModuleList(self.layers)\n",
    "        self.layernorms = nn.ModuleList(self.layer_norms)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.linear_in(x)\n",
    "        for layer, norm in zip(self.layers, self.layer_norms):\n",
    "            z = norm(z)\n",
    "            z = nn.GELU()(z)\n",
    "            z = layer(z)\n",
    "\n",
    "        out = self.linear_out(z)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleneckMLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, block_dims, norm='layer', checkpoint=None, name=None, bypass=False):\n",
    "        super(BottleneckMLP, self).__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.block_dims = block_dims\n",
    "        self.norm = NORMS[norm]\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "        self.name = name\n",
    "        self.linear_in = nn.Linear(self.dim_in, self.block_dims[0][1])\n",
    "        self.linear_out = nn.Linear(self.block_dims[-1][1], self.dim_out)\n",
    "        blocks = []\n",
    "        layernorms = []\n",
    "\n",
    "        for block_dim in self.block_dims:\n",
    "            wide, thin = block_dim\n",
    "            blocks.append(BottleneckBlock(thin=thin, wide=wide))\n",
    "            layernorms.append(self.norm(thin))\n",
    "\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.layernorms = nn.ModuleList(layernorms)\n",
    "\n",
    "        if self.checkpoint is not None:\n",
    "            if not bypass:\n",
    "                self.load(self.checkpoint)\n",
    "            else:\n",
    "                self.load_bypass(self.checkpoint)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_in(x)\n",
    "\n",
    "        for block, norm in zip(self.blocks, self.layernorms):\n",
    "            x = x + block(norm(x))\n",
    "\n",
    "        out = self.linear_out(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def load(self, name, checkpoint_path='./checkpoints/'):\n",
    "        #if name == True:\n",
    "            # This simply assumes Imagenet21 pre-trained weights at the latest epoch available, no fine-tuning\n",
    "        #    name = default_checkpoints[self.name]\n",
    "        #elif name in ['cifar10', 'cifar100', 'imagenet']:\n",
    "            # This loads the optimal fine-tuned weights for that dataset\n",
    "        #    name = default_checkpoints[self.name + '_' + name]\n",
    "        #else:\n",
    "            # This assumes a full path, e.g. also specifying which epoch etc\n",
    "        #    name = self.name + '_' + name\n",
    "        name = self.name + '_' + name\n",
    "        name = default_checkpoints[name]\n",
    "        weight_path, config_path = download(name, checkpoint_path)\n",
    "\n",
    "        with open(config_path, 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "\n",
    "        params = {\n",
    "            k: v\n",
    "            for k, v in torch.load(weight_path, map_location ='cpu').items()\n",
    "        }\n",
    "\n",
    "        # Load pre-trained parameters\n",
    "        print('Load_state output', self.load_state_dict(params, strict=False))\n",
    "\n",
    "    def load_bypass(self, name, checkpoint_path='./checkpoints/'):\n",
    "        #if name == True:\n",
    "            # This simply assumes Imagenet21 pre-trained weights at the latest epoch available, no fine-tuning\n",
    "        #    name = default_checkpoints[self.name]\n",
    "        #elif name in ['cifar10', 'cifar100', 'imagenet']:\n",
    "            # This loads the optimal fine-tuned weights for that dataset\n",
    "        #    name = default_checkpoints[self.name + '_' + name]\n",
    "        #else:\n",
    "            # This assumes a full path, e.g. also specifying which epoch etc\n",
    "        #    name = self.name + '_' + name\n",
    "        weight_path = checkpoint_path + name\n",
    "        \n",
    "        print(\"bypass\")\n",
    "\n",
    "        params = {\n",
    "            k: v\n",
    "            for k, v in torch.load(weight_path, map_location ='cpu').items()\n",
    "        }\n",
    "\n",
    "        # Load pre-trained parameters\n",
    "        print('Load_state output', self.load_state_dict(params, strict=True))\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, thin, wide, act=nn.GELU()):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(thin, wide), act, nn.Linear(wide, thin)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def B_12_Wi_1024(dim_in, dim_out, checkpoint=None, bypass=False):\n",
    "    block_dims = [[4 * 1024, 1024] for _ in range(12)]\n",
    "    return BottleneckMLP(dim_in=dim_in, dim_out=dim_out, norm='layer', block_dims=block_dims, checkpoint=checkpoint,\n",
    "                         name='B_' + str(len(block_dims)) + '-Wi_' + str(block_dims[0][1]) + '_res_' + str(int(np.sqrt(dim_in/3))), bypass=bypass)\n",
    "\n",
    "\n",
    "def B_12_Wi_512(dim_in, dim_out, checkpoint=None, bypass=False):\n",
    "    block_dims = [[4 * 512, 512] for _ in range(12)]\n",
    "    return BottleneckMLP(dim_in=dim_in, dim_out=dim_out, norm='layer', block_dims=block_dims, checkpoint=checkpoint,\n",
    "                         name='B_' + str(len(block_dims)) + '-Wi_' + str(block_dims[0][1]) + '_res_' + str(int(np.sqrt(dim_in/3))), bypass=bypass)\n",
    "\n",
    "\n",
    "def B_6_Wi_1024(dim_in, dim_out, checkpoint=None, bypass=False):\n",
    "    block_dims = [[4 * 1024, 1024] for _ in range(6)]\n",
    "    return BottleneckMLP(dim_in=dim_in, dim_out=dim_out, norm='layer', block_dims=block_dims, checkpoint=checkpoint,\n",
    "                         name='B_' + str(len(block_dims)) + '-Wi_' + str(block_dims[0][1]) + '_res_' + str(int(np.sqrt(dim_in/3))), bypass=bypass)\n",
    "\n",
    "\n",
    "def B_6_Wi_512(dim_in, dim_out, checkpoint=None, bypass=False):\n",
    "    block_dims = [[4 * 512, 512] for _ in range(6)]\n",
    "    return BottleneckMLP(dim_in=dim_in, dim_out=dim_out, norm='layer', block_dims=block_dims, checkpoint=checkpoint,\n",
    "                         name='B_' + str(len(block_dims)) + '-Wi_' + str(block_dims[0][1]) + '_res_' + str(int(np.sqrt(dim_in/3))), bypass=bypass)\n",
    "\n",
    "\n",
    "model_list = {\n",
    "    'B_12-Wi_1024': B_12_Wi_1024,\n",
    "    'B_12-Wi_512': B_12_Wi_512,\n",
    "    'B_6-Wi_1024': B_6_Wi_1024,\n",
    "    'B_6-Wi_512': B_6_Wi_512\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(architecture, checkpoint, resolution, num_classes):\n",
    "    return model_list[architecture](dim_in=resolution**2 * 3, dim_out=num_classes, checkpoint=checkpoint)\n",
    "\n",
    "def get_model_bypass(architecture, checkpoint, resolution, num_classes):\n",
    "    return model_list[architecture](dim_in=resolution**2 * 3, dim_out=num_classes, checkpoint=checkpoint, bypass=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Wrapper(nn.Module):\n",
    "    def __init__(self, architecture, resolution, num_classes, checkpoint):\n",
    "        super(MLP_Wrapper, self).__init__()\n",
    "        self.model = get_model_bypass(architecture=architecture, resolution=crop_resolution, num_classes=num_classes, checkpoint=checkpoint)\n",
    "        self.resize = transforms.Resize((resolution, resolution))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resize(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['module.linear_in.weight', 'module.linear_in.bias', 'module.linear_out.weight', 'module.linear_out.bias', 'module.blocks.0.block.0.weight', 'module.blocks.0.block.0.bias', 'module.blocks.0.block.2.weight', 'module.blocks.0.block.2.bias', 'module.blocks.1.block.0.weight', 'module.blocks.1.block.0.bias', 'module.blocks.1.block.2.weight', 'module.blocks.1.block.2.bias', 'module.blocks.2.block.0.weight', 'module.blocks.2.block.0.bias', 'module.blocks.2.block.2.weight', 'module.blocks.2.block.2.bias', 'module.blocks.3.block.0.weight', 'module.blocks.3.block.0.bias', 'module.blocks.3.block.2.weight', 'module.blocks.3.block.2.bias', 'module.blocks.4.block.0.weight', 'module.blocks.4.block.0.bias', 'module.blocks.4.block.2.weight', 'module.blocks.4.block.2.bias', 'module.blocks.5.block.0.weight', 'module.blocks.5.block.0.bias', 'module.blocks.5.block.2.weight', 'module.blocks.5.block.2.bias', 'module.blocks.6.block.0.weight', 'module.blocks.6.block.0.bias', 'module.blocks.6.block.2.weight', 'module.blocks.6.block.2.bias', 'module.blocks.7.block.0.weight', 'module.blocks.7.block.0.bias', 'module.blocks.7.block.2.weight', 'module.blocks.7.block.2.bias', 'module.blocks.8.block.0.weight', 'module.blocks.8.block.0.bias', 'module.blocks.8.block.2.weight', 'module.blocks.8.block.2.bias', 'module.blocks.9.block.0.weight', 'module.blocks.9.block.0.bias', 'module.blocks.9.block.2.weight', 'module.blocks.9.block.2.bias', 'module.blocks.10.block.0.weight', 'module.blocks.10.block.0.bias', 'module.blocks.10.block.2.weight', 'module.blocks.10.block.2.bias', 'module.blocks.11.block.0.weight', 'module.blocks.11.block.0.bias', 'module.blocks.11.block.2.weight', 'module.blocks.11.block.2.bias', 'module.layernorms.0.weight', 'module.layernorms.0.bias', 'module.layernorms.1.weight', 'module.layernorms.1.bias', 'module.layernorms.2.weight', 'module.layernorms.2.bias', 'module.layernorms.3.weight', 'module.layernorms.3.bias', 'module.layernorms.4.weight', 'module.layernorms.4.bias', 'module.layernorms.5.weight', 'module.layernorms.5.bias', 'module.layernorms.6.weight', 'module.layernorms.6.bias', 'module.layernorms.7.weight', 'module.layernorms.7.bias', 'module.layernorms.8.weight', 'module.layernorms.8.bias', 'module.layernorms.9.weight', 'module.layernorms.9.bias', 'module.layernorms.10.weight', 'module.layernorms.10.bias', 'module.layernorms.11.weight', 'module.layernorms.11.bias'])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bypass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BottleneckMLP:\n\tMissing key(s) in state_dict: \"linear_in.weight\", \"linear_in.bias\", \"linear_out.weight\", \"linear_out.bias\", \"blocks.0.block.0.weight\", \"blocks.0.block.0.bias\", \"blocks.0.block.2.weight\", \"blocks.0.block.2.bias\", \"blocks.1.block.0.weight\", \"blocks.1.block.0.bias\", \"blocks.1.block.2.weight\", \"blocks.1.block.2.bias\", \"blocks.2.block.0.weight\", \"blocks.2.block.0.bias\", \"blocks.2.block.2.weight\", \"blocks.2.block.2.bias\", \"blocks.3.block.0.weight\", \"blocks.3.block.0.bias\", \"blocks.3.block.2.weight\", \"blocks.3.block.2.bias\", \"blocks.4.block.0.weight\", \"blocks.4.block.0.bias\", \"blocks.4.block.2.weight\", \"blocks.4.block.2.bias\", \"blocks.5.block.0.weight\", \"blocks.5.block.0.bias\", \"blocks.5.block.2.weight\", \"blocks.5.block.2.bias\", \"blocks.6.block.0.weight\", \"blocks.6.block.0.bias\", \"blocks.6.block.2.weight\", \"blocks.6.block.2.bias\", \"blocks.7.block.0.weight\", \"blocks.7.block.0.bias\", \"blocks.7.block.2.weight\", \"blocks.7.block.2.bias\", \"blocks.8.block.0.weight\", \"blocks.8.block.0.bias\", \"blocks.8.block.2.weight\", \"blocks.8.block.2.bias\", \"blocks.9.block.0.weight\", \"blocks.9.block.0.bias\", \"blocks.9.block.2.weight\", \"blocks.9.block.2.bias\", \"blocks.10.block.0.weight\", \"blocks.10.block.0.bias\", \"blocks.10.block.2.weight\", \"blocks.10.block.2.bias\", \"blocks.11.block.0.weight\", \"blocks.11.block.0.bias\", \"blocks.11.block.2.weight\", \"blocks.11.block.2.bias\", \"layernorms.0.weight\", \"layernorms.0.bias\", \"layernorms.1.weight\", \"layernorms.1.bias\", \"layernorms.2.weight\", \"layernorms.2.bias\", \"layernorms.3.weight\", \"layernorms.3.bias\", \"layernorms.4.weight\", \"layernorms.4.bias\", \"layernorms.5.weight\", \"layernorms.5.bias\", \"layernorms.6.weight\", \"layernorms.6.bias\", \"layernorms.7.weight\", \"layernorms.7.bias\", \"layernorms.8.weight\", \"layernorms.8.bias\", \"layernorms.9.weight\", \"layernorms.9.bias\", \"layernorms.10.weight\", \"layernorms.10.bias\", \"layernorms.11.weight\", \"layernorms.11.bias\". \n\tUnexpected key(s) in state_dict: \"module.linear_in.weight\", \"module.linear_in.bias\", \"module.linear_out.weight\", \"module.linear_out.bias\", \"module.blocks.0.block.0.weight\", \"module.blocks.0.block.0.bias\", \"module.blocks.0.block.2.weight\", \"module.blocks.0.block.2.bias\", \"module.blocks.1.block.0.weight\", \"module.blocks.1.block.0.bias\", \"module.blocks.1.block.2.weight\", \"module.blocks.1.block.2.bias\", \"module.blocks.2.block.0.weight\", \"module.blocks.2.block.0.bias\", \"module.blocks.2.block.2.weight\", \"module.blocks.2.block.2.bias\", \"module.blocks.3.block.0.weight\", \"module.blocks.3.block.0.bias\", \"module.blocks.3.block.2.weight\", \"module.blocks.3.block.2.bias\", \"module.blocks.4.block.0.weight\", \"module.blocks.4.block.0.bias\", \"module.blocks.4.block.2.weight\", \"module.blocks.4.block.2.bias\", \"module.blocks.5.block.0.weight\", \"module.blocks.5.block.0.bias\", \"module.blocks.5.block.2.weight\", \"module.blocks.5.block.2.bias\", \"module.blocks.6.block.0.weight\", \"module.blocks.6.block.0.bias\", \"module.blocks.6.block.2.weight\", \"module.blocks.6.block.2.bias\", \"module.blocks.7.block.0.weight\", \"module.blocks.7.block.0.bias\", \"module.blocks.7.block.2.weight\", \"module.blocks.7.block.2.bias\", \"module.blocks.8.block.0.weight\", \"module.blocks.8.block.0.bias\", \"module.blocks.8.block.2.weight\", \"module.blocks.8.block.2.bias\", \"module.blocks.9.block.0.weight\", \"module.blocks.9.block.0.bias\", \"module.blocks.9.block.2.weight\", \"module.blocks.9.block.2.bias\", \"module.blocks.10.block.0.weight\", \"module.blocks.10.block.0.bias\", \"module.blocks.10.block.2.weight\", \"module.blocks.10.block.2.bias\", \"module.blocks.11.block.0.weight\", \"module.blocks.11.block.0.bias\", \"module.blocks.11.block.2.weight\", \"module.blocks.11.block.2.bias\", \"module.layernorms.0.weight\", \"module.layernorms.0.bias\", \"module.layernorms.1.weight\", \"module.layernorms.1.bias\", \"module.layernorms.2.weight\", \"module.layernorms.2.bias\", \"module.layernorms.3.weight\", \"module.layernorms.3.bias\", \"module.layernorms.4.weight\", \"module.layernorms.4.bias\", \"module.layernorms.5.weight\", \"module.layernorms.5.bias\", \"module.layernorms.6.weight\", \"module.layernorms.6.bias\", \"module.layernorms.7.weight\", \"module.layernorms.7.bias\", \"module.layernorms.8.weight\", \"module.layernorms.8.bias\", \"module.layernorms.9.weight\", \"module.layernorms.9.bias\", \"module.layernorms.10.weight\", \"module.layernorms.10.bias\", \"module.layernorms.11.weight\", \"module.layernorms.11.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_mlp \u001b[38;5;241m=\u001b[39m \u001b[43mMLP_Wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model_mlp \u001b[38;5;241m=\u001b[39m model_mlp\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m model_mlp\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[72], line 4\u001b[0m, in \u001b[0;36mMLP_Wrapper.__init__\u001b[0;34m(self, architecture, resolution, num_classes, checkpoint)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, architecture, resolution, num_classes, checkpoint):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m(MLP_Wrapper, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_bypass\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marchitecture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcrop_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((resolution, resolution))\n",
      "Cell \u001b[0;32mIn[71], line 188\u001b[0m, in \u001b[0;36mget_model_bypass\u001b[0;34m(architecture, checkpoint, resolution, num_classes)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_bypass\u001b[39m(architecture, checkpoint, resolution, num_classes):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbypass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 154\u001b[0m, in \u001b[0;36mB_12_Wi_1024\u001b[0;34m(dim_in, dim_out, checkpoint, bypass)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mB_12_Wi_1024\u001b[39m(dim_in, dim_out, checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, bypass\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    153\u001b[0m     block_dims \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m12\u001b[39m)]\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBottleneckMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mB_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mblock_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-Wi_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mblock_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_res_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_in\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbypass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbypass\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 78\u001b[0m, in \u001b[0;36mBottleneckMLP.__init__\u001b[0;34m(self, dim_in, dim_out, block_dims, norm, checkpoint, name, bypass)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_bypass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 135\u001b[0m, in \u001b[0;36mBottleneckMLP.load_bypass\u001b[0;34m(self, name, checkpoint_path)\u001b[0m\n\u001b[1;32m    129\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    130\u001b[0m     k: v\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(weight_path, map_location \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    132\u001b[0m }\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Load pre-trained parameters\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoad_state output\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ex4/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BottleneckMLP:\n\tMissing key(s) in state_dict: \"linear_in.weight\", \"linear_in.bias\", \"linear_out.weight\", \"linear_out.bias\", \"blocks.0.block.0.weight\", \"blocks.0.block.0.bias\", \"blocks.0.block.2.weight\", \"blocks.0.block.2.bias\", \"blocks.1.block.0.weight\", \"blocks.1.block.0.bias\", \"blocks.1.block.2.weight\", \"blocks.1.block.2.bias\", \"blocks.2.block.0.weight\", \"blocks.2.block.0.bias\", \"blocks.2.block.2.weight\", \"blocks.2.block.2.bias\", \"blocks.3.block.0.weight\", \"blocks.3.block.0.bias\", \"blocks.3.block.2.weight\", \"blocks.3.block.2.bias\", \"blocks.4.block.0.weight\", \"blocks.4.block.0.bias\", \"blocks.4.block.2.weight\", \"blocks.4.block.2.bias\", \"blocks.5.block.0.weight\", \"blocks.5.block.0.bias\", \"blocks.5.block.2.weight\", \"blocks.5.block.2.bias\", \"blocks.6.block.0.weight\", \"blocks.6.block.0.bias\", \"blocks.6.block.2.weight\", \"blocks.6.block.2.bias\", \"blocks.7.block.0.weight\", \"blocks.7.block.0.bias\", \"blocks.7.block.2.weight\", \"blocks.7.block.2.bias\", \"blocks.8.block.0.weight\", \"blocks.8.block.0.bias\", \"blocks.8.block.2.weight\", \"blocks.8.block.2.bias\", \"blocks.9.block.0.weight\", \"blocks.9.block.0.bias\", \"blocks.9.block.2.weight\", \"blocks.9.block.2.bias\", \"blocks.10.block.0.weight\", \"blocks.10.block.0.bias\", \"blocks.10.block.2.weight\", \"blocks.10.block.2.bias\", \"blocks.11.block.0.weight\", \"blocks.11.block.0.bias\", \"blocks.11.block.2.weight\", \"blocks.11.block.2.bias\", \"layernorms.0.weight\", \"layernorms.0.bias\", \"layernorms.1.weight\", \"layernorms.1.bias\", \"layernorms.2.weight\", \"layernorms.2.bias\", \"layernorms.3.weight\", \"layernorms.3.bias\", \"layernorms.4.weight\", \"layernorms.4.bias\", \"layernorms.5.weight\", \"layernorms.5.bias\", \"layernorms.6.weight\", \"layernorms.6.bias\", \"layernorms.7.weight\", \"layernorms.7.bias\", \"layernorms.8.weight\", \"layernorms.8.bias\", \"layernorms.9.weight\", \"layernorms.9.bias\", \"layernorms.10.weight\", \"layernorms.10.bias\", \"layernorms.11.weight\", \"layernorms.11.bias\". \n\tUnexpected key(s) in state_dict: \"module.linear_in.weight\", \"module.linear_in.bias\", \"module.linear_out.weight\", \"module.linear_out.bias\", \"module.blocks.0.block.0.weight\", \"module.blocks.0.block.0.bias\", \"module.blocks.0.block.2.weight\", \"module.blocks.0.block.2.bias\", \"module.blocks.1.block.0.weight\", \"module.blocks.1.block.0.bias\", \"module.blocks.1.block.2.weight\", \"module.blocks.1.block.2.bias\", \"module.blocks.2.block.0.weight\", \"module.blocks.2.block.0.bias\", \"module.blocks.2.block.2.weight\", \"module.blocks.2.block.2.bias\", \"module.blocks.3.block.0.weight\", \"module.blocks.3.block.0.bias\", \"module.blocks.3.block.2.weight\", \"module.blocks.3.block.2.bias\", \"module.blocks.4.block.0.weight\", \"module.blocks.4.block.0.bias\", \"module.blocks.4.block.2.weight\", \"module.blocks.4.block.2.bias\", \"module.blocks.5.block.0.weight\", \"module.blocks.5.block.0.bias\", \"module.blocks.5.block.2.weight\", \"module.blocks.5.block.2.bias\", \"module.blocks.6.block.0.weight\", \"module.blocks.6.block.0.bias\", \"module.blocks.6.block.2.weight\", \"module.blocks.6.block.2.bias\", \"module.blocks.7.block.0.weight\", \"module.blocks.7.block.0.bias\", \"module.blocks.7.block.2.weight\", \"module.blocks.7.block.2.bias\", \"module.blocks.8.block.0.weight\", \"module.blocks.8.block.0.bias\", \"module.blocks.8.block.2.weight\", \"module.blocks.8.block.2.bias\", \"module.blocks.9.block.0.weight\", \"module.blocks.9.block.0.bias\", \"module.blocks.9.block.2.weight\", \"module.blocks.9.block.2.bias\", \"module.blocks.10.block.0.weight\", \"module.blocks.10.block.0.bias\", \"module.blocks.10.block.2.weight\", \"module.blocks.10.block.2.bias\", \"module.blocks.11.block.0.weight\", \"module.blocks.11.block.0.bias\", \"module.blocks.11.block.2.weight\", \"module.blocks.11.block.2.bias\", \"module.layernorms.0.weight\", \"module.layernorms.0.bias\", \"module.layernorms.1.weight\", \"module.layernorms.1.bias\", \"module.layernorms.2.weight\", \"module.layernorms.2.bias\", \"module.layernorms.3.weight\", \"module.layernorms.3.bias\", \"module.layernorms.4.weight\", \"module.layernorms.4.bias\", \"module.layernorms.5.weight\", \"module.layernorms.5.bias\", \"module.layernorms.6.weight\", \"module.layernorms.6.bias\", \"module.layernorms.7.weight\", \"module.layernorms.7.bias\", \"module.layernorms.8.weight\", \"module.layernorms.8.bias\", \"module.layernorms.9.weight\", \"module.layernorms.9.bias\", \"module.layernorms.10.weight\", \"module.layernorms.10.bias\", \"module.layernorms.11.weight\", \"module.layernorms.11.bias\". "
     ]
    }
   ],
   "source": [
    "model_mlp = MLP_Wrapper(architecture, crop_resolution, num_classes, checkpoint)\n",
    "\n",
    "model_mlp = model_mlp.to(device)\n",
    "model_mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B_12-Wi_1024_res_64_in21k_tinyimagenet.t7'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B_12-Wi_1024_res_64_in21k_tinyimagenet.t7'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B_12-Wi_1024'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('module.linear_in.weight',\n",
       "              tensor([[ 2.2785e-01,  2.9513e-01,  3.1012e-03,  ..., -1.0594e-01,\n",
       "                        2.2147e-02,  8.5203e-02],\n",
       "                      [-2.3599e-01, -5.2422e-02, -4.1217e-02,  ..., -1.1177e-01,\n",
       "                       -4.4740e-01, -4.3109e-01],\n",
       "                      [-4.5495e-01, -4.4787e-01, -2.9203e-01,  ...,  1.4033e-01,\n",
       "                        2.5422e-01,  3.6087e-01],\n",
       "                      ...,\n",
       "                      [ 1.7607e-02, -3.5418e-01,  1.9371e-02,  ..., -2.6691e-01,\n",
       "                       -5.2200e-01, -2.3111e-01],\n",
       "                      [ 3.8978e-01,  1.9745e-01,  4.5564e-01,  ...,  2.0694e-01,\n",
       "                        4.6923e-01,  3.8922e-01],\n",
       "                      [-1.0741e-02,  2.3873e-05,  6.7596e-02,  ..., -3.2074e-02,\n",
       "                       -1.3536e-02, -9.1601e-02]])),\n",
       "             ('module.linear_in.bias',\n",
       "              tensor([ 1.0549,  0.4441, -0.6998,  ...,  0.7105,  1.1405, -0.3989])),\n",
       "             ('module.linear_out.weight',\n",
       "              tensor([[ 1.5310e-02,  4.5584e-03, -2.1657e-02,  ...,  1.4852e-02,\n",
       "                        1.7602e-02,  3.7569e-02],\n",
       "                      [-2.4131e-02,  4.3384e-02,  2.5383e-04,  ..., -1.0433e-02,\n",
       "                        6.3600e-02,  8.5815e-05],\n",
       "                      [ 5.1396e-03, -3.5944e-02,  1.3738e-02,  ...,  2.9717e-02,\n",
       "                        4.5733e-02, -1.3358e-02],\n",
       "                      ...,\n",
       "                      [ 1.7091e-02,  6.5796e-03,  2.2119e-02,  ..., -1.7120e-02,\n",
       "                        1.0750e-02, -1.7398e-03],\n",
       "                      [-2.2876e-02,  3.6375e-02, -8.4044e-03,  ...,  9.7869e-03,\n",
       "                        6.1939e-03,  5.1413e-02],\n",
       "                      [ 3.1976e-02,  2.7844e-02, -9.1802e-03,  ..., -2.3205e-02,\n",
       "                       -6.5282e-03, -9.3661e-03]])),\n",
       "             ('module.linear_out.bias',\n",
       "              tensor([-2.2233e-02,  4.7180e-03,  2.0199e-02, -1.7607e-02, -1.9371e-02,\n",
       "                      -1.3742e-02,  1.6159e-03,  1.9111e-02, -2.3729e-03, -1.7115e-02,\n",
       "                      -7.9429e-03,  4.0819e-04, -1.9304e-02,  1.2566e-02,  1.7746e-02,\n",
       "                      -7.6389e-03,  3.0585e-02,  2.8400e-03, -3.1321e-02, -3.2486e-02,\n",
       "                       2.7580e-03, -1.7624e-02, -1.4319e-02,  8.4333e-03,  4.3465e-02,\n",
       "                      -2.5389e-02, -2.2733e-03, -2.1876e-03, -4.5559e-03,  8.5563e-03,\n",
       "                       2.9591e-02,  5.8031e-03,  1.5234e-02, -3.1861e-03,  2.2088e-03,\n",
       "                      -8.0798e-03, -5.1366e-03, -1.3168e-02,  2.8559e-02,  1.9263e-02,\n",
       "                       1.6346e-02,  1.7107e-03, -6.0764e-03, -1.3113e-02,  8.1480e-03,\n",
       "                       1.5877e-02,  2.5368e-02, -2.3369e-02, -1.3015e-02, -3.2638e-02,\n",
       "                       2.1722e-02,  2.1401e-02,  8.7723e-04, -8.2391e-04,  2.2586e-02,\n",
       "                       2.0539e-03, -3.3381e-02,  2.6979e-02,  8.2059e-03,  1.9999e-02,\n",
       "                      -3.3741e-02,  5.5850e-02, -1.3285e-02,  3.6531e-02, -3.6205e-02,\n",
       "                      -3.1369e-04,  3.6244e-02, -3.4717e-02, -4.2435e-02, -5.4166e-03,\n",
       "                      -3.3767e-03, -1.5626e-02, -2.9058e-02, -1.5808e-02, -2.1920e-02,\n",
       "                       3.3340e-03,  2.6105e-03,  1.7108e-02,  3.1480e-03, -1.8088e-02,\n",
       "                      -2.3159e-02,  1.0950e-02, -1.1674e-02, -3.1949e-02, -1.2243e-02,\n",
       "                      -2.6470e-02, -1.1853e-03,  1.4320e-02, -2.2628e-02,  1.4019e-02,\n",
       "                       1.1958e-02,  7.9806e-03,  2.4497e-03, -9.1303e-03, -8.8073e-03,\n",
       "                      -1.3406e-02,  2.5348e-02,  5.6135e-03,  2.1293e-02,  1.0775e-03,\n",
       "                      -3.5777e-02,  2.5035e-02,  1.5730e-02, -2.5823e-02, -8.9496e-04,\n",
       "                       5.1155e-03, -6.0336e-03, -3.9254e-03,  1.7721e-03, -1.0459e-03,\n",
       "                      -1.2330e-02,  1.7300e-02,  1.4628e-02,  4.6622e-03,  8.3632e-03,\n",
       "                      -1.5671e-02, -2.1697e-02,  1.3259e-02,  2.2073e-02, -3.1746e-02,\n",
       "                      -2.2435e-02, -1.9867e-02, -1.4146e-02, -8.6846e-03, -1.0443e-02,\n",
       "                       2.3150e-02, -2.5896e-02,  2.3963e-02,  2.9827e-02, -1.4332e-02,\n",
       "                      -1.7334e-02,  2.2446e-02, -1.0597e-02,  1.7358e-02, -1.0068e-02,\n",
       "                       2.7703e-05,  6.1129e-03, -1.7153e-02, -1.1217e-02, -2.7542e-02,\n",
       "                       9.7831e-04,  2.6937e-02,  1.4052e-02, -2.3865e-02,  2.3427e-02,\n",
       "                      -2.0420e-02, -3.5422e-02,  1.0900e-02, -1.9206e-03,  3.7560e-02,\n",
       "                      -1.6947e-02,  3.2381e-02, -4.3584e-03, -2.3638e-02, -3.3368e-02,\n",
       "                       2.1793e-02,  1.4140e-02,  9.9275e-04, -1.0406e-02,  2.2203e-02,\n",
       "                       1.4997e-02, -8.5755e-03,  3.1612e-03, -1.4520e-02, -1.1920e-02,\n",
       "                       4.8340e-03,  3.7956e-02, -3.3018e-02,  2.5051e-02,  3.1364e-03,\n",
       "                       2.4817e-03, -6.5288e-03, -2.5929e-02, -5.5585e-04, -3.3942e-02,\n",
       "                       3.1199e-02, -3.5678e-02, -1.5957e-02, -1.9025e-02, -3.4255e-02,\n",
       "                      -1.0196e-02,  3.3542e-03, -2.5839e-02, -4.0004e-02,  1.6131e-02,\n",
       "                      -1.5694e-02,  5.2702e-02,  3.4145e-02, -3.3064e-02,  8.5454e-03,\n",
       "                      -7.3373e-03, -8.0496e-03, -1.6434e-02, -3.8912e-03,  1.3750e-03,\n",
       "                       1.6093e-02,  2.6178e-02, -2.2148e-02,  1.0205e-02, -1.3512e-02])),\n",
       "             ('module.blocks.0.block.0.weight',\n",
       "              tensor([[ 2.4233e-02,  3.9381e-02,  8.9794e-02,  ..., -3.8441e-02,\n",
       "                        1.1294e-01,  1.5066e-01],\n",
       "                      [ 4.0995e-02, -1.8943e-01, -7.3669e-02,  ...,  9.9511e-02,\n",
       "                        7.6077e-02, -1.1076e-01],\n",
       "                      [-7.1269e-02, -1.1163e-01, -2.0323e-01,  ...,  1.0534e-02,\n",
       "                        1.8004e-01,  1.3235e-02],\n",
       "                      ...,\n",
       "                      [ 5.5758e-02,  5.5843e-02,  3.1572e-02,  ...,  8.3500e-02,\n",
       "                        4.6117e-02,  1.1401e-01],\n",
       "                      [-4.4994e+00, -1.0062e+00, -1.3662e+00,  ...,  2.5330e+00,\n",
       "                       -1.2363e+01,  1.5670e+00],\n",
       "                      [-1.5782e-01,  5.3632e-02, -1.4314e-01,  ..., -4.4379e-03,\n",
       "                       -1.5784e-01,  7.1314e-02]])),\n",
       "             ('module.blocks.0.block.0.bias',\n",
       "              tensor([-9.6826e-02, -3.3783e-01, -5.6529e-01,  ..., -5.5730e-03,\n",
       "                      -1.7037e+01, -1.3502e-01])),\n",
       "             ('module.blocks.0.block.2.weight',\n",
       "              tensor([[ 7.6787e-02, -4.2139e-02,  2.3695e-02,  ..., -1.5165e-01,\n",
       "                        1.3121e+01,  2.0227e-01],\n",
       "                      [-4.8563e-02,  1.3263e-01, -1.7624e-01,  ..., -2.1772e-02,\n",
       "                        1.2372e+01, -1.0019e-01],\n",
       "                      [-2.4725e-01, -8.5116e-02, -2.1579e-01,  ...,  1.0976e-01,\n",
       "                       -1.2576e+00,  2.3959e-02],\n",
       "                      ...,\n",
       "                      [ 6.3970e-02,  1.7621e-02, -3.6636e-02,  ..., -1.4575e-02,\n",
       "                       -1.5215e+01,  7.4490e-03],\n",
       "                      [-1.3190e-01,  5.0964e-02,  8.1512e-02,  ...,  1.8872e-02,\n",
       "                       -4.2990e-01,  2.6202e-01],\n",
       "                      [-1.8552e-01,  5.9508e-02,  1.2069e-01,  ...,  6.4129e-03,\n",
       "                        6.4521e+00, -4.7724e-02]])),\n",
       "             ('module.blocks.0.block.2.bias',\n",
       "              tensor([ 0.2231,  0.1248, -0.0066,  ...,  0.0826,  0.0796, -0.0618])),\n",
       "             ('module.blocks.1.block.0.weight',\n",
       "              tensor([[ 1.5805e-01,  1.2721e-01,  2.4747e-01,  ..., -6.1117e-02,\n",
       "                       -1.8927e-02,  2.2562e-02],\n",
       "                      [-4.5649e+00, -1.0756e+01, -5.1982e+00,  ..., -1.0296e+01,\n",
       "                        1.2773e+01,  5.2307e+00],\n",
       "                      [-3.2303e-01,  2.6477e-02, -4.7228e-01,  ..., -2.8497e-03,\n",
       "                        1.2643e-01, -1.6862e-01],\n",
       "                      ...,\n",
       "                      [-1.0330e-01, -1.9956e-01, -1.4170e-02,  ..., -1.8513e-01,\n",
       "                       -1.8957e-01,  5.7182e-02],\n",
       "                      [-4.4263e-03, -1.8471e-02, -2.3810e-01,  ..., -1.0053e-01,\n",
       "                       -7.8911e-03,  3.2270e-01],\n",
       "                      [-7.8252e-02,  8.8742e-03,  1.1597e-02,  ...,  1.6769e-01,\n",
       "                       -3.4546e-01, -1.8271e-01]])),\n",
       "             ('module.blocks.1.block.0.bias',\n",
       "              tensor([ -0.0350, -15.9927,   0.0362,  ...,  -0.0368,  -0.2115,  -0.0833])),\n",
       "             ('module.blocks.1.block.2.weight',\n",
       "              tensor([[-2.2174e-01,  9.1575e+00,  1.3423e-01,  ...,  1.5716e-01,\n",
       "                        1.5954e-01,  1.0721e-01],\n",
       "                      [-4.5691e-02, -9.9904e+00, -5.5552e-02,  ...,  3.3170e-02,\n",
       "                       -4.1374e-02, -1.3305e-01],\n",
       "                      [-2.5440e-02, -3.7968e+00,  1.7103e-01,  ...,  3.7157e-02,\n",
       "                        3.7250e-01,  8.9967e-02],\n",
       "                      ...,\n",
       "                      [ 5.3361e-02, -7.8918e+00, -1.0511e-01,  ...,  3.1066e-01,\n",
       "                       -4.9761e-02, -1.9717e-02],\n",
       "                      [-7.0248e-02,  8.5312e+00,  9.7379e-03,  ...,  4.3080e-01,\n",
       "                        1.6155e-01,  2.2058e-01],\n",
       "                      [ 3.4357e-02, -1.1590e+01, -1.2784e-02,  ...,  1.0441e-01,\n",
       "                       -1.2664e-01,  5.7258e-02]])),\n",
       "             ('module.blocks.1.block.2.bias',\n",
       "              tensor([-0.0628, -0.0023,  0.1839,  ..., -0.1149, -0.1912,  0.1500])),\n",
       "             ('module.blocks.2.block.0.weight',\n",
       "              tensor([[-0.0401, -0.0463, -0.1020,  ...,  0.1745,  0.1535,  0.1457],\n",
       "                      [-0.0418,  0.3385,  0.0483,  ...,  0.0028, -0.0623, -0.1773],\n",
       "                      [-0.1768, -0.1277, -0.0892,  ..., -0.1049, -0.2704,  0.0574],\n",
       "                      ...,\n",
       "                      [ 0.0715, -0.0463,  0.3422,  ..., -0.0761,  0.0980, -0.3528],\n",
       "                      [-0.0050,  0.1508,  0.1946,  ...,  0.0653, -0.0704, -0.2119],\n",
       "                      [-0.1257,  0.0987, -0.1020,  ..., -0.3311, -0.0923,  0.0875]])),\n",
       "             ('module.blocks.2.block.0.bias',\n",
       "              tensor([-0.1835,  0.0248, -0.0889,  ..., -0.1258, -0.1065, -0.4614])),\n",
       "             ('module.blocks.2.block.2.weight',\n",
       "              tensor([[-0.0922,  0.2174, -0.1528,  ..., -0.2286, -0.1478,  0.2187],\n",
       "                      [ 0.2245, -0.0996, -0.4313,  ...,  0.1023, -0.1211,  0.4575],\n",
       "                      [ 0.1641, -0.0997,  0.0130,  ..., -0.2104, -0.3031, -0.0386],\n",
       "                      ...,\n",
       "                      [-0.3106,  0.0566, -0.0617,  ...,  0.0859, -0.2550, -0.4428],\n",
       "                      [-0.1854,  0.2102, -0.4331,  ..., -0.1370, -0.1197, -0.2925],\n",
       "                      [-0.3098, -0.0040,  0.1260,  ...,  0.4876,  0.1900, -0.1347]])),\n",
       "             ('module.blocks.2.block.2.bias',\n",
       "              tensor([ 0.0541,  0.2760, -0.0468,  ..., -0.1068, -0.0760,  0.0442])),\n",
       "             ('module.blocks.3.block.0.weight',\n",
       "              tensor([[-0.0149, -0.1589,  0.0622,  ..., -0.0914,  0.0261,  0.0955],\n",
       "                      [-0.0862,  0.1411, -0.1872,  ..., -0.1349, -0.1502, -0.0568],\n",
       "                      [ 0.0331,  0.2189,  0.3690,  ...,  0.0812,  0.0892,  0.0453],\n",
       "                      ...,\n",
       "                      [ 0.4206, -0.2235,  0.3928,  ..., -0.2843, -0.0754,  0.3149],\n",
       "                      [-0.0903,  0.0102, -0.1087,  ...,  0.0047, -0.0815,  0.1806],\n",
       "                      [ 0.0007, -0.2562, -0.3984,  ...,  0.0725, -0.0173,  0.0329]])),\n",
       "             ('module.blocks.3.block.0.bias',\n",
       "              tensor([-0.0444, -0.1055, -0.0827,  ...,  0.0153, -0.0643, -0.0482])),\n",
       "             ('module.blocks.3.block.2.weight',\n",
       "              tensor([[ 0.0025, -0.0875, -0.0262,  ..., -0.4101, -0.0492, -0.1684],\n",
       "                      [ 0.0424,  0.0136,  0.1662,  ...,  0.0643,  0.1026,  0.2275],\n",
       "                      [-0.0728, -0.0414, -0.1354,  ..., -0.0811, -0.1234,  0.2161],\n",
       "                      ...,\n",
       "                      [-0.3781,  0.0894,  0.1606,  ...,  0.1624, -0.1099,  0.2508],\n",
       "                      [-0.0792,  0.0778, -0.2900,  ..., -0.0656, -0.1363, -0.0520],\n",
       "                      [ 0.0254,  0.1960,  0.1438,  ..., -0.1249, -0.2361,  0.0886]])),\n",
       "             ('module.blocks.3.block.2.bias',\n",
       "              tensor([-0.0948, -0.1231,  0.1398,  ..., -0.0208, -0.0976,  0.2700])),\n",
       "             ('module.blocks.4.block.0.weight',\n",
       "              tensor([[-0.1049, -0.1305, -0.2855,  ..., -0.0542, -0.1488, -0.2262],\n",
       "                      [ 0.1183, -0.0056,  0.0053,  ...,  0.2778,  0.1699,  0.4299],\n",
       "                      [ 0.0561,  0.4976, -0.1264,  ..., -0.2325, -0.5417, -0.0406],\n",
       "                      ...,\n",
       "                      [-0.1623,  0.3207,  0.0237,  ...,  0.2013, -0.1697,  0.0517],\n",
       "                      [-0.1941,  0.2952, -0.1786,  ..., -0.2742,  0.0324,  0.2371],\n",
       "                      [-0.1229, -0.0871, -0.0038,  ..., -0.1492,  0.1354, -0.0576]])),\n",
       "             ('module.blocks.4.block.0.bias',\n",
       "              tensor([-0.1310, -0.0301, -0.0366,  ..., -0.2420, -0.0698,  0.1149])),\n",
       "             ('module.blocks.4.block.2.weight',\n",
       "              tensor([[-2.5173e-01,  1.5915e-01,  8.3085e-02,  ...,  5.6842e-02,\n",
       "                        2.4823e-01,  7.4795e-02],\n",
       "                      [-7.5000e-02, -7.6058e-02, -2.0464e-01,  ..., -1.2761e-01,\n",
       "                       -1.2017e-01,  3.3189e-02],\n",
       "                      [ 1.1113e-01,  8.2308e-02,  1.4933e-01,  ..., -4.1238e-02,\n",
       "                        1.0231e-01,  1.0150e-01],\n",
       "                      ...,\n",
       "                      [ 1.5821e-01, -6.9496e-02,  2.3649e-02,  ...,  3.3394e-01,\n",
       "                       -2.2377e-01,  1.0285e-01],\n",
       "                      [-2.6443e-02, -2.9853e-01,  9.3541e-02,  ..., -2.3186e-01,\n",
       "                        1.4372e-01, -2.2209e-01],\n",
       "                      [ 3.9844e-02, -2.9001e-01,  1.2870e-01,  ..., -9.0114e-02,\n",
       "                       -1.6892e-04,  1.0710e-01]])),\n",
       "             ('module.blocks.4.block.2.bias',\n",
       "              tensor([ 0.0576,  0.0713,  0.1490,  ..., -0.1096, -0.0439,  0.0933])),\n",
       "             ('module.blocks.5.block.0.weight',\n",
       "              tensor([[ 0.0433, -0.0102,  0.1682,  ...,  0.1891, -0.2939, -0.1417],\n",
       "                      [-0.0361,  0.0929, -0.0713,  ...,  0.3671,  0.1447, -0.0469],\n",
       "                      [ 0.0158,  0.0086,  0.3590,  ..., -0.0688,  0.2010, -0.0566],\n",
       "                      ...,\n",
       "                      [-0.1042,  0.0292, -0.0903,  ..., -0.1709,  0.0548, -0.0045],\n",
       "                      [ 0.3617,  0.1008,  0.2209,  ...,  0.1903, -0.1174,  0.0040],\n",
       "                      [ 0.3022, -0.0134, -0.2615,  ..., -0.1266, -0.2347,  0.1489]])),\n",
       "             ('module.blocks.5.block.0.bias',\n",
       "              tensor([-0.0612, -0.1720,  0.2523,  ...,  0.1828,  0.0049, -0.3810])),\n",
       "             ('module.blocks.5.block.2.weight',\n",
       "              tensor([[-0.1457, -0.0735,  0.0560,  ...,  0.0342, -0.2236,  0.0496],\n",
       "                      [ 0.2704, -0.1691, -0.1121,  ..., -0.0444, -0.0889,  0.0113],\n",
       "                      [-0.1391, -0.2265, -0.2293,  ...,  0.1369, -0.2971, -0.1346],\n",
       "                      ...,\n",
       "                      [-0.0325,  0.0118, -0.0784,  ...,  0.1964, -0.1227, -0.0334],\n",
       "                      [ 0.2843, -0.0619, -0.5277,  ..., -0.1750,  0.0763,  0.0587],\n",
       "                      [ 0.3160,  0.3021, -0.0541,  ..., -0.0644,  0.0752, -0.0881]])),\n",
       "             ('module.blocks.5.block.2.bias',\n",
       "              tensor([ 0.1512, -0.0741,  0.0560,  ..., -0.2392, -0.0169, -0.1515])),\n",
       "             ('module.blocks.6.block.0.weight',\n",
       "              tensor([[ 0.2789,  0.2492,  0.0623,  ...,  0.1983,  0.5675,  0.1864],\n",
       "                      [ 0.0419, -0.1490, -0.1362,  ...,  0.1188, -0.2285, -0.0376],\n",
       "                      [ 0.0506,  0.2802, -0.0599,  ...,  0.0633, -0.0984, -0.2974],\n",
       "                      ...,\n",
       "                      [-0.1737, -0.1943, -0.3088,  ..., -0.1296,  0.1287, -0.0828],\n",
       "                      [-0.2191,  0.4009, -0.2403,  ..., -0.2184, -0.2566, -0.1832],\n",
       "                      [-0.0420,  0.1972, -0.0740,  ...,  0.1082, -0.2329,  0.2128]])),\n",
       "             ('module.blocks.6.block.0.bias',\n",
       "              tensor([-0.2050, -0.0776, -0.0608,  ..., -0.0661, -0.1689, -0.0353])),\n",
       "             ('module.blocks.6.block.2.weight',\n",
       "              tensor([[ 0.4031, -0.0870, -0.0769,  ...,  0.2368, -0.0148,  0.1094],\n",
       "                      [ 0.0380, -0.4580, -0.0924,  ..., -0.0738, -0.0709, -0.2268],\n",
       "                      [ 0.0866,  0.0899,  0.2173,  ...,  0.0745,  0.1248,  0.0151],\n",
       "                      ...,\n",
       "                      [ 0.1775,  0.0206, -0.0639,  ...,  0.0946,  0.0309, -0.3480],\n",
       "                      [ 0.1583,  0.4017, -0.1850,  ..., -0.0006,  0.0633,  0.1378],\n",
       "                      [ 0.2393, -0.0512, -0.0965,  ...,  0.1852, -0.0739, -0.0981]])),\n",
       "             ('module.blocks.6.block.2.bias',\n",
       "              tensor([-0.1463, -0.0004, -0.0069,  ..., -0.1829,  0.1829,  0.1241])),\n",
       "             ('module.blocks.7.block.0.weight',\n",
       "              tensor([[ 0.0488,  0.2113, -0.2496,  ...,  0.1457, -0.1329,  0.2354],\n",
       "                      [ 0.1208, -0.1310, -0.0271,  ..., -0.2565, -0.2476, -0.1141],\n",
       "                      [ 0.0563, -0.0310, -0.1081,  ...,  0.1122, -0.0297, -0.3134],\n",
       "                      ...,\n",
       "                      [-0.1405, -0.1149, -0.1877,  ..., -0.0072, -0.0641,  0.0024],\n",
       "                      [ 0.2687,  0.0417,  0.1017,  ..., -0.1451,  0.2974,  0.0036],\n",
       "                      [ 0.1827, -0.1825,  0.0824,  ...,  0.0724, -0.1122,  0.0502]])),\n",
       "             ('module.blocks.7.block.0.bias',\n",
       "              tensor([-0.0607, -0.3520, -0.3511,  ..., -0.1786, -0.1535, -0.0389])),\n",
       "             ('module.blocks.7.block.2.weight',\n",
       "              tensor([[ 0.0372,  0.2423,  0.3225,  ..., -0.0641,  0.0167, -0.1562],\n",
       "                      [-0.4528, -0.0681,  0.1955,  ...,  0.1321, -0.1680,  0.0229],\n",
       "                      [ 0.2625,  0.3051,  0.0210,  ...,  0.2845, -0.0358,  0.2679],\n",
       "                      ...,\n",
       "                      [-0.0628,  0.0697,  0.0912,  ...,  0.0693, -0.3235, -0.1028],\n",
       "                      [ 0.0371,  0.0135,  0.3312,  ...,  0.2222,  0.1602, -0.1310],\n",
       "                      [-0.3948, -0.0661,  0.1588,  ..., -0.0406,  0.0803, -0.0985]])),\n",
       "             ('module.blocks.7.block.2.bias',\n",
       "              tensor([-0.2331, -0.0685,  0.0787,  ..., -0.1488, -0.1162, -0.0125])),\n",
       "             ('module.blocks.8.block.0.weight',\n",
       "              tensor([[-0.1329,  0.1128,  0.2585,  ...,  0.3509,  0.3478, -0.2969],\n",
       "                      [-0.4062, -0.2123, -0.2801,  ..., -0.2698,  0.1264, -0.0102],\n",
       "                      [ 0.0026,  0.0012, -0.0355,  ...,  0.0800,  0.0818,  0.3289],\n",
       "                      ...,\n",
       "                      [ 0.0878, -0.2870, -0.2456,  ...,  0.3155, -0.2911,  0.1864],\n",
       "                      [-0.0044, -0.0910, -0.2187,  ..., -0.1939,  0.2040,  0.0505],\n",
       "                      [-0.0659, -0.0759, -0.1948,  ...,  0.0603, -0.2600, -0.1939]])),\n",
       "             ('module.blocks.8.block.0.bias',\n",
       "              tensor([-0.1521, -0.5604, -0.0167,  ..., -0.0442, -0.1698, -0.1569])),\n",
       "             ('module.blocks.8.block.2.weight',\n",
       "              tensor([[ 0.4239, -0.5219,  0.0835,  ..., -0.1390, -0.1786,  0.2612],\n",
       "                      [ 0.0882,  0.3534,  0.2334,  ...,  0.1583, -0.1816, -0.0227],\n",
       "                      [-0.1304, -0.1955,  0.2400,  ...,  0.0651, -0.0893,  0.0124],\n",
       "                      ...,\n",
       "                      [-0.1140, -0.4710,  0.1111,  ..., -0.3501,  0.0589, -0.2795],\n",
       "                      [-0.0631,  0.0969, -0.1033,  ...,  0.2768,  0.1021, -0.4507],\n",
       "                      [ 0.2577,  0.6523, -0.1738,  ..., -0.1902,  0.1051, -0.2363]])),\n",
       "             ('module.blocks.8.block.2.bias',\n",
       "              tensor([ 0.5705, -0.1700,  0.0718,  ...,  0.2826,  0.0277, -0.2006])),\n",
       "             ('module.blocks.9.block.0.weight',\n",
       "              tensor([[-5.4681e-01,  1.7233e-01, -2.3662e-01,  ...,  7.6808e-02,\n",
       "                       -1.3844e-01, -2.1736e-01],\n",
       "                      [-2.3053e-01,  2.2240e-01,  1.7622e-01,  ..., -2.0501e-01,\n",
       "                       -2.0087e-01, -1.1129e-05],\n",
       "                      [-2.4110e-01,  1.1053e-01, -3.0185e-01,  ..., -3.1189e-03,\n",
       "                       -3.1544e-01, -2.5933e-01],\n",
       "                      ...,\n",
       "                      [-3.6606e-01,  3.0700e-01,  7.9647e-02,  ..., -5.7641e-02,\n",
       "                       -4.2583e-02, -2.0506e-01],\n",
       "                      [ 3.0240e-02, -2.7110e-01,  4.9311e-03,  ...,  8.5930e-03,\n",
       "                        3.9594e-01, -1.6317e-02],\n",
       "                      [-8.8926e-02,  2.9905e-01, -1.2803e-01,  ..., -8.8166e-02,\n",
       "                       -8.0361e-02, -2.2465e-02]])),\n",
       "             ('module.blocks.9.block.0.bias',\n",
       "              tensor([-0.1209, -0.1316, -0.1077,  ..., -0.1076, -0.0539, -0.1108])),\n",
       "             ('module.blocks.9.block.2.weight',\n",
       "              tensor([[-0.1729,  0.2911,  0.0305,  ...,  0.1897,  0.0710, -0.2637],\n",
       "                      [ 0.1845,  0.0675, -0.2104,  ..., -0.2019,  0.2738,  0.0769],\n",
       "                      [ 0.0414, -0.2912,  0.2329,  ..., -0.1551,  0.1065, -0.2333],\n",
       "                      ...,\n",
       "                      [-0.1732,  0.0322,  0.1817,  ...,  0.0914,  0.0264, -0.0509],\n",
       "                      [ 0.0146,  0.0947,  0.3782,  ..., -0.1059, -0.2776, -0.3160],\n",
       "                      [-0.0592,  0.2106,  0.2623,  ...,  0.2718,  0.3792, -0.2180]])),\n",
       "             ('module.blocks.9.block.2.bias',\n",
       "              tensor([ 0.3617, -0.1231,  0.0051,  ...,  0.4921, -0.3412, -0.4197])),\n",
       "             ('module.blocks.10.block.0.weight',\n",
       "              tensor([[-0.1259, -0.2608,  0.0318,  ...,  0.1310, -0.2084,  0.3234],\n",
       "                      [-0.0597, -0.2142,  0.2485,  ..., -0.1086, -0.1105, -0.1124],\n",
       "                      [-0.2514, -0.0705, -0.0705,  ..., -0.0954, -0.3634, -0.0395],\n",
       "                      ...,\n",
       "                      [-0.0895,  0.0642,  0.0132,  ..., -0.1944, -0.0343,  0.0579],\n",
       "                      [-0.1464, -0.1184,  0.5011,  ..., -0.0938, -0.0509,  0.2741],\n",
       "                      [-0.4393, -0.1006, -0.1088,  ...,  0.0229,  0.2068,  0.1248]])),\n",
       "             ('module.blocks.10.block.0.bias',\n",
       "              tensor([-0.1646, -0.1190, -0.1184,  ..., -0.1149, -0.0544, -0.1384])),\n",
       "             ('module.blocks.10.block.2.weight',\n",
       "              tensor([[-0.0707, -0.2212,  0.0088,  ...,  0.0052, -0.0012,  0.4452],\n",
       "                      [-0.1535, -0.0938,  0.2192,  ...,  0.2728, -0.0158,  0.3178],\n",
       "                      [ 0.0045,  0.1536,  0.0682,  ..., -0.3985, -0.1143, -0.1686],\n",
       "                      ...,\n",
       "                      [-0.1344,  0.0571, -0.4595,  ..., -0.0474, -0.1225,  0.2309],\n",
       "                      [-0.0859, -0.2490,  0.0189,  ...,  0.2357, -0.1344, -0.0667],\n",
       "                      [-0.0505, -0.1259,  0.1689,  ..., -0.1016, -0.1179, -0.2958]])),\n",
       "             ('module.blocks.10.block.2.bias',\n",
       "              tensor([-0.1881, -0.1191, -0.1331,  ...,  0.2857,  0.2426, -0.0808])),\n",
       "             ('module.blocks.11.block.0.weight',\n",
       "              tensor([[ 0.3498,  0.3685, -0.0771,  ..., -0.1898,  0.2694,  0.0108],\n",
       "                      [ 0.1173,  0.0034,  0.2116,  ...,  0.0707, -0.1339, -0.0334],\n",
       "                      [ 0.3335,  0.3951,  0.0677,  ..., -0.2024,  0.1051,  0.1829],\n",
       "                      ...,\n",
       "                      [ 0.2038, -0.0376, -0.0153,  ...,  0.0005, -0.1825, -0.0800],\n",
       "                      [ 0.1062, -0.2900, -0.3776,  ...,  0.1351,  0.1971, -0.0456],\n",
       "                      [ 0.0198,  0.2003,  0.1895,  ...,  0.1400,  0.0239,  0.1355]])),\n",
       "             ('module.blocks.11.block.0.bias',\n",
       "              tensor([-0.3503, -0.0335, -1.0195,  ..., -0.0114,  0.0095, -0.0758])),\n",
       "             ('module.blocks.11.block.2.weight',\n",
       "              tensor([[ 3.9136e-01,  2.0257e-01, -1.2120e-01,  ..., -1.6126e-01,\n",
       "                       -6.9070e-02,  6.2569e-03],\n",
       "                      [ 1.3570e-04, -1.5888e-01, -3.4307e-01,  ..., -1.8210e-01,\n",
       "                        9.7553e-02,  8.3246e-02],\n",
       "                      [ 8.8605e-02,  4.7405e-02, -8.0760e-02,  ...,  1.4525e-01,\n",
       "                        1.6544e-01, -1.2190e-01],\n",
       "                      ...,\n",
       "                      [ 3.1106e-02,  1.0694e-01,  8.5001e-04,  ...,  1.2493e-01,\n",
       "                       -6.5309e-02, -9.3870e-02],\n",
       "                      [ 5.1937e-02,  1.3546e-01,  4.2549e-02,  ...,  1.7703e-01,\n",
       "                       -5.5576e-02, -2.0335e-01],\n",
       "                      [ 1.1598e-01, -1.1772e-01, -2.3812e-01,  ..., -1.6924e-01,\n",
       "                       -8.2993e-02, -3.5532e-02]])),\n",
       "             ('module.blocks.11.block.2.bias',\n",
       "              tensor([ 0.0025,  0.1024,  0.1288,  ...,  0.1316,  0.1059, -0.0692])),\n",
       "             ('module.layernorms.0.weight',\n",
       "              tensor([0.3462, 0.2206, 0.2124,  ..., 0.2532, 0.1970, 0.1850])),\n",
       "             ('module.layernorms.0.bias',\n",
       "              tensor([-0.0758, -0.0272,  0.0182,  ..., -0.0301,  0.0094,  0.0490])),\n",
       "             ('module.layernorms.1.weight',\n",
       "              tensor([0.4872, 0.5277, 0.4854,  ..., 0.5822, 0.7441, 0.5769])),\n",
       "             ('module.layernorms.1.bias',\n",
       "              tensor([-0.4408, -0.3070,  0.2620,  ..., -0.1967, -0.5229,  0.1637])),\n",
       "             ('module.layernorms.2.weight',\n",
       "              tensor([0.8940, 0.8926, 0.8699,  ..., 0.9129, 0.9220, 0.8844])),\n",
       "             ('module.layernorms.2.bias',\n",
       "              tensor([-0.0990, -0.0691,  0.2487,  ...,  0.1285, -0.1782,  0.0909])),\n",
       "             ('module.layernorms.3.weight',\n",
       "              tensor([0.8122, 0.7996, 0.8641,  ..., 0.8786, 0.8450, 0.8884])),\n",
       "             ('module.layernorms.3.bias',\n",
       "              tensor([-0.2907, -0.3874,  0.2169,  ..., -0.1600, -0.1326,  0.0014])),\n",
       "             ('module.layernorms.4.weight',\n",
       "              tensor([0.7250, 0.8961, 0.8889,  ..., 0.8510, 0.8953, 0.9786])),\n",
       "             ('module.layernorms.4.bias',\n",
       "              tensor([-0.2527, -0.2054, -0.1907,  ..., -0.0262, -0.2111, -0.1264])),\n",
       "             ('module.layernorms.5.weight',\n",
       "              tensor([0.6619, 0.9248, 0.9464,  ..., 0.9331, 0.9304, 0.9398])),\n",
       "             ('module.layernorms.5.bias',\n",
       "              tensor([-0.2507, -0.1164, -0.2353,  ..., -0.0676, -0.1726, -0.0612])),\n",
       "             ('module.layernorms.6.weight',\n",
       "              tensor([0.7985, 1.0012, 0.9667,  ..., 1.0386, 1.0886, 1.1030])),\n",
       "             ('module.layernorms.6.bias',\n",
       "              tensor([-0.0493,  0.0159, -0.2879,  ...,  0.0044,  0.0504,  0.1249])),\n",
       "             ('module.layernorms.7.weight',\n",
       "              tensor([0.5662, 0.8900, 0.8827,  ..., 0.8112, 0.7658, 0.9159])),\n",
       "             ('module.layernorms.7.bias',\n",
       "              tensor([ 0.4042, -0.0552,  0.0860,  ...,  0.0921,  0.2659,  0.1072])),\n",
       "             ('module.layernorms.8.weight',\n",
       "              tensor([0.3534, 0.8476, 0.7647,  ..., 0.8002, 0.6811, 0.9719])),\n",
       "             ('module.layernorms.8.bias',\n",
       "              tensor([0.6891, 0.1136, 0.2119,  ..., 0.1988, 0.4895, 0.1491])),\n",
       "             ('module.layernorms.9.weight',\n",
       "              tensor([0.5258, 0.8001, 0.8131,  ..., 0.7431, 0.7215, 0.7230])),\n",
       "             ('module.layernorms.9.bias',\n",
       "              tensor([ 0.3274,  0.0666,  0.0127,  ...,  0.0796,  0.2011, -0.1690])),\n",
       "             ('module.layernorms.10.weight',\n",
       "              tensor([0.7109, 0.9358, 0.9428,  ..., 0.9312, 0.6910, 0.8972])),\n",
       "             ('module.layernorms.10.bias',\n",
       "              tensor([0.1473, 0.1155, 0.0568,  ..., 0.0393, 0.4727, 0.0775])),\n",
       "             ('module.layernorms.11.weight',\n",
       "              tensor([0.5621, 0.5854, 0.6019,  ..., 0.6596, 0.5826, 0.5692])),\n",
       "             ('module.layernorms.11.bias',\n",
       "              tensor([-0.0735,  0.0330, -0.0415,  ..., -0.0570,  0.0869,  0.0243]))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = torch.load('checkpoints/B_12-Wi_1024_res_64_in21k_tinyimagenet.t7', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dd['model'], 'checkpoints/B_12-Wi_1024_res_64_in21k_tinyimagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd1 = torch.load('checkpoints/B_12-Wi_1024_res_64_in21k_tinyimagenet', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['module.linear_in.weight', 'module.linear_in.bias', 'module.linear_out.weight', 'module.linear_out.bias', 'module.blocks.0.block.0.weight', 'module.blocks.0.block.0.bias', 'module.blocks.0.block.2.weight', 'module.blocks.0.block.2.bias', 'module.blocks.1.block.0.weight', 'module.blocks.1.block.0.bias', 'module.blocks.1.block.2.weight', 'module.blocks.1.block.2.bias', 'module.blocks.2.block.0.weight', 'module.blocks.2.block.0.bias', 'module.blocks.2.block.2.weight', 'module.blocks.2.block.2.bias', 'module.blocks.3.block.0.weight', 'module.blocks.3.block.0.bias', 'module.blocks.3.block.2.weight', 'module.blocks.3.block.2.bias', 'module.blocks.4.block.0.weight', 'module.blocks.4.block.0.bias', 'module.blocks.4.block.2.weight', 'module.blocks.4.block.2.bias', 'module.blocks.5.block.0.weight', 'module.blocks.5.block.0.bias', 'module.blocks.5.block.2.weight', 'module.blocks.5.block.2.bias', 'module.blocks.6.block.0.weight', 'module.blocks.6.block.0.bias', 'module.blocks.6.block.2.weight', 'module.blocks.6.block.2.bias', 'module.blocks.7.block.0.weight', 'module.blocks.7.block.0.bias', 'module.blocks.7.block.2.weight', 'module.blocks.7.block.2.bias', 'module.blocks.8.block.0.weight', 'module.blocks.8.block.0.bias', 'module.blocks.8.block.2.weight', 'module.blocks.8.block.2.bias', 'module.blocks.9.block.0.weight', 'module.blocks.9.block.0.bias', 'module.blocks.9.block.2.weight', 'module.blocks.9.block.2.bias', 'module.blocks.10.block.0.weight', 'module.blocks.10.block.0.bias', 'module.blocks.10.block.2.weight', 'module.blocks.10.block.2.bias', 'module.blocks.11.block.0.weight', 'module.blocks.11.block.0.bias', 'module.blocks.11.block.2.weight', 'module.blocks.11.block.2.bias', 'module.layernorms.0.weight', 'module.layernorms.0.bias', 'module.layernorms.1.weight', 'module.layernorms.1.bias', 'module.layernorms.2.weight', 'module.layernorms.2.bias', 'module.layernorms.3.weight', 'module.layernorms.3.bias', 'module.layernorms.4.weight', 'module.layernorms.4.bias', 'module.layernorms.5.weight', 'module.layernorms.5.bias', 'module.layernorms.6.weight', 'module.layernorms.6.bias', 'module.layernorms.7.weight', 'module.layernorms.7.bias', 'module.layernorms.8.weight', 'module.layernorms.8.bias', 'module.layernorms.9.weight', 'module.layernorms.9.bias', 'module.layernorms.10.weight', 'module.layernorms.10.bias', 'module.layernorms.11.weight', 'module.layernorms.11.bias'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "c2=0\n",
    "for k in dd1:\n",
    "    if k[:7] != 'module.':\n",
    "        c += 1\n",
    "    else:\n",
    "        c2 +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in dd1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'linear_in.weight'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_cpy = dd1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dd1:\n",
    "    val = dd1[k]\n",
    "    dd_cpy.pop(k)\n",
    "    dd_cpy[k[7:]] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_in.weight',\n",
       "              tensor([[ 2.2785e-01,  2.9513e-01,  3.1012e-03,  ..., -1.0594e-01,\n",
       "                        2.2147e-02,  8.5203e-02],\n",
       "                      [-2.3599e-01, -5.2422e-02, -4.1217e-02,  ..., -1.1177e-01,\n",
       "                       -4.4740e-01, -4.3109e-01],\n",
       "                      [-4.5495e-01, -4.4787e-01, -2.9203e-01,  ...,  1.4033e-01,\n",
       "                        2.5422e-01,  3.6087e-01],\n",
       "                      ...,\n",
       "                      [ 1.7607e-02, -3.5418e-01,  1.9371e-02,  ..., -2.6691e-01,\n",
       "                       -5.2200e-01, -2.3111e-01],\n",
       "                      [ 3.8978e-01,  1.9745e-01,  4.5564e-01,  ...,  2.0694e-01,\n",
       "                        4.6923e-01,  3.8922e-01],\n",
       "                      [-1.0741e-02,  2.3873e-05,  6.7596e-02,  ..., -3.2074e-02,\n",
       "                       -1.3536e-02, -9.1601e-02]])),\n",
       "             ('linear_in.bias',\n",
       "              tensor([ 1.0549,  0.4441, -0.6998,  ...,  0.7105,  1.1405, -0.3989])),\n",
       "             ('linear_out.weight',\n",
       "              tensor([[ 1.5310e-02,  4.5584e-03, -2.1657e-02,  ...,  1.4852e-02,\n",
       "                        1.7602e-02,  3.7569e-02],\n",
       "                      [-2.4131e-02,  4.3384e-02,  2.5383e-04,  ..., -1.0433e-02,\n",
       "                        6.3600e-02,  8.5815e-05],\n",
       "                      [ 5.1396e-03, -3.5944e-02,  1.3738e-02,  ...,  2.9717e-02,\n",
       "                        4.5733e-02, -1.3358e-02],\n",
       "                      ...,\n",
       "                      [ 1.7091e-02,  6.5796e-03,  2.2119e-02,  ..., -1.7120e-02,\n",
       "                        1.0750e-02, -1.7398e-03],\n",
       "                      [-2.2876e-02,  3.6375e-02, -8.4044e-03,  ...,  9.7869e-03,\n",
       "                        6.1939e-03,  5.1413e-02],\n",
       "                      [ 3.1976e-02,  2.7844e-02, -9.1802e-03,  ..., -2.3205e-02,\n",
       "                       -6.5282e-03, -9.3661e-03]])),\n",
       "             ('linear_out.bias',\n",
       "              tensor([-2.2233e-02,  4.7180e-03,  2.0199e-02, -1.7607e-02, -1.9371e-02,\n",
       "                      -1.3742e-02,  1.6159e-03,  1.9111e-02, -2.3729e-03, -1.7115e-02,\n",
       "                      -7.9429e-03,  4.0819e-04, -1.9304e-02,  1.2566e-02,  1.7746e-02,\n",
       "                      -7.6389e-03,  3.0585e-02,  2.8400e-03, -3.1321e-02, -3.2486e-02,\n",
       "                       2.7580e-03, -1.7624e-02, -1.4319e-02,  8.4333e-03,  4.3465e-02,\n",
       "                      -2.5389e-02, -2.2733e-03, -2.1876e-03, -4.5559e-03,  8.5563e-03,\n",
       "                       2.9591e-02,  5.8031e-03,  1.5234e-02, -3.1861e-03,  2.2088e-03,\n",
       "                      -8.0798e-03, -5.1366e-03, -1.3168e-02,  2.8559e-02,  1.9263e-02,\n",
       "                       1.6346e-02,  1.7107e-03, -6.0764e-03, -1.3113e-02,  8.1480e-03,\n",
       "                       1.5877e-02,  2.5368e-02, -2.3369e-02, -1.3015e-02, -3.2638e-02,\n",
       "                       2.1722e-02,  2.1401e-02,  8.7723e-04, -8.2391e-04,  2.2586e-02,\n",
       "                       2.0539e-03, -3.3381e-02,  2.6979e-02,  8.2059e-03,  1.9999e-02,\n",
       "                      -3.3741e-02,  5.5850e-02, -1.3285e-02,  3.6531e-02, -3.6205e-02,\n",
       "                      -3.1369e-04,  3.6244e-02, -3.4717e-02, -4.2435e-02, -5.4166e-03,\n",
       "                      -3.3767e-03, -1.5626e-02, -2.9058e-02, -1.5808e-02, -2.1920e-02,\n",
       "                       3.3340e-03,  2.6105e-03,  1.7108e-02,  3.1480e-03, -1.8088e-02,\n",
       "                      -2.3159e-02,  1.0950e-02, -1.1674e-02, -3.1949e-02, -1.2243e-02,\n",
       "                      -2.6470e-02, -1.1853e-03,  1.4320e-02, -2.2628e-02,  1.4019e-02,\n",
       "                       1.1958e-02,  7.9806e-03,  2.4497e-03, -9.1303e-03, -8.8073e-03,\n",
       "                      -1.3406e-02,  2.5348e-02,  5.6135e-03,  2.1293e-02,  1.0775e-03,\n",
       "                      -3.5777e-02,  2.5035e-02,  1.5730e-02, -2.5823e-02, -8.9496e-04,\n",
       "                       5.1155e-03, -6.0336e-03, -3.9254e-03,  1.7721e-03, -1.0459e-03,\n",
       "                      -1.2330e-02,  1.7300e-02,  1.4628e-02,  4.6622e-03,  8.3632e-03,\n",
       "                      -1.5671e-02, -2.1697e-02,  1.3259e-02,  2.2073e-02, -3.1746e-02,\n",
       "                      -2.2435e-02, -1.9867e-02, -1.4146e-02, -8.6846e-03, -1.0443e-02,\n",
       "                       2.3150e-02, -2.5896e-02,  2.3963e-02,  2.9827e-02, -1.4332e-02,\n",
       "                      -1.7334e-02,  2.2446e-02, -1.0597e-02,  1.7358e-02, -1.0068e-02,\n",
       "                       2.7703e-05,  6.1129e-03, -1.7153e-02, -1.1217e-02, -2.7542e-02,\n",
       "                       9.7831e-04,  2.6937e-02,  1.4052e-02, -2.3865e-02,  2.3427e-02,\n",
       "                      -2.0420e-02, -3.5422e-02,  1.0900e-02, -1.9206e-03,  3.7560e-02,\n",
       "                      -1.6947e-02,  3.2381e-02, -4.3584e-03, -2.3638e-02, -3.3368e-02,\n",
       "                       2.1793e-02,  1.4140e-02,  9.9275e-04, -1.0406e-02,  2.2203e-02,\n",
       "                       1.4997e-02, -8.5755e-03,  3.1612e-03, -1.4520e-02, -1.1920e-02,\n",
       "                       4.8340e-03,  3.7956e-02, -3.3018e-02,  2.5051e-02,  3.1364e-03,\n",
       "                       2.4817e-03, -6.5288e-03, -2.5929e-02, -5.5585e-04, -3.3942e-02,\n",
       "                       3.1199e-02, -3.5678e-02, -1.5957e-02, -1.9025e-02, -3.4255e-02,\n",
       "                      -1.0196e-02,  3.3542e-03, -2.5839e-02, -4.0004e-02,  1.6131e-02,\n",
       "                      -1.5694e-02,  5.2702e-02,  3.4145e-02, -3.3064e-02,  8.5454e-03,\n",
       "                      -7.3373e-03, -8.0496e-03, -1.6434e-02, -3.8912e-03,  1.3750e-03,\n",
       "                       1.6093e-02,  2.6178e-02, -2.2148e-02,  1.0205e-02, -1.3512e-02])),\n",
       "             ('blocks.0.block.0.weight',\n",
       "              tensor([[ 2.4233e-02,  3.9381e-02,  8.9794e-02,  ..., -3.8441e-02,\n",
       "                        1.1294e-01,  1.5066e-01],\n",
       "                      [ 4.0995e-02, -1.8943e-01, -7.3669e-02,  ...,  9.9511e-02,\n",
       "                        7.6077e-02, -1.1076e-01],\n",
       "                      [-7.1269e-02, -1.1163e-01, -2.0323e-01,  ...,  1.0534e-02,\n",
       "                        1.8004e-01,  1.3235e-02],\n",
       "                      ...,\n",
       "                      [ 5.5758e-02,  5.5843e-02,  3.1572e-02,  ...,  8.3500e-02,\n",
       "                        4.6117e-02,  1.1401e-01],\n",
       "                      [-4.4994e+00, -1.0062e+00, -1.3662e+00,  ...,  2.5330e+00,\n",
       "                       -1.2363e+01,  1.5670e+00],\n",
       "                      [-1.5782e-01,  5.3632e-02, -1.4314e-01,  ..., -4.4379e-03,\n",
       "                       -1.5784e-01,  7.1314e-02]])),\n",
       "             ('blocks.0.block.0.bias',\n",
       "              tensor([-9.6826e-02, -3.3783e-01, -5.6529e-01,  ..., -5.5730e-03,\n",
       "                      -1.7037e+01, -1.3502e-01])),\n",
       "             ('blocks.0.block.2.weight',\n",
       "              tensor([[ 7.6787e-02, -4.2139e-02,  2.3695e-02,  ..., -1.5165e-01,\n",
       "                        1.3121e+01,  2.0227e-01],\n",
       "                      [-4.8563e-02,  1.3263e-01, -1.7624e-01,  ..., -2.1772e-02,\n",
       "                        1.2372e+01, -1.0019e-01],\n",
       "                      [-2.4725e-01, -8.5116e-02, -2.1579e-01,  ...,  1.0976e-01,\n",
       "                       -1.2576e+00,  2.3959e-02],\n",
       "                      ...,\n",
       "                      [ 6.3970e-02,  1.7621e-02, -3.6636e-02,  ..., -1.4575e-02,\n",
       "                       -1.5215e+01,  7.4490e-03],\n",
       "                      [-1.3190e-01,  5.0964e-02,  8.1512e-02,  ...,  1.8872e-02,\n",
       "                       -4.2990e-01,  2.6202e-01],\n",
       "                      [-1.8552e-01,  5.9508e-02,  1.2069e-01,  ...,  6.4129e-03,\n",
       "                        6.4521e+00, -4.7724e-02]])),\n",
       "             ('blocks.0.block.2.bias',\n",
       "              tensor([ 0.2231,  0.1248, -0.0066,  ...,  0.0826,  0.0796, -0.0618])),\n",
       "             ('blocks.1.block.0.weight',\n",
       "              tensor([[ 1.5805e-01,  1.2721e-01,  2.4747e-01,  ..., -6.1117e-02,\n",
       "                       -1.8927e-02,  2.2562e-02],\n",
       "                      [-4.5649e+00, -1.0756e+01, -5.1982e+00,  ..., -1.0296e+01,\n",
       "                        1.2773e+01,  5.2307e+00],\n",
       "                      [-3.2303e-01,  2.6477e-02, -4.7228e-01,  ..., -2.8497e-03,\n",
       "                        1.2643e-01, -1.6862e-01],\n",
       "                      ...,\n",
       "                      [-1.0330e-01, -1.9956e-01, -1.4170e-02,  ..., -1.8513e-01,\n",
       "                       -1.8957e-01,  5.7182e-02],\n",
       "                      [-4.4263e-03, -1.8471e-02, -2.3810e-01,  ..., -1.0053e-01,\n",
       "                       -7.8911e-03,  3.2270e-01],\n",
       "                      [-7.8252e-02,  8.8742e-03,  1.1597e-02,  ...,  1.6769e-01,\n",
       "                       -3.4546e-01, -1.8271e-01]])),\n",
       "             ('blocks.1.block.0.bias',\n",
       "              tensor([ -0.0350, -15.9927,   0.0362,  ...,  -0.0368,  -0.2115,  -0.0833])),\n",
       "             ('blocks.1.block.2.weight',\n",
       "              tensor([[-2.2174e-01,  9.1575e+00,  1.3423e-01,  ...,  1.5716e-01,\n",
       "                        1.5954e-01,  1.0721e-01],\n",
       "                      [-4.5691e-02, -9.9904e+00, -5.5552e-02,  ...,  3.3170e-02,\n",
       "                       -4.1374e-02, -1.3305e-01],\n",
       "                      [-2.5440e-02, -3.7968e+00,  1.7103e-01,  ...,  3.7157e-02,\n",
       "                        3.7250e-01,  8.9967e-02],\n",
       "                      ...,\n",
       "                      [ 5.3361e-02, -7.8918e+00, -1.0511e-01,  ...,  3.1066e-01,\n",
       "                       -4.9761e-02, -1.9717e-02],\n",
       "                      [-7.0248e-02,  8.5312e+00,  9.7379e-03,  ...,  4.3080e-01,\n",
       "                        1.6155e-01,  2.2058e-01],\n",
       "                      [ 3.4357e-02, -1.1590e+01, -1.2784e-02,  ...,  1.0441e-01,\n",
       "                       -1.2664e-01,  5.7258e-02]])),\n",
       "             ('blocks.1.block.2.bias',\n",
       "              tensor([-0.0628, -0.0023,  0.1839,  ..., -0.1149, -0.1912,  0.1500])),\n",
       "             ('blocks.2.block.0.weight',\n",
       "              tensor([[-0.0401, -0.0463, -0.1020,  ...,  0.1745,  0.1535,  0.1457],\n",
       "                      [-0.0418,  0.3385,  0.0483,  ...,  0.0028, -0.0623, -0.1773],\n",
       "                      [-0.1768, -0.1277, -0.0892,  ..., -0.1049, -0.2704,  0.0574],\n",
       "                      ...,\n",
       "                      [ 0.0715, -0.0463,  0.3422,  ..., -0.0761,  0.0980, -0.3528],\n",
       "                      [-0.0050,  0.1508,  0.1946,  ...,  0.0653, -0.0704, -0.2119],\n",
       "                      [-0.1257,  0.0987, -0.1020,  ..., -0.3311, -0.0923,  0.0875]])),\n",
       "             ('blocks.2.block.0.bias',\n",
       "              tensor([-0.1835,  0.0248, -0.0889,  ..., -0.1258, -0.1065, -0.4614])),\n",
       "             ('blocks.2.block.2.weight',\n",
       "              tensor([[-0.0922,  0.2174, -0.1528,  ..., -0.2286, -0.1478,  0.2187],\n",
       "                      [ 0.2245, -0.0996, -0.4313,  ...,  0.1023, -0.1211,  0.4575],\n",
       "                      [ 0.1641, -0.0997,  0.0130,  ..., -0.2104, -0.3031, -0.0386],\n",
       "                      ...,\n",
       "                      [-0.3106,  0.0566, -0.0617,  ...,  0.0859, -0.2550, -0.4428],\n",
       "                      [-0.1854,  0.2102, -0.4331,  ..., -0.1370, -0.1197, -0.2925],\n",
       "                      [-0.3098, -0.0040,  0.1260,  ...,  0.4876,  0.1900, -0.1347]])),\n",
       "             ('blocks.2.block.2.bias',\n",
       "              tensor([ 0.0541,  0.2760, -0.0468,  ..., -0.1068, -0.0760,  0.0442])),\n",
       "             ('blocks.3.block.0.weight',\n",
       "              tensor([[-0.0149, -0.1589,  0.0622,  ..., -0.0914,  0.0261,  0.0955],\n",
       "                      [-0.0862,  0.1411, -0.1872,  ..., -0.1349, -0.1502, -0.0568],\n",
       "                      [ 0.0331,  0.2189,  0.3690,  ...,  0.0812,  0.0892,  0.0453],\n",
       "                      ...,\n",
       "                      [ 0.4206, -0.2235,  0.3928,  ..., -0.2843, -0.0754,  0.3149],\n",
       "                      [-0.0903,  0.0102, -0.1087,  ...,  0.0047, -0.0815,  0.1806],\n",
       "                      [ 0.0007, -0.2562, -0.3984,  ...,  0.0725, -0.0173,  0.0329]])),\n",
       "             ('blocks.3.block.0.bias',\n",
       "              tensor([-0.0444, -0.1055, -0.0827,  ...,  0.0153, -0.0643, -0.0482])),\n",
       "             ('blocks.3.block.2.weight',\n",
       "              tensor([[ 0.0025, -0.0875, -0.0262,  ..., -0.4101, -0.0492, -0.1684],\n",
       "                      [ 0.0424,  0.0136,  0.1662,  ...,  0.0643,  0.1026,  0.2275],\n",
       "                      [-0.0728, -0.0414, -0.1354,  ..., -0.0811, -0.1234,  0.2161],\n",
       "                      ...,\n",
       "                      [-0.3781,  0.0894,  0.1606,  ...,  0.1624, -0.1099,  0.2508],\n",
       "                      [-0.0792,  0.0778, -0.2900,  ..., -0.0656, -0.1363, -0.0520],\n",
       "                      [ 0.0254,  0.1960,  0.1438,  ..., -0.1249, -0.2361,  0.0886]])),\n",
       "             ('blocks.3.block.2.bias',\n",
       "              tensor([-0.0948, -0.1231,  0.1398,  ..., -0.0208, -0.0976,  0.2700])),\n",
       "             ('blocks.4.block.0.weight',\n",
       "              tensor([[-0.1049, -0.1305, -0.2855,  ..., -0.0542, -0.1488, -0.2262],\n",
       "                      [ 0.1183, -0.0056,  0.0053,  ...,  0.2778,  0.1699,  0.4299],\n",
       "                      [ 0.0561,  0.4976, -0.1264,  ..., -0.2325, -0.5417, -0.0406],\n",
       "                      ...,\n",
       "                      [-0.1623,  0.3207,  0.0237,  ...,  0.2013, -0.1697,  0.0517],\n",
       "                      [-0.1941,  0.2952, -0.1786,  ..., -0.2742,  0.0324,  0.2371],\n",
       "                      [-0.1229, -0.0871, -0.0038,  ..., -0.1492,  0.1354, -0.0576]])),\n",
       "             ('blocks.4.block.0.bias',\n",
       "              tensor([-0.1310, -0.0301, -0.0366,  ..., -0.2420, -0.0698,  0.1149])),\n",
       "             ('blocks.4.block.2.weight',\n",
       "              tensor([[-2.5173e-01,  1.5915e-01,  8.3085e-02,  ...,  5.6842e-02,\n",
       "                        2.4823e-01,  7.4795e-02],\n",
       "                      [-7.5000e-02, -7.6058e-02, -2.0464e-01,  ..., -1.2761e-01,\n",
       "                       -1.2017e-01,  3.3189e-02],\n",
       "                      [ 1.1113e-01,  8.2308e-02,  1.4933e-01,  ..., -4.1238e-02,\n",
       "                        1.0231e-01,  1.0150e-01],\n",
       "                      ...,\n",
       "                      [ 1.5821e-01, -6.9496e-02,  2.3649e-02,  ...,  3.3394e-01,\n",
       "                       -2.2377e-01,  1.0285e-01],\n",
       "                      [-2.6443e-02, -2.9853e-01,  9.3541e-02,  ..., -2.3186e-01,\n",
       "                        1.4372e-01, -2.2209e-01],\n",
       "                      [ 3.9844e-02, -2.9001e-01,  1.2870e-01,  ..., -9.0114e-02,\n",
       "                       -1.6892e-04,  1.0710e-01]])),\n",
       "             ('blocks.4.block.2.bias',\n",
       "              tensor([ 0.0576,  0.0713,  0.1490,  ..., -0.1096, -0.0439,  0.0933])),\n",
       "             ('blocks.5.block.0.weight',\n",
       "              tensor([[ 0.0433, -0.0102,  0.1682,  ...,  0.1891, -0.2939, -0.1417],\n",
       "                      [-0.0361,  0.0929, -0.0713,  ...,  0.3671,  0.1447, -0.0469],\n",
       "                      [ 0.0158,  0.0086,  0.3590,  ..., -0.0688,  0.2010, -0.0566],\n",
       "                      ...,\n",
       "                      [-0.1042,  0.0292, -0.0903,  ..., -0.1709,  0.0548, -0.0045],\n",
       "                      [ 0.3617,  0.1008,  0.2209,  ...,  0.1903, -0.1174,  0.0040],\n",
       "                      [ 0.3022, -0.0134, -0.2615,  ..., -0.1266, -0.2347,  0.1489]])),\n",
       "             ('blocks.5.block.0.bias',\n",
       "              tensor([-0.0612, -0.1720,  0.2523,  ...,  0.1828,  0.0049, -0.3810])),\n",
       "             ('blocks.5.block.2.weight',\n",
       "              tensor([[-0.1457, -0.0735,  0.0560,  ...,  0.0342, -0.2236,  0.0496],\n",
       "                      [ 0.2704, -0.1691, -0.1121,  ..., -0.0444, -0.0889,  0.0113],\n",
       "                      [-0.1391, -0.2265, -0.2293,  ...,  0.1369, -0.2971, -0.1346],\n",
       "                      ...,\n",
       "                      [-0.0325,  0.0118, -0.0784,  ...,  0.1964, -0.1227, -0.0334],\n",
       "                      [ 0.2843, -0.0619, -0.5277,  ..., -0.1750,  0.0763,  0.0587],\n",
       "                      [ 0.3160,  0.3021, -0.0541,  ..., -0.0644,  0.0752, -0.0881]])),\n",
       "             ('blocks.5.block.2.bias',\n",
       "              tensor([ 0.1512, -0.0741,  0.0560,  ..., -0.2392, -0.0169, -0.1515])),\n",
       "             ('blocks.6.block.0.weight',\n",
       "              tensor([[ 0.2789,  0.2492,  0.0623,  ...,  0.1983,  0.5675,  0.1864],\n",
       "                      [ 0.0419, -0.1490, -0.1362,  ...,  0.1188, -0.2285, -0.0376],\n",
       "                      [ 0.0506,  0.2802, -0.0599,  ...,  0.0633, -0.0984, -0.2974],\n",
       "                      ...,\n",
       "                      [-0.1737, -0.1943, -0.3088,  ..., -0.1296,  0.1287, -0.0828],\n",
       "                      [-0.2191,  0.4009, -0.2403,  ..., -0.2184, -0.2566, -0.1832],\n",
       "                      [-0.0420,  0.1972, -0.0740,  ...,  0.1082, -0.2329,  0.2128]])),\n",
       "             ('blocks.6.block.0.bias',\n",
       "              tensor([-0.2050, -0.0776, -0.0608,  ..., -0.0661, -0.1689, -0.0353])),\n",
       "             ('blocks.6.block.2.weight',\n",
       "              tensor([[ 0.4031, -0.0870, -0.0769,  ...,  0.2368, -0.0148,  0.1094],\n",
       "                      [ 0.0380, -0.4580, -0.0924,  ..., -0.0738, -0.0709, -0.2268],\n",
       "                      [ 0.0866,  0.0899,  0.2173,  ...,  0.0745,  0.1248,  0.0151],\n",
       "                      ...,\n",
       "                      [ 0.1775,  0.0206, -0.0639,  ...,  0.0946,  0.0309, -0.3480],\n",
       "                      [ 0.1583,  0.4017, -0.1850,  ..., -0.0006,  0.0633,  0.1378],\n",
       "                      [ 0.2393, -0.0512, -0.0965,  ...,  0.1852, -0.0739, -0.0981]])),\n",
       "             ('blocks.6.block.2.bias',\n",
       "              tensor([-0.1463, -0.0004, -0.0069,  ..., -0.1829,  0.1829,  0.1241])),\n",
       "             ('blocks.7.block.0.weight',\n",
       "              tensor([[ 0.0488,  0.2113, -0.2496,  ...,  0.1457, -0.1329,  0.2354],\n",
       "                      [ 0.1208, -0.1310, -0.0271,  ..., -0.2565, -0.2476, -0.1141],\n",
       "                      [ 0.0563, -0.0310, -0.1081,  ...,  0.1122, -0.0297, -0.3134],\n",
       "                      ...,\n",
       "                      [-0.1405, -0.1149, -0.1877,  ..., -0.0072, -0.0641,  0.0024],\n",
       "                      [ 0.2687,  0.0417,  0.1017,  ..., -0.1451,  0.2974,  0.0036],\n",
       "                      [ 0.1827, -0.1825,  0.0824,  ...,  0.0724, -0.1122,  0.0502]])),\n",
       "             ('blocks.7.block.0.bias',\n",
       "              tensor([-0.0607, -0.3520, -0.3511,  ..., -0.1786, -0.1535, -0.0389])),\n",
       "             ('blocks.7.block.2.weight',\n",
       "              tensor([[ 0.0372,  0.2423,  0.3225,  ..., -0.0641,  0.0167, -0.1562],\n",
       "                      [-0.4528, -0.0681,  0.1955,  ...,  0.1321, -0.1680,  0.0229],\n",
       "                      [ 0.2625,  0.3051,  0.0210,  ...,  0.2845, -0.0358,  0.2679],\n",
       "                      ...,\n",
       "                      [-0.0628,  0.0697,  0.0912,  ...,  0.0693, -0.3235, -0.1028],\n",
       "                      [ 0.0371,  0.0135,  0.3312,  ...,  0.2222,  0.1602, -0.1310],\n",
       "                      [-0.3948, -0.0661,  0.1588,  ..., -0.0406,  0.0803, -0.0985]])),\n",
       "             ('blocks.7.block.2.bias',\n",
       "              tensor([-0.2331, -0.0685,  0.0787,  ..., -0.1488, -0.1162, -0.0125])),\n",
       "             ('blocks.8.block.0.weight',\n",
       "              tensor([[-0.1329,  0.1128,  0.2585,  ...,  0.3509,  0.3478, -0.2969],\n",
       "                      [-0.4062, -0.2123, -0.2801,  ..., -0.2698,  0.1264, -0.0102],\n",
       "                      [ 0.0026,  0.0012, -0.0355,  ...,  0.0800,  0.0818,  0.3289],\n",
       "                      ...,\n",
       "                      [ 0.0878, -0.2870, -0.2456,  ...,  0.3155, -0.2911,  0.1864],\n",
       "                      [-0.0044, -0.0910, -0.2187,  ..., -0.1939,  0.2040,  0.0505],\n",
       "                      [-0.0659, -0.0759, -0.1948,  ...,  0.0603, -0.2600, -0.1939]])),\n",
       "             ('blocks.8.block.0.bias',\n",
       "              tensor([-0.1521, -0.5604, -0.0167,  ..., -0.0442, -0.1698, -0.1569])),\n",
       "             ('blocks.8.block.2.weight',\n",
       "              tensor([[ 0.4239, -0.5219,  0.0835,  ..., -0.1390, -0.1786,  0.2612],\n",
       "                      [ 0.0882,  0.3534,  0.2334,  ...,  0.1583, -0.1816, -0.0227],\n",
       "                      [-0.1304, -0.1955,  0.2400,  ...,  0.0651, -0.0893,  0.0124],\n",
       "                      ...,\n",
       "                      [-0.1140, -0.4710,  0.1111,  ..., -0.3501,  0.0589, -0.2795],\n",
       "                      [-0.0631,  0.0969, -0.1033,  ...,  0.2768,  0.1021, -0.4507],\n",
       "                      [ 0.2577,  0.6523, -0.1738,  ..., -0.1902,  0.1051, -0.2363]])),\n",
       "             ('blocks.8.block.2.bias',\n",
       "              tensor([ 0.5705, -0.1700,  0.0718,  ...,  0.2826,  0.0277, -0.2006])),\n",
       "             ('blocks.9.block.0.weight',\n",
       "              tensor([[-5.4681e-01,  1.7233e-01, -2.3662e-01,  ...,  7.6808e-02,\n",
       "                       -1.3844e-01, -2.1736e-01],\n",
       "                      [-2.3053e-01,  2.2240e-01,  1.7622e-01,  ..., -2.0501e-01,\n",
       "                       -2.0087e-01, -1.1129e-05],\n",
       "                      [-2.4110e-01,  1.1053e-01, -3.0185e-01,  ..., -3.1189e-03,\n",
       "                       -3.1544e-01, -2.5933e-01],\n",
       "                      ...,\n",
       "                      [-3.6606e-01,  3.0700e-01,  7.9647e-02,  ..., -5.7641e-02,\n",
       "                       -4.2583e-02, -2.0506e-01],\n",
       "                      [ 3.0240e-02, -2.7110e-01,  4.9311e-03,  ...,  8.5930e-03,\n",
       "                        3.9594e-01, -1.6317e-02],\n",
       "                      [-8.8926e-02,  2.9905e-01, -1.2803e-01,  ..., -8.8166e-02,\n",
       "                       -8.0361e-02, -2.2465e-02]])),\n",
       "             ('blocks.9.block.0.bias',\n",
       "              tensor([-0.1209, -0.1316, -0.1077,  ..., -0.1076, -0.0539, -0.1108])),\n",
       "             ('blocks.9.block.2.weight',\n",
       "              tensor([[-0.1729,  0.2911,  0.0305,  ...,  0.1897,  0.0710, -0.2637],\n",
       "                      [ 0.1845,  0.0675, -0.2104,  ..., -0.2019,  0.2738,  0.0769],\n",
       "                      [ 0.0414, -0.2912,  0.2329,  ..., -0.1551,  0.1065, -0.2333],\n",
       "                      ...,\n",
       "                      [-0.1732,  0.0322,  0.1817,  ...,  0.0914,  0.0264, -0.0509],\n",
       "                      [ 0.0146,  0.0947,  0.3782,  ..., -0.1059, -0.2776, -0.3160],\n",
       "                      [-0.0592,  0.2106,  0.2623,  ...,  0.2718,  0.3792, -0.2180]])),\n",
       "             ('blocks.9.block.2.bias',\n",
       "              tensor([ 0.3617, -0.1231,  0.0051,  ...,  0.4921, -0.3412, -0.4197])),\n",
       "             ('blocks.10.block.0.weight',\n",
       "              tensor([[-0.1259, -0.2608,  0.0318,  ...,  0.1310, -0.2084,  0.3234],\n",
       "                      [-0.0597, -0.2142,  0.2485,  ..., -0.1086, -0.1105, -0.1124],\n",
       "                      [-0.2514, -0.0705, -0.0705,  ..., -0.0954, -0.3634, -0.0395],\n",
       "                      ...,\n",
       "                      [-0.0895,  0.0642,  0.0132,  ..., -0.1944, -0.0343,  0.0579],\n",
       "                      [-0.1464, -0.1184,  0.5011,  ..., -0.0938, -0.0509,  0.2741],\n",
       "                      [-0.4393, -0.1006, -0.1088,  ...,  0.0229,  0.2068,  0.1248]])),\n",
       "             ('blocks.10.block.0.bias',\n",
       "              tensor([-0.1646, -0.1190, -0.1184,  ..., -0.1149, -0.0544, -0.1384])),\n",
       "             ('blocks.10.block.2.weight',\n",
       "              tensor([[-0.0707, -0.2212,  0.0088,  ...,  0.0052, -0.0012,  0.4452],\n",
       "                      [-0.1535, -0.0938,  0.2192,  ...,  0.2728, -0.0158,  0.3178],\n",
       "                      [ 0.0045,  0.1536,  0.0682,  ..., -0.3985, -0.1143, -0.1686],\n",
       "                      ...,\n",
       "                      [-0.1344,  0.0571, -0.4595,  ..., -0.0474, -0.1225,  0.2309],\n",
       "                      [-0.0859, -0.2490,  0.0189,  ...,  0.2357, -0.1344, -0.0667],\n",
       "                      [-0.0505, -0.1259,  0.1689,  ..., -0.1016, -0.1179, -0.2958]])),\n",
       "             ('blocks.10.block.2.bias',\n",
       "              tensor([-0.1881, -0.1191, -0.1331,  ...,  0.2857,  0.2426, -0.0808])),\n",
       "             ('blocks.11.block.0.weight',\n",
       "              tensor([[ 0.3498,  0.3685, -0.0771,  ..., -0.1898,  0.2694,  0.0108],\n",
       "                      [ 0.1173,  0.0034,  0.2116,  ...,  0.0707, -0.1339, -0.0334],\n",
       "                      [ 0.3335,  0.3951,  0.0677,  ..., -0.2024,  0.1051,  0.1829],\n",
       "                      ...,\n",
       "                      [ 0.2038, -0.0376, -0.0153,  ...,  0.0005, -0.1825, -0.0800],\n",
       "                      [ 0.1062, -0.2900, -0.3776,  ...,  0.1351,  0.1971, -0.0456],\n",
       "                      [ 0.0198,  0.2003,  0.1895,  ...,  0.1400,  0.0239,  0.1355]])),\n",
       "             ('blocks.11.block.0.bias',\n",
       "              tensor([-0.3503, -0.0335, -1.0195,  ..., -0.0114,  0.0095, -0.0758])),\n",
       "             ('blocks.11.block.2.weight',\n",
       "              tensor([[ 3.9136e-01,  2.0257e-01, -1.2120e-01,  ..., -1.6126e-01,\n",
       "                       -6.9070e-02,  6.2569e-03],\n",
       "                      [ 1.3570e-04, -1.5888e-01, -3.4307e-01,  ..., -1.8210e-01,\n",
       "                        9.7553e-02,  8.3246e-02],\n",
       "                      [ 8.8605e-02,  4.7405e-02, -8.0760e-02,  ...,  1.4525e-01,\n",
       "                        1.6544e-01, -1.2190e-01],\n",
       "                      ...,\n",
       "                      [ 3.1106e-02,  1.0694e-01,  8.5001e-04,  ...,  1.2493e-01,\n",
       "                       -6.5309e-02, -9.3870e-02],\n",
       "                      [ 5.1937e-02,  1.3546e-01,  4.2549e-02,  ...,  1.7703e-01,\n",
       "                       -5.5576e-02, -2.0335e-01],\n",
       "                      [ 1.1598e-01, -1.1772e-01, -2.3812e-01,  ..., -1.6924e-01,\n",
       "                       -8.2993e-02, -3.5532e-02]])),\n",
       "             ('blocks.11.block.2.bias',\n",
       "              tensor([ 0.0025,  0.1024,  0.1288,  ...,  0.1316,  0.1059, -0.0692])),\n",
       "             ('layernorms.0.weight',\n",
       "              tensor([0.3462, 0.2206, 0.2124,  ..., 0.2532, 0.1970, 0.1850])),\n",
       "             ('layernorms.0.bias',\n",
       "              tensor([-0.0758, -0.0272,  0.0182,  ..., -0.0301,  0.0094,  0.0490])),\n",
       "             ('layernorms.1.weight',\n",
       "              tensor([0.4872, 0.5277, 0.4854,  ..., 0.5822, 0.7441, 0.5769])),\n",
       "             ('layernorms.1.bias',\n",
       "              tensor([-0.4408, -0.3070,  0.2620,  ..., -0.1967, -0.5229,  0.1637])),\n",
       "             ('layernorms.2.weight',\n",
       "              tensor([0.8940, 0.8926, 0.8699,  ..., 0.9129, 0.9220, 0.8844])),\n",
       "             ('layernorms.2.bias',\n",
       "              tensor([-0.0990, -0.0691,  0.2487,  ...,  0.1285, -0.1782,  0.0909])),\n",
       "             ('layernorms.3.weight',\n",
       "              tensor([0.8122, 0.7996, 0.8641,  ..., 0.8786, 0.8450, 0.8884])),\n",
       "             ('layernorms.3.bias',\n",
       "              tensor([-0.2907, -0.3874,  0.2169,  ..., -0.1600, -0.1326,  0.0014])),\n",
       "             ('layernorms.4.weight',\n",
       "              tensor([0.7250, 0.8961, 0.8889,  ..., 0.8510, 0.8953, 0.9786])),\n",
       "             ('layernorms.4.bias',\n",
       "              tensor([-0.2527, -0.2054, -0.1907,  ..., -0.0262, -0.2111, -0.1264])),\n",
       "             ('layernorms.5.weight',\n",
       "              tensor([0.6619, 0.9248, 0.9464,  ..., 0.9331, 0.9304, 0.9398])),\n",
       "             ('layernorms.5.bias',\n",
       "              tensor([-0.2507, -0.1164, -0.2353,  ..., -0.0676, -0.1726, -0.0612])),\n",
       "             ('layernorms.6.weight',\n",
       "              tensor([0.7985, 1.0012, 0.9667,  ..., 1.0386, 1.0886, 1.1030])),\n",
       "             ('layernorms.6.bias',\n",
       "              tensor([-0.0493,  0.0159, -0.2879,  ...,  0.0044,  0.0504,  0.1249])),\n",
       "             ('layernorms.7.weight',\n",
       "              tensor([0.5662, 0.8900, 0.8827,  ..., 0.8112, 0.7658, 0.9159])),\n",
       "             ('layernorms.7.bias',\n",
       "              tensor([ 0.4042, -0.0552,  0.0860,  ...,  0.0921,  0.2659,  0.1072])),\n",
       "             ('layernorms.8.weight',\n",
       "              tensor([0.3534, 0.8476, 0.7647,  ..., 0.8002, 0.6811, 0.9719])),\n",
       "             ('layernorms.8.bias',\n",
       "              tensor([0.6891, 0.1136, 0.2119,  ..., 0.1988, 0.4895, 0.1491])),\n",
       "             ('layernorms.9.weight',\n",
       "              tensor([0.5258, 0.8001, 0.8131,  ..., 0.7431, 0.7215, 0.7230])),\n",
       "             ('layernorms.9.bias',\n",
       "              tensor([ 0.3274,  0.0666,  0.0127,  ...,  0.0796,  0.2011, -0.1690])),\n",
       "             ('layernorms.10.weight',\n",
       "              tensor([0.7109, 0.9358, 0.9428,  ..., 0.9312, 0.6910, 0.8972])),\n",
       "             ('layernorms.10.bias',\n",
       "              tensor([0.1473, 0.1155, 0.0568,  ..., 0.0393, 0.4727, 0.0775])),\n",
       "             ('layernorms.11.weight',\n",
       "              tensor([0.5621, 0.5854, 0.6019,  ..., 0.6596, 0.5826, 0.5692])),\n",
       "             ('layernorms.11.bias',\n",
       "              tensor([-0.0735,  0.0330, -0.0415,  ..., -0.0570,  0.0869,  0.0243]))])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_cpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dd_cpy, 'checkpoints/B_12-Wi_1024_res_64_in21k_tinyimagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['module.linear_in.weight', 'module.linear_in.bias', 'module.linear_out.weight', 'module.linear_out.bias', 'module.blocks.0.block.0.weight', 'module.blocks.0.block.0.bias', 'module.blocks.0.block.2.weight', 'module.blocks.0.block.2.bias', 'module.blocks.1.block.0.weight', 'module.blocks.1.block.0.bias', 'module.blocks.1.block.2.weight', 'module.blocks.1.block.2.bias', 'module.blocks.2.block.0.weight', 'module.blocks.2.block.0.bias', 'module.blocks.2.block.2.weight', 'module.blocks.2.block.2.bias', 'module.blocks.3.block.0.weight', 'module.blocks.3.block.0.bias', 'module.blocks.3.block.2.weight', 'module.blocks.3.block.2.bias', 'module.blocks.4.block.0.weight', 'module.blocks.4.block.0.bias', 'module.blocks.4.block.2.weight', 'module.blocks.4.block.2.bias', 'module.blocks.5.block.0.weight', 'module.blocks.5.block.0.bias', 'module.blocks.5.block.2.weight', 'module.blocks.5.block.2.bias', 'module.blocks.6.block.0.weight', 'module.blocks.6.block.0.bias', 'module.blocks.6.block.2.weight', 'module.blocks.6.block.2.bias', 'module.blocks.7.block.0.weight', 'module.blocks.7.block.0.bias', 'module.blocks.7.block.2.weight', 'module.blocks.7.block.2.bias', 'module.blocks.8.block.0.weight', 'module.blocks.8.block.0.bias', 'module.blocks.8.block.2.weight', 'module.blocks.8.block.2.bias', 'module.blocks.9.block.0.weight', 'module.blocks.9.block.0.bias', 'module.blocks.9.block.2.weight', 'module.blocks.9.block.2.bias', 'module.blocks.10.block.0.weight', 'module.blocks.10.block.0.bias', 'module.blocks.10.block.2.weight', 'module.blocks.10.block.2.bias', 'module.blocks.11.block.0.weight', 'module.blocks.11.block.0.bias', 'module.blocks.11.block.2.weight', 'module.blocks.11.block.2.bias', 'module.layernorms.0.weight', 'module.layernorms.0.bias', 'module.layernorms.1.weight', 'module.layernorms.1.bias', 'module.layernorms.2.weight', 'module.layernorms.2.bias', 'module.layernorms.3.weight', 'module.layernorms.3.bias', 'module.layernorms.4.weight', 'module.layernorms.4.bias', 'module.layernorms.5.weight', 'module.layernorms.5.bias', 'module.layernorms.6.weight', 'module.layernorms.6.bias', 'module.layernorms.7.weight', 'module.layernorms.7.bias', 'module.layernorms.8.weight', 'module.layernorms.8.bias', 'module.layernorms.9.weight', 'module.layernorms.9.bias', 'module.layernorms.10.weight', 'module.layernorms.10.bias', 'module.layernorms.11.weight', 'module.layernorms.11.bias'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    k: v\n",
    "    for k, v in dd1.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'module.linear_in.weight': tensor([[ 2.2785e-01,  2.9513e-01,  3.1012e-03,  ..., -1.0594e-01,\n",
       "           2.2147e-02,  8.5203e-02],\n",
       "         [-2.3599e-01, -5.2422e-02, -4.1217e-02,  ..., -1.1177e-01,\n",
       "          -4.4740e-01, -4.3109e-01],\n",
       "         [-4.5495e-01, -4.4787e-01, -2.9203e-01,  ...,  1.4033e-01,\n",
       "           2.5422e-01,  3.6087e-01],\n",
       "         ...,\n",
       "         [ 1.7607e-02, -3.5418e-01,  1.9371e-02,  ..., -2.6691e-01,\n",
       "          -5.2200e-01, -2.3111e-01],\n",
       "         [ 3.8978e-01,  1.9745e-01,  4.5564e-01,  ...,  2.0694e-01,\n",
       "           4.6923e-01,  3.8922e-01],\n",
       "         [-1.0741e-02,  2.3873e-05,  6.7596e-02,  ..., -3.2074e-02,\n",
       "          -1.3536e-02, -9.1601e-02]]),\n",
       " 'module.linear_in.bias': tensor([ 1.0549,  0.4441, -0.6998,  ...,  0.7105,  1.1405, -0.3989]),\n",
       " 'module.linear_out.weight': tensor([[ 1.5310e-02,  4.5584e-03, -2.1657e-02,  ...,  1.4852e-02,\n",
       "           1.7602e-02,  3.7569e-02],\n",
       "         [-2.4131e-02,  4.3384e-02,  2.5383e-04,  ..., -1.0433e-02,\n",
       "           6.3600e-02,  8.5815e-05],\n",
       "         [ 5.1396e-03, -3.5944e-02,  1.3738e-02,  ...,  2.9717e-02,\n",
       "           4.5733e-02, -1.3358e-02],\n",
       "         ...,\n",
       "         [ 1.7091e-02,  6.5796e-03,  2.2119e-02,  ..., -1.7120e-02,\n",
       "           1.0750e-02, -1.7398e-03],\n",
       "         [-2.2876e-02,  3.6375e-02, -8.4044e-03,  ...,  9.7869e-03,\n",
       "           6.1939e-03,  5.1413e-02],\n",
       "         [ 3.1976e-02,  2.7844e-02, -9.1802e-03,  ..., -2.3205e-02,\n",
       "          -6.5282e-03, -9.3661e-03]]),\n",
       " 'module.linear_out.bias': tensor([-2.2233e-02,  4.7180e-03,  2.0199e-02, -1.7607e-02, -1.9371e-02,\n",
       "         -1.3742e-02,  1.6159e-03,  1.9111e-02, -2.3729e-03, -1.7115e-02,\n",
       "         -7.9429e-03,  4.0819e-04, -1.9304e-02,  1.2566e-02,  1.7746e-02,\n",
       "         -7.6389e-03,  3.0585e-02,  2.8400e-03, -3.1321e-02, -3.2486e-02,\n",
       "          2.7580e-03, -1.7624e-02, -1.4319e-02,  8.4333e-03,  4.3465e-02,\n",
       "         -2.5389e-02, -2.2733e-03, -2.1876e-03, -4.5559e-03,  8.5563e-03,\n",
       "          2.9591e-02,  5.8031e-03,  1.5234e-02, -3.1861e-03,  2.2088e-03,\n",
       "         -8.0798e-03, -5.1366e-03, -1.3168e-02,  2.8559e-02,  1.9263e-02,\n",
       "          1.6346e-02,  1.7107e-03, -6.0764e-03, -1.3113e-02,  8.1480e-03,\n",
       "          1.5877e-02,  2.5368e-02, -2.3369e-02, -1.3015e-02, -3.2638e-02,\n",
       "          2.1722e-02,  2.1401e-02,  8.7723e-04, -8.2391e-04,  2.2586e-02,\n",
       "          2.0539e-03, -3.3381e-02,  2.6979e-02,  8.2059e-03,  1.9999e-02,\n",
       "         -3.3741e-02,  5.5850e-02, -1.3285e-02,  3.6531e-02, -3.6205e-02,\n",
       "         -3.1369e-04,  3.6244e-02, -3.4717e-02, -4.2435e-02, -5.4166e-03,\n",
       "         -3.3767e-03, -1.5626e-02, -2.9058e-02, -1.5808e-02, -2.1920e-02,\n",
       "          3.3340e-03,  2.6105e-03,  1.7108e-02,  3.1480e-03, -1.8088e-02,\n",
       "         -2.3159e-02,  1.0950e-02, -1.1674e-02, -3.1949e-02, -1.2243e-02,\n",
       "         -2.6470e-02, -1.1853e-03,  1.4320e-02, -2.2628e-02,  1.4019e-02,\n",
       "          1.1958e-02,  7.9806e-03,  2.4497e-03, -9.1303e-03, -8.8073e-03,\n",
       "         -1.3406e-02,  2.5348e-02,  5.6135e-03,  2.1293e-02,  1.0775e-03,\n",
       "         -3.5777e-02,  2.5035e-02,  1.5730e-02, -2.5823e-02, -8.9496e-04,\n",
       "          5.1155e-03, -6.0336e-03, -3.9254e-03,  1.7721e-03, -1.0459e-03,\n",
       "         -1.2330e-02,  1.7300e-02,  1.4628e-02,  4.6622e-03,  8.3632e-03,\n",
       "         -1.5671e-02, -2.1697e-02,  1.3259e-02,  2.2073e-02, -3.1746e-02,\n",
       "         -2.2435e-02, -1.9867e-02, -1.4146e-02, -8.6846e-03, -1.0443e-02,\n",
       "          2.3150e-02, -2.5896e-02,  2.3963e-02,  2.9827e-02, -1.4332e-02,\n",
       "         -1.7334e-02,  2.2446e-02, -1.0597e-02,  1.7358e-02, -1.0068e-02,\n",
       "          2.7703e-05,  6.1129e-03, -1.7153e-02, -1.1217e-02, -2.7542e-02,\n",
       "          9.7831e-04,  2.6937e-02,  1.4052e-02, -2.3865e-02,  2.3427e-02,\n",
       "         -2.0420e-02, -3.5422e-02,  1.0900e-02, -1.9206e-03,  3.7560e-02,\n",
       "         -1.6947e-02,  3.2381e-02, -4.3584e-03, -2.3638e-02, -3.3368e-02,\n",
       "          2.1793e-02,  1.4140e-02,  9.9275e-04, -1.0406e-02,  2.2203e-02,\n",
       "          1.4997e-02, -8.5755e-03,  3.1612e-03, -1.4520e-02, -1.1920e-02,\n",
       "          4.8340e-03,  3.7956e-02, -3.3018e-02,  2.5051e-02,  3.1364e-03,\n",
       "          2.4817e-03, -6.5288e-03, -2.5929e-02, -5.5585e-04, -3.3942e-02,\n",
       "          3.1199e-02, -3.5678e-02, -1.5957e-02, -1.9025e-02, -3.4255e-02,\n",
       "         -1.0196e-02,  3.3542e-03, -2.5839e-02, -4.0004e-02,  1.6131e-02,\n",
       "         -1.5694e-02,  5.2702e-02,  3.4145e-02, -3.3064e-02,  8.5454e-03,\n",
       "         -7.3373e-03, -8.0496e-03, -1.6434e-02, -3.8912e-03,  1.3750e-03,\n",
       "          1.6093e-02,  2.6178e-02, -2.2148e-02,  1.0205e-02, -1.3512e-02]),\n",
       " 'module.blocks.0.block.0.weight': tensor([[ 2.4233e-02,  3.9381e-02,  8.9794e-02,  ..., -3.8441e-02,\n",
       "           1.1294e-01,  1.5066e-01],\n",
       "         [ 4.0995e-02, -1.8943e-01, -7.3669e-02,  ...,  9.9511e-02,\n",
       "           7.6077e-02, -1.1076e-01],\n",
       "         [-7.1269e-02, -1.1163e-01, -2.0323e-01,  ...,  1.0534e-02,\n",
       "           1.8004e-01,  1.3235e-02],\n",
       "         ...,\n",
       "         [ 5.5758e-02,  5.5843e-02,  3.1572e-02,  ...,  8.3500e-02,\n",
       "           4.6117e-02,  1.1401e-01],\n",
       "         [-4.4994e+00, -1.0062e+00, -1.3662e+00,  ...,  2.5330e+00,\n",
       "          -1.2363e+01,  1.5670e+00],\n",
       "         [-1.5782e-01,  5.3632e-02, -1.4314e-01,  ..., -4.4379e-03,\n",
       "          -1.5784e-01,  7.1314e-02]]),\n",
       " 'module.blocks.0.block.0.bias': tensor([-9.6826e-02, -3.3783e-01, -5.6529e-01,  ..., -5.5730e-03,\n",
       "         -1.7037e+01, -1.3502e-01]),\n",
       " 'module.blocks.0.block.2.weight': tensor([[ 7.6787e-02, -4.2139e-02,  2.3695e-02,  ..., -1.5165e-01,\n",
       "           1.3121e+01,  2.0227e-01],\n",
       "         [-4.8563e-02,  1.3263e-01, -1.7624e-01,  ..., -2.1772e-02,\n",
       "           1.2372e+01, -1.0019e-01],\n",
       "         [-2.4725e-01, -8.5116e-02, -2.1579e-01,  ...,  1.0976e-01,\n",
       "          -1.2576e+00,  2.3959e-02],\n",
       "         ...,\n",
       "         [ 6.3970e-02,  1.7621e-02, -3.6636e-02,  ..., -1.4575e-02,\n",
       "          -1.5215e+01,  7.4490e-03],\n",
       "         [-1.3190e-01,  5.0964e-02,  8.1512e-02,  ...,  1.8872e-02,\n",
       "          -4.2990e-01,  2.6202e-01],\n",
       "         [-1.8552e-01,  5.9508e-02,  1.2069e-01,  ...,  6.4129e-03,\n",
       "           6.4521e+00, -4.7724e-02]]),\n",
       " 'module.blocks.0.block.2.bias': tensor([ 0.2231,  0.1248, -0.0066,  ...,  0.0826,  0.0796, -0.0618]),\n",
       " 'module.blocks.1.block.0.weight': tensor([[ 1.5805e-01,  1.2721e-01,  2.4747e-01,  ..., -6.1117e-02,\n",
       "          -1.8927e-02,  2.2562e-02],\n",
       "         [-4.5649e+00, -1.0756e+01, -5.1982e+00,  ..., -1.0296e+01,\n",
       "           1.2773e+01,  5.2307e+00],\n",
       "         [-3.2303e-01,  2.6477e-02, -4.7228e-01,  ..., -2.8497e-03,\n",
       "           1.2643e-01, -1.6862e-01],\n",
       "         ...,\n",
       "         [-1.0330e-01, -1.9956e-01, -1.4170e-02,  ..., -1.8513e-01,\n",
       "          -1.8957e-01,  5.7182e-02],\n",
       "         [-4.4263e-03, -1.8471e-02, -2.3810e-01,  ..., -1.0053e-01,\n",
       "          -7.8911e-03,  3.2270e-01],\n",
       "         [-7.8252e-02,  8.8742e-03,  1.1597e-02,  ...,  1.6769e-01,\n",
       "          -3.4546e-01, -1.8271e-01]]),\n",
       " 'module.blocks.1.block.0.bias': tensor([ -0.0350, -15.9927,   0.0362,  ...,  -0.0368,  -0.2115,  -0.0833]),\n",
       " 'module.blocks.1.block.2.weight': tensor([[-2.2174e-01,  9.1575e+00,  1.3423e-01,  ...,  1.5716e-01,\n",
       "           1.5954e-01,  1.0721e-01],\n",
       "         [-4.5691e-02, -9.9904e+00, -5.5552e-02,  ...,  3.3170e-02,\n",
       "          -4.1374e-02, -1.3305e-01],\n",
       "         [-2.5440e-02, -3.7968e+00,  1.7103e-01,  ...,  3.7157e-02,\n",
       "           3.7250e-01,  8.9967e-02],\n",
       "         ...,\n",
       "         [ 5.3361e-02, -7.8918e+00, -1.0511e-01,  ...,  3.1066e-01,\n",
       "          -4.9761e-02, -1.9717e-02],\n",
       "         [-7.0248e-02,  8.5312e+00,  9.7379e-03,  ...,  4.3080e-01,\n",
       "           1.6155e-01,  2.2058e-01],\n",
       "         [ 3.4357e-02, -1.1590e+01, -1.2784e-02,  ...,  1.0441e-01,\n",
       "          -1.2664e-01,  5.7258e-02]]),\n",
       " 'module.blocks.1.block.2.bias': tensor([-0.0628, -0.0023,  0.1839,  ..., -0.1149, -0.1912,  0.1500]),\n",
       " 'module.blocks.2.block.0.weight': tensor([[-0.0401, -0.0463, -0.1020,  ...,  0.1745,  0.1535,  0.1457],\n",
       "         [-0.0418,  0.3385,  0.0483,  ...,  0.0028, -0.0623, -0.1773],\n",
       "         [-0.1768, -0.1277, -0.0892,  ..., -0.1049, -0.2704,  0.0574],\n",
       "         ...,\n",
       "         [ 0.0715, -0.0463,  0.3422,  ..., -0.0761,  0.0980, -0.3528],\n",
       "         [-0.0050,  0.1508,  0.1946,  ...,  0.0653, -0.0704, -0.2119],\n",
       "         [-0.1257,  0.0987, -0.1020,  ..., -0.3311, -0.0923,  0.0875]]),\n",
       " 'module.blocks.2.block.0.bias': tensor([-0.1835,  0.0248, -0.0889,  ..., -0.1258, -0.1065, -0.4614]),\n",
       " 'module.blocks.2.block.2.weight': tensor([[-0.0922,  0.2174, -0.1528,  ..., -0.2286, -0.1478,  0.2187],\n",
       "         [ 0.2245, -0.0996, -0.4313,  ...,  0.1023, -0.1211,  0.4575],\n",
       "         [ 0.1641, -0.0997,  0.0130,  ..., -0.2104, -0.3031, -0.0386],\n",
       "         ...,\n",
       "         [-0.3106,  0.0566, -0.0617,  ...,  0.0859, -0.2550, -0.4428],\n",
       "         [-0.1854,  0.2102, -0.4331,  ..., -0.1370, -0.1197, -0.2925],\n",
       "         [-0.3098, -0.0040,  0.1260,  ...,  0.4876,  0.1900, -0.1347]]),\n",
       " 'module.blocks.2.block.2.bias': tensor([ 0.0541,  0.2760, -0.0468,  ..., -0.1068, -0.0760,  0.0442]),\n",
       " 'module.blocks.3.block.0.weight': tensor([[-0.0149, -0.1589,  0.0622,  ..., -0.0914,  0.0261,  0.0955],\n",
       "         [-0.0862,  0.1411, -0.1872,  ..., -0.1349, -0.1502, -0.0568],\n",
       "         [ 0.0331,  0.2189,  0.3690,  ...,  0.0812,  0.0892,  0.0453],\n",
       "         ...,\n",
       "         [ 0.4206, -0.2235,  0.3928,  ..., -0.2843, -0.0754,  0.3149],\n",
       "         [-0.0903,  0.0102, -0.1087,  ...,  0.0047, -0.0815,  0.1806],\n",
       "         [ 0.0007, -0.2562, -0.3984,  ...,  0.0725, -0.0173,  0.0329]]),\n",
       " 'module.blocks.3.block.0.bias': tensor([-0.0444, -0.1055, -0.0827,  ...,  0.0153, -0.0643, -0.0482]),\n",
       " 'module.blocks.3.block.2.weight': tensor([[ 0.0025, -0.0875, -0.0262,  ..., -0.4101, -0.0492, -0.1684],\n",
       "         [ 0.0424,  0.0136,  0.1662,  ...,  0.0643,  0.1026,  0.2275],\n",
       "         [-0.0728, -0.0414, -0.1354,  ..., -0.0811, -0.1234,  0.2161],\n",
       "         ...,\n",
       "         [-0.3781,  0.0894,  0.1606,  ...,  0.1624, -0.1099,  0.2508],\n",
       "         [-0.0792,  0.0778, -0.2900,  ..., -0.0656, -0.1363, -0.0520],\n",
       "         [ 0.0254,  0.1960,  0.1438,  ..., -0.1249, -0.2361,  0.0886]]),\n",
       " 'module.blocks.3.block.2.bias': tensor([-0.0948, -0.1231,  0.1398,  ..., -0.0208, -0.0976,  0.2700]),\n",
       " 'module.blocks.4.block.0.weight': tensor([[-0.1049, -0.1305, -0.2855,  ..., -0.0542, -0.1488, -0.2262],\n",
       "         [ 0.1183, -0.0056,  0.0053,  ...,  0.2778,  0.1699,  0.4299],\n",
       "         [ 0.0561,  0.4976, -0.1264,  ..., -0.2325, -0.5417, -0.0406],\n",
       "         ...,\n",
       "         [-0.1623,  0.3207,  0.0237,  ...,  0.2013, -0.1697,  0.0517],\n",
       "         [-0.1941,  0.2952, -0.1786,  ..., -0.2742,  0.0324,  0.2371],\n",
       "         [-0.1229, -0.0871, -0.0038,  ..., -0.1492,  0.1354, -0.0576]]),\n",
       " 'module.blocks.4.block.0.bias': tensor([-0.1310, -0.0301, -0.0366,  ..., -0.2420, -0.0698,  0.1149]),\n",
       " 'module.blocks.4.block.2.weight': tensor([[-2.5173e-01,  1.5915e-01,  8.3085e-02,  ...,  5.6842e-02,\n",
       "           2.4823e-01,  7.4795e-02],\n",
       "         [-7.5000e-02, -7.6058e-02, -2.0464e-01,  ..., -1.2761e-01,\n",
       "          -1.2017e-01,  3.3189e-02],\n",
       "         [ 1.1113e-01,  8.2308e-02,  1.4933e-01,  ..., -4.1238e-02,\n",
       "           1.0231e-01,  1.0150e-01],\n",
       "         ...,\n",
       "         [ 1.5821e-01, -6.9496e-02,  2.3649e-02,  ...,  3.3394e-01,\n",
       "          -2.2377e-01,  1.0285e-01],\n",
       "         [-2.6443e-02, -2.9853e-01,  9.3541e-02,  ..., -2.3186e-01,\n",
       "           1.4372e-01, -2.2209e-01],\n",
       "         [ 3.9844e-02, -2.9001e-01,  1.2870e-01,  ..., -9.0114e-02,\n",
       "          -1.6892e-04,  1.0710e-01]]),\n",
       " 'module.blocks.4.block.2.bias': tensor([ 0.0576,  0.0713,  0.1490,  ..., -0.1096, -0.0439,  0.0933]),\n",
       " 'module.blocks.5.block.0.weight': tensor([[ 0.0433, -0.0102,  0.1682,  ...,  0.1891, -0.2939, -0.1417],\n",
       "         [-0.0361,  0.0929, -0.0713,  ...,  0.3671,  0.1447, -0.0469],\n",
       "         [ 0.0158,  0.0086,  0.3590,  ..., -0.0688,  0.2010, -0.0566],\n",
       "         ...,\n",
       "         [-0.1042,  0.0292, -0.0903,  ..., -0.1709,  0.0548, -0.0045],\n",
       "         [ 0.3617,  0.1008,  0.2209,  ...,  0.1903, -0.1174,  0.0040],\n",
       "         [ 0.3022, -0.0134, -0.2615,  ..., -0.1266, -0.2347,  0.1489]]),\n",
       " 'module.blocks.5.block.0.bias': tensor([-0.0612, -0.1720,  0.2523,  ...,  0.1828,  0.0049, -0.3810]),\n",
       " 'module.blocks.5.block.2.weight': tensor([[-0.1457, -0.0735,  0.0560,  ...,  0.0342, -0.2236,  0.0496],\n",
       "         [ 0.2704, -0.1691, -0.1121,  ..., -0.0444, -0.0889,  0.0113],\n",
       "         [-0.1391, -0.2265, -0.2293,  ...,  0.1369, -0.2971, -0.1346],\n",
       "         ...,\n",
       "         [-0.0325,  0.0118, -0.0784,  ...,  0.1964, -0.1227, -0.0334],\n",
       "         [ 0.2843, -0.0619, -0.5277,  ..., -0.1750,  0.0763,  0.0587],\n",
       "         [ 0.3160,  0.3021, -0.0541,  ..., -0.0644,  0.0752, -0.0881]]),\n",
       " 'module.blocks.5.block.2.bias': tensor([ 0.1512, -0.0741,  0.0560,  ..., -0.2392, -0.0169, -0.1515]),\n",
       " 'module.blocks.6.block.0.weight': tensor([[ 0.2789,  0.2492,  0.0623,  ...,  0.1983,  0.5675,  0.1864],\n",
       "         [ 0.0419, -0.1490, -0.1362,  ...,  0.1188, -0.2285, -0.0376],\n",
       "         [ 0.0506,  0.2802, -0.0599,  ...,  0.0633, -0.0984, -0.2974],\n",
       "         ...,\n",
       "         [-0.1737, -0.1943, -0.3088,  ..., -0.1296,  0.1287, -0.0828],\n",
       "         [-0.2191,  0.4009, -0.2403,  ..., -0.2184, -0.2566, -0.1832],\n",
       "         [-0.0420,  0.1972, -0.0740,  ...,  0.1082, -0.2329,  0.2128]]),\n",
       " 'module.blocks.6.block.0.bias': tensor([-0.2050, -0.0776, -0.0608,  ..., -0.0661, -0.1689, -0.0353]),\n",
       " 'module.blocks.6.block.2.weight': tensor([[ 0.4031, -0.0870, -0.0769,  ...,  0.2368, -0.0148,  0.1094],\n",
       "         [ 0.0380, -0.4580, -0.0924,  ..., -0.0738, -0.0709, -0.2268],\n",
       "         [ 0.0866,  0.0899,  0.2173,  ...,  0.0745,  0.1248,  0.0151],\n",
       "         ...,\n",
       "         [ 0.1775,  0.0206, -0.0639,  ...,  0.0946,  0.0309, -0.3480],\n",
       "         [ 0.1583,  0.4017, -0.1850,  ..., -0.0006,  0.0633,  0.1378],\n",
       "         [ 0.2393, -0.0512, -0.0965,  ...,  0.1852, -0.0739, -0.0981]]),\n",
       " 'module.blocks.6.block.2.bias': tensor([-0.1463, -0.0004, -0.0069,  ..., -0.1829,  0.1829,  0.1241]),\n",
       " 'module.blocks.7.block.0.weight': tensor([[ 0.0488,  0.2113, -0.2496,  ...,  0.1457, -0.1329,  0.2354],\n",
       "         [ 0.1208, -0.1310, -0.0271,  ..., -0.2565, -0.2476, -0.1141],\n",
       "         [ 0.0563, -0.0310, -0.1081,  ...,  0.1122, -0.0297, -0.3134],\n",
       "         ...,\n",
       "         [-0.1405, -0.1149, -0.1877,  ..., -0.0072, -0.0641,  0.0024],\n",
       "         [ 0.2687,  0.0417,  0.1017,  ..., -0.1451,  0.2974,  0.0036],\n",
       "         [ 0.1827, -0.1825,  0.0824,  ...,  0.0724, -0.1122,  0.0502]]),\n",
       " 'module.blocks.7.block.0.bias': tensor([-0.0607, -0.3520, -0.3511,  ..., -0.1786, -0.1535, -0.0389]),\n",
       " 'module.blocks.7.block.2.weight': tensor([[ 0.0372,  0.2423,  0.3225,  ..., -0.0641,  0.0167, -0.1562],\n",
       "         [-0.4528, -0.0681,  0.1955,  ...,  0.1321, -0.1680,  0.0229],\n",
       "         [ 0.2625,  0.3051,  0.0210,  ...,  0.2845, -0.0358,  0.2679],\n",
       "         ...,\n",
       "         [-0.0628,  0.0697,  0.0912,  ...,  0.0693, -0.3235, -0.1028],\n",
       "         [ 0.0371,  0.0135,  0.3312,  ...,  0.2222,  0.1602, -0.1310],\n",
       "         [-0.3948, -0.0661,  0.1588,  ..., -0.0406,  0.0803, -0.0985]]),\n",
       " 'module.blocks.7.block.2.bias': tensor([-0.2331, -0.0685,  0.0787,  ..., -0.1488, -0.1162, -0.0125]),\n",
       " 'module.blocks.8.block.0.weight': tensor([[-0.1329,  0.1128,  0.2585,  ...,  0.3509,  0.3478, -0.2969],\n",
       "         [-0.4062, -0.2123, -0.2801,  ..., -0.2698,  0.1264, -0.0102],\n",
       "         [ 0.0026,  0.0012, -0.0355,  ...,  0.0800,  0.0818,  0.3289],\n",
       "         ...,\n",
       "         [ 0.0878, -0.2870, -0.2456,  ...,  0.3155, -0.2911,  0.1864],\n",
       "         [-0.0044, -0.0910, -0.2187,  ..., -0.1939,  0.2040,  0.0505],\n",
       "         [-0.0659, -0.0759, -0.1948,  ...,  0.0603, -0.2600, -0.1939]]),\n",
       " 'module.blocks.8.block.0.bias': tensor([-0.1521, -0.5604, -0.0167,  ..., -0.0442, -0.1698, -0.1569]),\n",
       " 'module.blocks.8.block.2.weight': tensor([[ 0.4239, -0.5219,  0.0835,  ..., -0.1390, -0.1786,  0.2612],\n",
       "         [ 0.0882,  0.3534,  0.2334,  ...,  0.1583, -0.1816, -0.0227],\n",
       "         [-0.1304, -0.1955,  0.2400,  ...,  0.0651, -0.0893,  0.0124],\n",
       "         ...,\n",
       "         [-0.1140, -0.4710,  0.1111,  ..., -0.3501,  0.0589, -0.2795],\n",
       "         [-0.0631,  0.0969, -0.1033,  ...,  0.2768,  0.1021, -0.4507],\n",
       "         [ 0.2577,  0.6523, -0.1738,  ..., -0.1902,  0.1051, -0.2363]]),\n",
       " 'module.blocks.8.block.2.bias': tensor([ 0.5705, -0.1700,  0.0718,  ...,  0.2826,  0.0277, -0.2006]),\n",
       " 'module.blocks.9.block.0.weight': tensor([[-5.4681e-01,  1.7233e-01, -2.3662e-01,  ...,  7.6808e-02,\n",
       "          -1.3844e-01, -2.1736e-01],\n",
       "         [-2.3053e-01,  2.2240e-01,  1.7622e-01,  ..., -2.0501e-01,\n",
       "          -2.0087e-01, -1.1129e-05],\n",
       "         [-2.4110e-01,  1.1053e-01, -3.0185e-01,  ..., -3.1189e-03,\n",
       "          -3.1544e-01, -2.5933e-01],\n",
       "         ...,\n",
       "         [-3.6606e-01,  3.0700e-01,  7.9647e-02,  ..., -5.7641e-02,\n",
       "          -4.2583e-02, -2.0506e-01],\n",
       "         [ 3.0240e-02, -2.7110e-01,  4.9311e-03,  ...,  8.5930e-03,\n",
       "           3.9594e-01, -1.6317e-02],\n",
       "         [-8.8926e-02,  2.9905e-01, -1.2803e-01,  ..., -8.8166e-02,\n",
       "          -8.0361e-02, -2.2465e-02]]),\n",
       " 'module.blocks.9.block.0.bias': tensor([-0.1209, -0.1316, -0.1077,  ..., -0.1076, -0.0539, -0.1108]),\n",
       " 'module.blocks.9.block.2.weight': tensor([[-0.1729,  0.2911,  0.0305,  ...,  0.1897,  0.0710, -0.2637],\n",
       "         [ 0.1845,  0.0675, -0.2104,  ..., -0.2019,  0.2738,  0.0769],\n",
       "         [ 0.0414, -0.2912,  0.2329,  ..., -0.1551,  0.1065, -0.2333],\n",
       "         ...,\n",
       "         [-0.1732,  0.0322,  0.1817,  ...,  0.0914,  0.0264, -0.0509],\n",
       "         [ 0.0146,  0.0947,  0.3782,  ..., -0.1059, -0.2776, -0.3160],\n",
       "         [-0.0592,  0.2106,  0.2623,  ...,  0.2718,  0.3792, -0.2180]]),\n",
       " 'module.blocks.9.block.2.bias': tensor([ 0.3617, -0.1231,  0.0051,  ...,  0.4921, -0.3412, -0.4197]),\n",
       " 'module.blocks.10.block.0.weight': tensor([[-0.1259, -0.2608,  0.0318,  ...,  0.1310, -0.2084,  0.3234],\n",
       "         [-0.0597, -0.2142,  0.2485,  ..., -0.1086, -0.1105, -0.1124],\n",
       "         [-0.2514, -0.0705, -0.0705,  ..., -0.0954, -0.3634, -0.0395],\n",
       "         ...,\n",
       "         [-0.0895,  0.0642,  0.0132,  ..., -0.1944, -0.0343,  0.0579],\n",
       "         [-0.1464, -0.1184,  0.5011,  ..., -0.0938, -0.0509,  0.2741],\n",
       "         [-0.4393, -0.1006, -0.1088,  ...,  0.0229,  0.2068,  0.1248]]),\n",
       " 'module.blocks.10.block.0.bias': tensor([-0.1646, -0.1190, -0.1184,  ..., -0.1149, -0.0544, -0.1384]),\n",
       " 'module.blocks.10.block.2.weight': tensor([[-0.0707, -0.2212,  0.0088,  ...,  0.0052, -0.0012,  0.4452],\n",
       "         [-0.1535, -0.0938,  0.2192,  ...,  0.2728, -0.0158,  0.3178],\n",
       "         [ 0.0045,  0.1536,  0.0682,  ..., -0.3985, -0.1143, -0.1686],\n",
       "         ...,\n",
       "         [-0.1344,  0.0571, -0.4595,  ..., -0.0474, -0.1225,  0.2309],\n",
       "         [-0.0859, -0.2490,  0.0189,  ...,  0.2357, -0.1344, -0.0667],\n",
       "         [-0.0505, -0.1259,  0.1689,  ..., -0.1016, -0.1179, -0.2958]]),\n",
       " 'module.blocks.10.block.2.bias': tensor([-0.1881, -0.1191, -0.1331,  ...,  0.2857,  0.2426, -0.0808]),\n",
       " 'module.blocks.11.block.0.weight': tensor([[ 0.3498,  0.3685, -0.0771,  ..., -0.1898,  0.2694,  0.0108],\n",
       "         [ 0.1173,  0.0034,  0.2116,  ...,  0.0707, -0.1339, -0.0334],\n",
       "         [ 0.3335,  0.3951,  0.0677,  ..., -0.2024,  0.1051,  0.1829],\n",
       "         ...,\n",
       "         [ 0.2038, -0.0376, -0.0153,  ...,  0.0005, -0.1825, -0.0800],\n",
       "         [ 0.1062, -0.2900, -0.3776,  ...,  0.1351,  0.1971, -0.0456],\n",
       "         [ 0.0198,  0.2003,  0.1895,  ...,  0.1400,  0.0239,  0.1355]]),\n",
       " 'module.blocks.11.block.0.bias': tensor([-0.3503, -0.0335, -1.0195,  ..., -0.0114,  0.0095, -0.0758]),\n",
       " 'module.blocks.11.block.2.weight': tensor([[ 3.9136e-01,  2.0257e-01, -1.2120e-01,  ..., -1.6126e-01,\n",
       "          -6.9070e-02,  6.2569e-03],\n",
       "         [ 1.3570e-04, -1.5888e-01, -3.4307e-01,  ..., -1.8210e-01,\n",
       "           9.7553e-02,  8.3246e-02],\n",
       "         [ 8.8605e-02,  4.7405e-02, -8.0760e-02,  ...,  1.4525e-01,\n",
       "           1.6544e-01, -1.2190e-01],\n",
       "         ...,\n",
       "         [ 3.1106e-02,  1.0694e-01,  8.5001e-04,  ...,  1.2493e-01,\n",
       "          -6.5309e-02, -9.3870e-02],\n",
       "         [ 5.1937e-02,  1.3546e-01,  4.2549e-02,  ...,  1.7703e-01,\n",
       "          -5.5576e-02, -2.0335e-01],\n",
       "         [ 1.1598e-01, -1.1772e-01, -2.3812e-01,  ..., -1.6924e-01,\n",
       "          -8.2993e-02, -3.5532e-02]]),\n",
       " 'module.blocks.11.block.2.bias': tensor([ 0.0025,  0.1024,  0.1288,  ...,  0.1316,  0.1059, -0.0692]),\n",
       " 'module.layernorms.0.weight': tensor([0.3462, 0.2206, 0.2124,  ..., 0.2532, 0.1970, 0.1850]),\n",
       " 'module.layernorms.0.bias': tensor([-0.0758, -0.0272,  0.0182,  ..., -0.0301,  0.0094,  0.0490]),\n",
       " 'module.layernorms.1.weight': tensor([0.4872, 0.5277, 0.4854,  ..., 0.5822, 0.7441, 0.5769]),\n",
       " 'module.layernorms.1.bias': tensor([-0.4408, -0.3070,  0.2620,  ..., -0.1967, -0.5229,  0.1637]),\n",
       " 'module.layernorms.2.weight': tensor([0.8940, 0.8926, 0.8699,  ..., 0.9129, 0.9220, 0.8844]),\n",
       " 'module.layernorms.2.bias': tensor([-0.0990, -0.0691,  0.2487,  ...,  0.1285, -0.1782,  0.0909]),\n",
       " 'module.layernorms.3.weight': tensor([0.8122, 0.7996, 0.8641,  ..., 0.8786, 0.8450, 0.8884]),\n",
       " 'module.layernorms.3.bias': tensor([-0.2907, -0.3874,  0.2169,  ..., -0.1600, -0.1326,  0.0014]),\n",
       " 'module.layernorms.4.weight': tensor([0.7250, 0.8961, 0.8889,  ..., 0.8510, 0.8953, 0.9786]),\n",
       " 'module.layernorms.4.bias': tensor([-0.2527, -0.2054, -0.1907,  ..., -0.0262, -0.2111, -0.1264]),\n",
       " 'module.layernorms.5.weight': tensor([0.6619, 0.9248, 0.9464,  ..., 0.9331, 0.9304, 0.9398]),\n",
       " 'module.layernorms.5.bias': tensor([-0.2507, -0.1164, -0.2353,  ..., -0.0676, -0.1726, -0.0612]),\n",
       " 'module.layernorms.6.weight': tensor([0.7985, 1.0012, 0.9667,  ..., 1.0386, 1.0886, 1.1030]),\n",
       " 'module.layernorms.6.bias': tensor([-0.0493,  0.0159, -0.2879,  ...,  0.0044,  0.0504,  0.1249]),\n",
       " 'module.layernorms.7.weight': tensor([0.5662, 0.8900, 0.8827,  ..., 0.8112, 0.7658, 0.9159]),\n",
       " 'module.layernorms.7.bias': tensor([ 0.4042, -0.0552,  0.0860,  ...,  0.0921,  0.2659,  0.1072]),\n",
       " 'module.layernorms.8.weight': tensor([0.3534, 0.8476, 0.7647,  ..., 0.8002, 0.6811, 0.9719]),\n",
       " 'module.layernorms.8.bias': tensor([0.6891, 0.1136, 0.2119,  ..., 0.1988, 0.4895, 0.1491]),\n",
       " 'module.layernorms.9.weight': tensor([0.5258, 0.8001, 0.8131,  ..., 0.7431, 0.7215, 0.7230]),\n",
       " 'module.layernorms.9.bias': tensor([ 0.3274,  0.0666,  0.0127,  ...,  0.0796,  0.2011, -0.1690]),\n",
       " 'module.layernorms.10.weight': tensor([0.7109, 0.9358, 0.9428,  ..., 0.9312, 0.6910, 0.8972]),\n",
       " 'module.layernorms.10.bias': tensor([0.1473, 0.1155, 0.0568,  ..., 0.0393, 0.4727, 0.0775]),\n",
       " 'module.layernorms.11.weight': tensor([0.5621, 0.5854, 0.6019,  ..., 0.6596, 0.5826, 0.5692]),\n",
       " 'module.layernorms.11.bias': tensor([-0.0735,  0.0330, -0.0415,  ..., -0.0570,  0.0869,  0.0243])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Load_state output', self.load_state_dict(params, strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of MLP_Wrapper(\n",
       "  (model): BottleneckMLP(\n",
       "    (linear_in): Linear(in_features=12288, out_features=1024, bias=True)\n",
       "    (linear_out): Linear(in_features=1024, out_features=200, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x BottleneckBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorms): ModuleList(\n",
       "      (0-11): 12 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (resize): Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'optimizer', 'scaler', 'epoch', 'best_acc'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd2 = torch.load('checkpoints/B-12_Wi-1024_res_64_cifar10_epochs_20_weights', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_in.weight',\n",
       "              tensor([[ 2.2766e-01,  2.9493e-01,  2.6623e-03,  ..., -1.0544e-01,\n",
       "                        2.2706e-02,  8.5756e-02],\n",
       "                      [-2.3604e-01, -5.2510e-02, -4.1368e-02,  ..., -1.1234e-01,\n",
       "                       -4.4782e-01, -4.3150e-01],\n",
       "                      [-4.5428e-01, -4.4719e-01, -2.9147e-01,  ...,  1.3970e-01,\n",
       "                        2.5342e-01,  3.6002e-01],\n",
       "                      ...,\n",
       "                      [ 1.5980e-02, -3.5570e-01,  1.7743e-02,  ..., -2.6705e-01,\n",
       "                       -5.2203e-01, -2.3121e-01],\n",
       "                      [ 3.9012e-01,  1.9785e-01,  4.5613e-01,  ...,  2.0843e-01,\n",
       "                        4.7058e-01,  3.9061e-01],\n",
       "                      [-1.0476e-02,  2.8890e-04,  6.7830e-02,  ..., -3.1759e-02,\n",
       "                       -1.3554e-02, -9.1598e-02]])),\n",
       "             ('linear_in.bias',\n",
       "              tensor([ 1.0531,  0.4436, -0.6995,  ...,  0.7111,  1.1395, -0.3991])),\n",
       "             ('linear_out.weight',\n",
       "              tensor([[ 0.0211,  0.0066, -0.0079,  ..., -0.0030, -0.0046, -0.0085],\n",
       "                      [ 0.0076,  0.0070, -0.0181,  ..., -0.0016,  0.0004, -0.0042],\n",
       "                      [-0.0109,  0.0144,  0.0316,  ..., -0.0082, -0.0122, -0.0073],\n",
       "                      ...,\n",
       "                      [-0.0097,  0.0015,  0.0065,  ..., -0.0098, -0.0028, -0.0071],\n",
       "                      [-0.0104, -0.0036,  0.0049,  ...,  0.0244, -0.0011, -0.0194],\n",
       "                      [-0.0135, -0.0043, -0.0081,  ..., -0.0003,  0.0052, -0.0181]])),\n",
       "             ('linear_out.bias',\n",
       "              tensor([ 0.0275,  0.0021, -0.0119,  0.0264, -0.0284,  0.0098, -0.0212,  0.0159,\n",
       "                      -0.0252,  0.0131])),\n",
       "             ('blocks.0.block.0.weight',\n",
       "              tensor([[ 2.4390e-02,  3.9078e-02,  8.8151e-02,  ..., -3.7553e-02,\n",
       "                        1.1398e-01,  1.5030e-01],\n",
       "                      [ 4.2020e-02, -1.8807e-01, -7.4436e-02,  ...,  9.9966e-02,\n",
       "                        7.3992e-02, -1.1015e-01],\n",
       "                      [-6.9535e-02, -1.1096e-01, -2.0458e-01,  ...,  1.0783e-02,\n",
       "                        1.8029e-01,  1.2622e-02],\n",
       "                      ...,\n",
       "                      [ 5.5725e-02,  5.6394e-02,  3.2873e-02,  ...,  8.4056e-02,\n",
       "                        4.4604e-02,  1.1359e-01],\n",
       "                      [-4.4984e+00, -1.0060e+00, -1.3659e+00,  ...,  2.5324e+00,\n",
       "                       -1.2361e+01,  1.5667e+00],\n",
       "                      [-1.5739e-01,  5.3330e-02, -1.4360e-01,  ..., -6.5392e-03,\n",
       "                       -1.5871e-01,  7.0274e-02]])),\n",
       "             ('blocks.0.block.0.bias',\n",
       "              tensor([-9.7620e-02, -3.3814e-01, -5.6706e-01,  ..., -5.3952e-03,\n",
       "                      -1.7033e+01, -1.3509e-01])),\n",
       "             ('blocks.0.block.2.weight',\n",
       "              tensor([[ 7.6268e-02, -4.2277e-02,  2.4125e-02,  ..., -1.5100e-01,\n",
       "                        1.3117e+01,  2.0212e-01],\n",
       "                      [-4.8444e-02,  1.3314e-01, -1.7605e-01,  ..., -2.2260e-02,\n",
       "                        1.2369e+01, -9.9792e-02],\n",
       "                      [-2.4869e-01, -8.5573e-02, -2.1541e-01,  ...,  1.0829e-01,\n",
       "                       -1.2573e+00,  2.3891e-02],\n",
       "                      ...,\n",
       "                      [ 6.3602e-02,  1.7928e-02, -3.6147e-02,  ..., -1.3794e-02,\n",
       "                       -1.5212e+01,  6.6146e-03],\n",
       "                      [-1.3237e-01,  5.0180e-02,  8.1924e-02,  ...,  1.8482e-02,\n",
       "                       -4.2979e-01,  2.6373e-01],\n",
       "                      [-1.8508e-01,  6.0538e-02,  1.1938e-01,  ...,  5.8624e-03,\n",
       "                        6.4507e+00, -4.6804e-02]])),\n",
       "             ('blocks.0.block.2.bias',\n",
       "              tensor([ 0.2221,  0.1245, -0.0065,  ...,  0.0827,  0.0793, -0.0621])),\n",
       "             ('blocks.1.block.0.weight',\n",
       "              tensor([[ 1.5992e-01,  1.2736e-01,  2.4778e-01,  ..., -6.0952e-02,\n",
       "                       -1.8430e-02,  2.1070e-02],\n",
       "                      [-4.5637e+00, -1.0754e+01, -5.1970e+00,  ..., -1.0294e+01,\n",
       "                        1.2770e+01,  5.2295e+00],\n",
       "                      [-3.2326e-01,  2.4740e-02, -4.7431e-01,  ..., -4.9848e-03,\n",
       "                        1.2505e-01, -1.6805e-01],\n",
       "                      ...,\n",
       "                      [-1.0255e-01, -2.0116e-01, -1.4471e-02,  ..., -1.8441e-01,\n",
       "                       -1.8931e-01,  5.8654e-02],\n",
       "                      [-6.8732e-03, -1.5880e-02, -2.3716e-01,  ..., -1.0370e-01,\n",
       "                       -7.5564e-03,  3.2345e-01],\n",
       "                      [-7.9370e-02,  7.5774e-03,  1.0472e-02,  ...,  1.6738e-01,\n",
       "                       -3.4496e-01, -1.8311e-01]])),\n",
       "             ('blocks.1.block.0.bias',\n",
       "              tensor([ -0.0358, -15.9889,   0.0378,  ...,  -0.0355,  -0.2105,  -0.0831])),\n",
       "             ('blocks.1.block.2.weight',\n",
       "              tensor([[-2.1946e-01,  9.1551e+00,  1.3271e-01,  ...,  1.5631e-01,\n",
       "                        1.6192e-01,  1.0931e-01],\n",
       "                      [-4.4778e-02, -9.9880e+00, -5.5855e-02,  ...,  3.3596e-02,\n",
       "                       -4.1973e-02, -1.3096e-01],\n",
       "                      [-2.5010e-02, -3.7958e+00,  1.7106e-01,  ...,  3.9055e-02,\n",
       "                        3.7028e-01,  8.9050e-02],\n",
       "                      ...,\n",
       "                      [ 5.2773e-02, -7.8899e+00, -1.0580e-01,  ...,  3.1125e-01,\n",
       "                       -5.0866e-02, -1.9785e-02],\n",
       "                      [-6.8466e-02,  8.5293e+00,  9.4732e-03,  ...,  4.3088e-01,\n",
       "                        1.6183e-01,  2.1849e-01],\n",
       "                      [ 3.5556e-02, -1.1587e+01, -1.1348e-02,  ...,  1.0573e-01,\n",
       "                       -1.2484e-01,  5.9882e-02]])),\n",
       "             ('blocks.1.block.2.bias',\n",
       "              tensor([-0.0629, -0.0024,  0.1838,  ..., -0.1146, -0.1916,  0.1499])),\n",
       "             ('blocks.2.block.0.weight',\n",
       "              tensor([[-0.0389, -0.0443, -0.1026,  ...,  0.1763,  0.1530,  0.1447],\n",
       "                      [-0.0431,  0.3360,  0.0473,  ...,  0.0031, -0.0601, -0.1786],\n",
       "                      [-0.1770, -0.1278, -0.0891,  ..., -0.1049, -0.2702,  0.0572],\n",
       "                      ...,\n",
       "                      [ 0.0730, -0.0450,  0.3429,  ..., -0.0776,  0.0994, -0.3519],\n",
       "                      [-0.0037,  0.1523,  0.1946,  ...,  0.0642, -0.0695, -0.2109],\n",
       "                      [-0.1243,  0.0992, -0.1016,  ..., -0.3294, -0.0941,  0.0915]])),\n",
       "             ('blocks.2.block.0.bias',\n",
       "              tensor([-0.1818,  0.0230, -0.0898,  ..., -0.1252, -0.1066, -0.4593])),\n",
       "             ('blocks.2.block.2.weight',\n",
       "              tensor([[-0.0911,  0.2175, -0.1541,  ..., -0.2286, -0.1481,  0.2191],\n",
       "                      [ 0.2228, -0.0992, -0.4315,  ...,  0.1031, -0.1210,  0.4573],\n",
       "                      [ 0.1652, -0.0988,  0.0121,  ..., -0.2083, -0.3039, -0.0386],\n",
       "                      ...,\n",
       "                      [-0.3122,  0.0556, -0.0619,  ...,  0.0853, -0.2550, -0.4419],\n",
       "                      [-0.1847,  0.2081, -0.4333,  ..., -0.1355, -0.1178, -0.2909],\n",
       "                      [-0.3105, -0.0028,  0.1256,  ...,  0.4874,  0.1898, -0.1328]])),\n",
       "             ('blocks.2.block.2.bias',\n",
       "              tensor([ 0.0545,  0.2762, -0.0465,  ..., -0.1069, -0.0761,  0.0439])),\n",
       "             ('blocks.3.block.0.weight',\n",
       "              tensor([[-1.2945e-02, -1.6108e-01,  6.2980e-02,  ..., -8.8516e-02,\n",
       "                        2.5318e-02,  9.8071e-02],\n",
       "                      [-8.7076e-02,  1.4123e-01, -1.8861e-01,  ..., -1.3551e-01,\n",
       "                       -1.5286e-01, -5.7323e-02],\n",
       "                      [ 3.2014e-02,  2.1681e-01,  3.6905e-01,  ...,  8.3320e-02,\n",
       "                        8.9034e-02,  4.6305e-02],\n",
       "                      ...,\n",
       "                      [ 4.2181e-01, -2.2214e-01,  3.9434e-01,  ..., -2.8348e-01,\n",
       "                       -7.4228e-02,  3.1351e-01],\n",
       "                      [-8.7238e-02,  1.2275e-02, -1.0860e-01,  ...,  7.0274e-03,\n",
       "                       -8.3183e-02,  1.7974e-01],\n",
       "                      [ 8.1167e-05, -2.5598e-01, -4.0049e-01,  ...,  7.1013e-02,\n",
       "                       -1.8904e-02,  2.9805e-02]])),\n",
       "             ('blocks.3.block.0.bias',\n",
       "              tensor([-0.0448, -0.1017, -0.0821,  ...,  0.0156, -0.0640, -0.0484])),\n",
       "             ('blocks.3.block.2.weight',\n",
       "              tensor([[ 0.0046, -0.0887, -0.0262,  ..., -0.4092, -0.0501, -0.1679],\n",
       "                      [ 0.0440,  0.0158,  0.1671,  ...,  0.0663,  0.1016,  0.2294],\n",
       "                      [-0.0736, -0.0383, -0.1375,  ..., -0.0805, -0.1225,  0.2170],\n",
       "                      ...,\n",
       "                      [-0.3779,  0.0895,  0.1611,  ...,  0.1643, -0.1121,  0.2491],\n",
       "                      [-0.0795,  0.0770, -0.2908,  ..., -0.0673, -0.1340, -0.0546],\n",
       "                      [ 0.0247,  0.1946,  0.1441,  ..., -0.1249, -0.2367,  0.0882]])),\n",
       "             ('blocks.3.block.2.bias',\n",
       "              tensor([-0.0946, -0.1222,  0.1405,  ..., -0.0215, -0.0979,  0.2691])),\n",
       "             ('blocks.4.block.0.weight',\n",
       "              tensor([[-0.1047, -0.1301, -0.2838,  ..., -0.0521, -0.1497, -0.2280],\n",
       "                      [ 0.1181, -0.0072,  0.0054,  ...,  0.2791,  0.1709,  0.4303],\n",
       "                      [ 0.0565,  0.4970, -0.1254,  ..., -0.2336, -0.5402, -0.0416],\n",
       "                      ...,\n",
       "                      [-0.1644,  0.3214,  0.0268,  ...,  0.2028, -0.1710,  0.0557],\n",
       "                      [-0.1948,  0.2967, -0.1763,  ..., -0.2748,  0.0310,  0.2389],\n",
       "                      [-0.1208, -0.0860, -0.0037,  ..., -0.1478,  0.1361, -0.0592]])),\n",
       "             ('blocks.4.block.0.bias',\n",
       "              tensor([-0.1317, -0.0300, -0.0367,  ..., -0.2449, -0.0705,  0.1157])),\n",
       "             ('blocks.4.block.2.weight',\n",
       "              tensor([[-2.5192e-01,  1.5877e-01,  8.4830e-02,  ...,  5.7725e-02,\n",
       "                        2.4875e-01,  7.5609e-02],\n",
       "                      [-7.3177e-02, -7.4829e-02, -2.0356e-01,  ..., -1.2677e-01,\n",
       "                       -1.1882e-01,  3.2763e-02],\n",
       "                      [ 1.1219e-01,  8.2803e-02,  1.4997e-01,  ..., -4.1641e-02,\n",
       "                        9.9312e-02,  9.9711e-02],\n",
       "                      ...,\n",
       "                      [ 1.5803e-01, -6.9475e-02,  2.2888e-02,  ...,  3.3608e-01,\n",
       "                       -2.1960e-01,  1.0070e-01],\n",
       "                      [-2.5818e-02, -3.0003e-01,  9.4203e-02,  ..., -2.3497e-01,\n",
       "                        1.4506e-01, -2.2137e-01],\n",
       "                      [ 3.9449e-02, -2.8767e-01,  1.3161e-01,  ..., -8.7257e-02,\n",
       "                        2.3371e-04,  1.0701e-01]])),\n",
       "             ('blocks.4.block.2.bias',\n",
       "              tensor([ 0.0583,  0.0713,  0.1498,  ..., -0.1106, -0.0441,  0.0936])),\n",
       "             ('blocks.5.block.0.weight',\n",
       "              tensor([[ 0.0426, -0.0103,  0.1684,  ...,  0.1894, -0.2931, -0.1418],\n",
       "                      [-0.0371,  0.0939, -0.0738,  ...,  0.3637,  0.1435, -0.0453],\n",
       "                      [ 0.0117,  0.0085,  0.3580,  ..., -0.0657,  0.2033, -0.0588],\n",
       "                      ...,\n",
       "                      [-0.1036,  0.0291, -0.0869,  ..., -0.1705,  0.0569, -0.0035],\n",
       "                      [ 0.3606,  0.0988,  0.2213,  ...,  0.1889, -0.1169,  0.0030],\n",
       "                      [ 0.3046, -0.0128, -0.2584,  ..., -0.1269, -0.2355,  0.1517]])),\n",
       "             ('blocks.5.block.0.bias',\n",
       "              tensor([-0.0614, -0.1706,  0.2507,  ...,  0.1828,  0.0062, -0.3788])),\n",
       "             ('blocks.5.block.2.weight',\n",
       "              tensor([[-0.1450, -0.0751,  0.0556,  ...,  0.0334, -0.2237,  0.0499],\n",
       "                      [ 0.2736, -0.1673, -0.1127,  ..., -0.0460, -0.0903,  0.0113],\n",
       "                      [-0.1390, -0.2284, -0.2286,  ...,  0.1402, -0.2969, -0.1366],\n",
       "                      ...,\n",
       "                      [-0.0322,  0.0123, -0.0767,  ...,  0.1970, -0.1223, -0.0330],\n",
       "                      [ 0.2825, -0.0631, -0.5269,  ..., -0.1766,  0.0748,  0.0623],\n",
       "                      [ 0.3151,  0.3026, -0.0555,  ..., -0.0640,  0.0742, -0.0885]])),\n",
       "             ('blocks.5.block.2.bias',\n",
       "              tensor([ 0.1520, -0.0737,  0.0577,  ..., -0.2397, -0.0180, -0.1517])),\n",
       "             ('blocks.6.block.0.weight',\n",
       "              tensor([[ 0.2786,  0.2476,  0.0657,  ...,  0.2007,  0.5699,  0.1856],\n",
       "                      [ 0.0397, -0.1472, -0.1374,  ...,  0.1184, -0.2295, -0.0362],\n",
       "                      [ 0.0524,  0.2803, -0.0625,  ...,  0.0607, -0.0976, -0.2975],\n",
       "                      ...,\n",
       "                      [-0.1699, -0.1897, -0.3114,  ..., -0.1313,  0.1287, -0.0808],\n",
       "                      [-0.2224,  0.4010, -0.2383,  ..., -0.2161, -0.2547, -0.1878],\n",
       "                      [-0.0450,  0.1971, -0.0734,  ...,  0.1086, -0.2340,  0.2153]])),\n",
       "             ('blocks.6.block.0.bias',\n",
       "              tensor([-0.2073, -0.0763, -0.0610,  ..., -0.0657, -0.1720, -0.0349])),\n",
       "             ('blocks.6.block.2.weight',\n",
       "              tensor([[ 0.4024, -0.0871, -0.0775,  ...,  0.2375, -0.0145,  0.1095],\n",
       "                      [ 0.0359, -0.4595, -0.0948,  ..., -0.0721, -0.0709, -0.2224],\n",
       "                      [ 0.0874,  0.0906,  0.2169,  ...,  0.0757,  0.1228,  0.0140],\n",
       "                      ...,\n",
       "                      [ 0.1797,  0.0195, -0.0651,  ...,  0.0952,  0.0310, -0.3493],\n",
       "                      [ 0.1581,  0.4016, -0.1864,  ..., -0.0019,  0.0678,  0.1391],\n",
       "                      [ 0.2396, -0.0514, -0.0949,  ...,  0.1856, -0.0765, -0.0964]])),\n",
       "             ('blocks.6.block.2.bias',\n",
       "              tensor([-0.1448,  0.0006, -0.0067,  ..., -0.1841,  0.1827,  0.1230])),\n",
       "             ('blocks.7.block.0.weight',\n",
       "              tensor([[ 0.0473,  0.2151, -0.2508,  ...,  0.1482, -0.1334,  0.2343],\n",
       "                      [ 0.1193, -0.1316, -0.0283,  ..., -0.2575, -0.2519, -0.1143],\n",
       "                      [ 0.0556, -0.0321, -0.1045,  ...,  0.1085, -0.0294, -0.3142],\n",
       "                      ...,\n",
       "                      [-0.1416, -0.1175, -0.1841,  ..., -0.0041, -0.0629,  0.0065],\n",
       "                      [ 0.2658,  0.0376,  0.1045,  ..., -0.1432,  0.2968,  0.0015],\n",
       "                      [ 0.1808, -0.1803,  0.0849,  ...,  0.0701, -0.1139,  0.0519]])),\n",
       "             ('blocks.7.block.0.bias',\n",
       "              tensor([-0.0613, -0.3537, -0.3522,  ..., -0.1770, -0.1538, -0.0415])),\n",
       "             ('blocks.7.block.2.weight',\n",
       "              tensor([[ 0.0374,  0.2426,  0.3209,  ..., -0.0663,  0.0181, -0.1576],\n",
       "                      [-0.4517, -0.0684,  0.1979,  ...,  0.1306, -0.1668,  0.0194],\n",
       "                      [ 0.2609,  0.3077,  0.0205,  ...,  0.2848, -0.0359,  0.2718],\n",
       "                      ...,\n",
       "                      [-0.0644,  0.0715,  0.0929,  ...,  0.0699, -0.3269, -0.1032],\n",
       "                      [ 0.0350,  0.0153,  0.3318,  ...,  0.2231,  0.1605, -0.1320],\n",
       "                      [-0.3927, -0.0633,  0.1568,  ..., -0.0398,  0.0805, -0.1005]])),\n",
       "             ('blocks.7.block.2.bias',\n",
       "              tensor([-0.2313, -0.0671,  0.0772,  ..., -0.1492, -0.1166, -0.0133])),\n",
       "             ('blocks.8.block.0.weight',\n",
       "              tensor([[-0.1348,  0.1110,  0.2569,  ...,  0.3512,  0.3491, -0.2957],\n",
       "                      [-0.4054, -0.2112, -0.2833,  ..., -0.2701,  0.1225, -0.0107],\n",
       "                      [ 0.0024, -0.0007, -0.0346,  ...,  0.0798,  0.0823,  0.3293],\n",
       "                      ...,\n",
       "                      [ 0.0901, -0.2876, -0.2466,  ...,  0.3140, -0.2890,  0.1871],\n",
       "                      [-0.0038, -0.0907, -0.2180,  ..., -0.1932,  0.2042,  0.0507],\n",
       "                      [-0.0624, -0.0773, -0.2004,  ...,  0.0625, -0.2587, -0.1924]])),\n",
       "             ('blocks.8.block.0.bias',\n",
       "              tensor([-0.1507, -0.5617, -0.0176,  ..., -0.0448, -0.1694, -0.1541])),\n",
       "             ('blocks.8.block.2.weight',\n",
       "              tensor([[ 0.4218, -0.5218,  0.0844,  ..., -0.1393, -0.1765,  0.2631],\n",
       "                      [ 0.0867,  0.3535,  0.2358,  ...,  0.1577, -0.1800, -0.0224],\n",
       "                      [-0.1292, -0.1963,  0.2420,  ...,  0.0664, -0.0888,  0.0147],\n",
       "                      ...,\n",
       "                      [-0.1150, -0.4716,  0.1097,  ..., -0.3523,  0.0566, -0.2764],\n",
       "                      [-0.0608,  0.0986, -0.1039,  ...,  0.2752,  0.1022, -0.4459],\n",
       "                      [ 0.2592,  0.6516, -0.1749,  ..., -0.1900,  0.1041, -0.2360]])),\n",
       "             ('blocks.8.block.2.bias',\n",
       "              tensor([ 0.5713, -0.1693,  0.0712,  ...,  0.2824,  0.0268, -0.2016])),\n",
       "             ('blocks.9.block.0.weight',\n",
       "              tensor([[-0.5425,  0.1751, -0.2378,  ...,  0.0765, -0.1405, -0.2187],\n",
       "                      [-0.2306,  0.2220,  0.1767,  ..., -0.2063, -0.1993,  0.0022],\n",
       "                      [-0.2418,  0.1072, -0.3016,  ..., -0.0032, -0.3117, -0.2576],\n",
       "                      ...,\n",
       "                      [-0.3651,  0.3093,  0.0802,  ..., -0.0606, -0.0414, -0.2058],\n",
       "                      [ 0.0301, -0.2685,  0.0059,  ...,  0.0085,  0.3921, -0.0192],\n",
       "                      [-0.0880,  0.3022, -0.1283,  ..., -0.0892, -0.0802, -0.0203]])),\n",
       "             ('blocks.9.block.0.bias',\n",
       "              tensor([-0.1192, -0.1317, -0.1080,  ..., -0.1081, -0.0553, -0.1103])),\n",
       "             ('blocks.9.block.2.weight',\n",
       "              tensor([[-0.1726,  0.2893,  0.0308,  ...,  0.1916,  0.0698, -0.2644],\n",
       "                      [ 0.1850,  0.0687, -0.2132,  ..., -0.2017,  0.2730,  0.0763],\n",
       "                      [ 0.0414, -0.2897,  0.2288,  ..., -0.1587,  0.1026, -0.2323],\n",
       "                      ...,\n",
       "                      [-0.1736,  0.0323,  0.1810,  ...,  0.0945,  0.0265, -0.0488],\n",
       "                      [ 0.0153,  0.0988,  0.3763,  ..., -0.1045, -0.2794, -0.3198],\n",
       "                      [-0.0583,  0.2107,  0.2629,  ...,  0.2707,  0.3788, -0.2189]])),\n",
       "             ('blocks.9.block.2.bias',\n",
       "              tensor([ 0.3618, -0.1239,  0.0048,  ...,  0.4909, -0.3405, -0.4205])),\n",
       "             ('blocks.10.block.0.weight',\n",
       "              tensor([[-0.1233, -0.2597,  0.0288,  ...,  0.1307, -0.2059,  0.3234],\n",
       "                      [-0.0617, -0.2118,  0.2473,  ..., -0.1096, -0.1086, -0.1120],\n",
       "                      [-0.2510, -0.0718, -0.0712,  ..., -0.0928, -0.3645, -0.0387],\n",
       "                      ...,\n",
       "                      [-0.0875,  0.0640,  0.0128,  ..., -0.1950, -0.0336,  0.0585],\n",
       "                      [-0.1463, -0.1178,  0.5007,  ..., -0.0929, -0.0514,  0.2747],\n",
       "                      [-0.4385, -0.1008, -0.1076,  ...,  0.0260,  0.2055,  0.1234]])),\n",
       "             ('blocks.10.block.0.bias',\n",
       "              tensor([-0.1673, -0.1228, -0.1165,  ..., -0.1144, -0.0545, -0.1369])),\n",
       "             ('blocks.10.block.2.weight',\n",
       "              tensor([[-0.0717, -0.2212,  0.0081,  ...,  0.0041, -0.0005,  0.4451],\n",
       "                      [-0.1521, -0.0926,  0.2203,  ...,  0.2733, -0.0137,  0.3186],\n",
       "                      [ 0.0039,  0.1535,  0.0667,  ..., -0.3972, -0.1119, -0.1683],\n",
       "                      ...,\n",
       "                      [-0.1340,  0.0572, -0.4591,  ..., -0.0466, -0.1207,  0.2313],\n",
       "                      [-0.0858, -0.2482,  0.0189,  ...,  0.2356, -0.1347, -0.0653],\n",
       "                      [-0.0516, -0.1256,  0.1671,  ..., -0.0986, -0.1176, -0.2960]])),\n",
       "             ('blocks.10.block.2.bias',\n",
       "              tensor([-0.1877, -0.1204, -0.1332,  ...,  0.2850,  0.2426, -0.0808])),\n",
       "             ('blocks.11.block.0.weight',\n",
       "              tensor([[ 3.5003e-01,  3.6754e-01, -7.7130e-02,  ..., -1.8867e-01,\n",
       "                        2.6944e-01,  1.0887e-02],\n",
       "                      [ 1.1667e-01,  1.1375e-03,  2.1168e-01,  ...,  6.9825e-02,\n",
       "                       -1.3389e-01, -3.3588e-02],\n",
       "                      [ 3.3213e-01,  3.9313e-01,  6.6455e-02,  ..., -2.0286e-01,\n",
       "                        1.0478e-01,  1.8432e-01],\n",
       "                      ...,\n",
       "                      [ 2.0263e-01, -3.5224e-02, -1.5932e-02,  ..., -2.2173e-04,\n",
       "                       -1.8344e-01, -7.6958e-02],\n",
       "                      [ 1.0693e-01, -2.8960e-01, -3.7749e-01,  ...,  1.3415e-01,\n",
       "                        1.9631e-01, -4.4281e-02],\n",
       "                      [ 1.9959e-02,  1.9971e-01,  1.8964e-01,  ...,  1.3936e-01,\n",
       "                        2.4298e-02,  1.3576e-01]])),\n",
       "             ('blocks.11.block.0.bias',\n",
       "              tensor([-0.3502, -0.0319, -1.0218,  ..., -0.0103,  0.0105, -0.0739])),\n",
       "             ('blocks.11.block.2.weight',\n",
       "              tensor([[ 0.3915,  0.2013, -0.1203,  ..., -0.1612, -0.0686,  0.0068],\n",
       "                      [ 0.0012, -0.1608, -0.3432,  ..., -0.1820,  0.0969,  0.0830],\n",
       "                      [ 0.0878,  0.0494, -0.0787,  ...,  0.1439,  0.1657, -0.1213],\n",
       "                      ...,\n",
       "                      [ 0.0304,  0.1077,  0.0009,  ...,  0.1248, -0.0658, -0.0945],\n",
       "                      [ 0.0522,  0.1358,  0.0435,  ...,  0.1772, -0.0563, -0.2030],\n",
       "                      [ 0.1161, -0.1182, -0.2378,  ..., -0.1696, -0.0835, -0.0354]])),\n",
       "             ('blocks.11.block.2.bias',\n",
       "              tensor([ 0.0029,  0.1024,  0.1288,  ...,  0.1321,  0.1058, -0.0690])),\n",
       "             ('layernorms.0.weight',\n",
       "              tensor([0.3415, 0.2218, 0.2054,  ..., 0.2332, 0.1922, 0.1637])),\n",
       "             ('layernorms.0.bias',\n",
       "              tensor([-0.0899, -0.0229,  0.0244,  ..., -0.0250, -0.0064,  0.0191])),\n",
       "             ('layernorms.1.weight',\n",
       "              tensor([0.4885, 0.5380, 0.5001,  ..., 0.5845, 0.7193, 0.5801])),\n",
       "             ('layernorms.1.bias',\n",
       "              tensor([-0.4326, -0.2948,  0.2503,  ..., -0.1657, -0.4991,  0.1697])),\n",
       "             ('layernorms.2.weight',\n",
       "              tensor([0.8990, 0.8761, 0.8693,  ..., 0.9283, 0.9138, 0.8926])),\n",
       "             ('layernorms.2.bias',\n",
       "              tensor([-0.0973, -0.0819,  0.2564,  ...,  0.1301, -0.1672,  0.0858])),\n",
       "             ('layernorms.3.weight',\n",
       "              tensor([0.8480, 0.8139, 0.8696,  ..., 0.8702, 0.8548, 0.8963])),\n",
       "             ('layernorms.3.bias',\n",
       "              tensor([-0.2614, -0.3607,  0.2025,  ..., -0.1502, -0.1305,  0.0122])),\n",
       "             ('layernorms.4.weight',\n",
       "              tensor([0.7458, 0.9010, 0.8847,  ..., 0.8662, 0.8987, 0.9473])),\n",
       "             ('layernorms.4.bias',\n",
       "              tensor([-0.2302, -0.2075, -0.2028,  ..., -0.0187, -0.1943, -0.1459])),\n",
       "             ('layernorms.5.weight',\n",
       "              tensor([0.6819, 0.9202, 0.9453,  ..., 0.9146, 0.9167, 0.9624])),\n",
       "             ('layernorms.5.bias',\n",
       "              tensor([-0.2199, -0.1281, -0.2263,  ..., -0.0759, -0.1521, -0.0497])),\n",
       "             ('layernorms.6.weight',\n",
       "              tensor([0.7873, 0.9913, 0.9610,  ..., 1.0099, 1.0778, 1.0862])),\n",
       "             ('layernorms.6.bias',\n",
       "              tensor([-0.0583,  0.0275, -0.2849,  ...,  0.0130,  0.0561,  0.1317])),\n",
       "             ('layernorms.7.weight',\n",
       "              tensor([0.5964, 0.8752, 0.8881,  ..., 0.8202, 0.7804, 0.9364])),\n",
       "             ('layernorms.7.bias',\n",
       "              tensor([ 0.3796, -0.0591,  0.0885,  ...,  0.0779,  0.2519,  0.0882])),\n",
       "             ('layernorms.8.weight',\n",
       "              tensor([0.3547, 0.8467, 0.7785,  ..., 0.7999, 0.6723, 0.9616])),\n",
       "             ('layernorms.8.bias',\n",
       "              tensor([0.6758, 0.1053, 0.2026,  ..., 0.1931, 0.4990, 0.1530])),\n",
       "             ('layernorms.9.weight',\n",
       "              tensor([0.5469, 0.7796, 0.8154,  ..., 0.7349, 0.7268, 0.7277])),\n",
       "             ('layernorms.9.bias',\n",
       "              tensor([ 0.3059,  0.0711,  0.0066,  ...,  0.0749,  0.1977, -0.1583])),\n",
       "             ('layernorms.10.weight',\n",
       "              tensor([0.7327, 0.9618, 0.9614,  ..., 0.9727, 0.6925, 0.9330])),\n",
       "             ('layernorms.10.bias',\n",
       "              tensor([0.1282, 0.1023, 0.0599,  ..., 0.0284, 0.4723, 0.0700])),\n",
       "             ('layernorms.11.weight',\n",
       "              tensor([0.5416, 0.5874, 0.5861,  ..., 0.6726, 0.5787, 0.5453])),\n",
       "             ('layernorms.11.bias',\n",
       "              tensor([-0.0812,  0.0151, -0.0369,  ..., -0.0657,  0.0787,  0.0246]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['linear_in.weight', 'linear_in.bias', 'linear_out.weight', 'linear_out.bias', 'blocks.0.block.0.weight', 'blocks.0.block.0.bias', 'blocks.0.block.2.weight', 'blocks.0.block.2.bias', 'blocks.1.block.0.weight', 'blocks.1.block.0.bias', 'blocks.1.block.2.weight', 'blocks.1.block.2.bias', 'blocks.2.block.0.weight', 'blocks.2.block.0.bias', 'blocks.2.block.2.weight', 'blocks.2.block.2.bias', 'blocks.3.block.0.weight', 'blocks.3.block.0.bias', 'blocks.3.block.2.weight', 'blocks.3.block.2.bias', 'blocks.4.block.0.weight', 'blocks.4.block.0.bias', 'blocks.4.block.2.weight', 'blocks.4.block.2.bias', 'blocks.5.block.0.weight', 'blocks.5.block.0.bias', 'blocks.5.block.2.weight', 'blocks.5.block.2.bias', 'blocks.6.block.0.weight', 'blocks.6.block.0.bias', 'blocks.6.block.2.weight', 'blocks.6.block.2.bias', 'blocks.7.block.0.weight', 'blocks.7.block.0.bias', 'blocks.7.block.2.weight', 'blocks.7.block.2.bias', 'blocks.8.block.0.weight', 'blocks.8.block.0.bias', 'blocks.8.block.2.weight', 'blocks.8.block.2.bias', 'blocks.9.block.0.weight', 'blocks.9.block.0.bias', 'blocks.9.block.2.weight', 'blocks.9.block.2.bias', 'blocks.10.block.0.weight', 'blocks.10.block.0.bias', 'blocks.10.block.2.weight', 'blocks.10.block.2.bias', 'blocks.11.block.0.weight', 'blocks.11.block.0.bias', 'blocks.11.block.2.weight', 'blocks.11.block.2.bias', 'layernorms.0.weight', 'layernorms.0.bias', 'layernorms.1.weight', 'layernorms.1.bias', 'layernorms.2.weight', 'layernorms.2.bias', 'layernorms.3.weight', 'layernorms.3.bias', 'layernorms.4.weight', 'layernorms.4.bias', 'layernorms.5.weight', 'layernorms.5.bias', 'layernorms.6.weight', 'layernorms.6.bias', 'layernorms.7.weight', 'layernorms.7.bias', 'layernorms.8.weight', 'layernorms.8.bias', 'layernorms.9.weight', 'layernorms.9.bias', 'layernorms.10.weight', 'layernorms.10.bias', 'layernorms.11.weight', 'layernorms.11.bias'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['module.linear_in.weight', 'module.linear_in.bias', 'module.linear_out.weight', 'module.linear_out.bias', 'module.blocks.0.block.0.weight', 'module.blocks.0.block.0.bias', 'module.blocks.0.block.2.weight', 'module.blocks.0.block.2.bias', 'module.blocks.1.block.0.weight', 'module.blocks.1.block.0.bias', 'module.blocks.1.block.2.weight', 'module.blocks.1.block.2.bias', 'module.blocks.2.block.0.weight', 'module.blocks.2.block.0.bias', 'module.blocks.2.block.2.weight', 'module.blocks.2.block.2.bias', 'module.blocks.3.block.0.weight', 'module.blocks.3.block.0.bias', 'module.blocks.3.block.2.weight', 'module.blocks.3.block.2.bias', 'module.blocks.4.block.0.weight', 'module.blocks.4.block.0.bias', 'module.blocks.4.block.2.weight', 'module.blocks.4.block.2.bias', 'module.blocks.5.block.0.weight', 'module.blocks.5.block.0.bias', 'module.blocks.5.block.2.weight', 'module.blocks.5.block.2.bias', 'module.blocks.6.block.0.weight', 'module.blocks.6.block.0.bias', 'module.blocks.6.block.2.weight', 'module.blocks.6.block.2.bias', 'module.blocks.7.block.0.weight', 'module.blocks.7.block.0.bias', 'module.blocks.7.block.2.weight', 'module.blocks.7.block.2.bias', 'module.blocks.8.block.0.weight', 'module.blocks.8.block.0.bias', 'module.blocks.8.block.2.weight', 'module.blocks.8.block.2.bias', 'module.blocks.9.block.0.weight', 'module.blocks.9.block.0.bias', 'module.blocks.9.block.2.weight', 'module.blocks.9.block.2.bias', 'module.blocks.10.block.0.weight', 'module.blocks.10.block.0.bias', 'module.blocks.10.block.2.weight', 'module.blocks.10.block.2.bias', 'module.blocks.11.block.0.weight', 'module.blocks.11.block.0.bias', 'module.blocks.11.block.2.weight', 'module.blocks.11.block.2.bias', 'module.layernorms.0.weight', 'module.layernorms.0.bias', 'module.layernorms.1.weight', 'module.layernorms.1.bias', 'module.layernorms.2.weight', 'module.layernorms.2.bias', 'module.layernorms.3.weight', 'module.layernorms.3.bias', 'module.layernorms.4.weight', 'module.layernorms.4.bias', 'module.layernorms.5.weight', 'module.layernorms.5.bias', 'module.layernorms.6.weight', 'module.layernorms.6.bias', 'module.layernorms.7.weight', 'module.layernorms.7.bias', 'module.layernorms.8.weight', 'module.layernorms.8.bias', 'module.layernorms.9.weight', 'module.layernorms.9.bias', 'module.layernorms.10.weight', 'module.layernorms.10.bias', 'module.layernorms.11.weight', 'module.layernorms.11.bias'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_Wrapper(\n",
       "  (model): BottleneckMLP(\n",
       "    (linear_in): Linear(in_features=12288, out_features=1024, bias=True)\n",
       "    (linear_out): Linear(in_features=1024, out_features=200, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x BottleneckBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorms): ModuleList(\n",
       "      (0-11): 12 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (resize): Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B_12-Wi_1024'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B_12-Wi_1024_res_64_in21k_tinyimagenet'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load_state output _IncompatibleKeys(missing_keys=['linear_in.weight', 'linear_in.bias', 'linear_out.weight', 'linear_out.bias', 'blocks.0.block.0.weight', 'blocks.0.block.0.bias', 'blocks.0.block.2.weight', 'blocks.0.block.2.bias', 'blocks.1.block.0.weight', 'blocks.1.block.0.bias', 'blocks.1.block.2.weight', 'blocks.1.block.2.bias', 'blocks.2.block.0.weight', 'blocks.2.block.0.bias', 'blocks.2.block.2.weight', 'blocks.2.block.2.bias', 'blocks.3.block.0.weight', 'blocks.3.block.0.bias', 'blocks.3.block.2.weight', 'blocks.3.block.2.bias', 'blocks.4.block.0.weight', 'blocks.4.block.0.bias', 'blocks.4.block.2.weight', 'blocks.4.block.2.bias', 'blocks.5.block.0.weight', 'blocks.5.block.0.bias', 'blocks.5.block.2.weight', 'blocks.5.block.2.bias', 'blocks.6.block.0.weight', 'blocks.6.block.0.bias', 'blocks.6.block.2.weight', 'blocks.6.block.2.bias', 'blocks.7.block.0.weight', 'blocks.7.block.0.bias', 'blocks.7.block.2.weight', 'blocks.7.block.2.bias', 'blocks.8.block.0.weight', 'blocks.8.block.0.bias', 'blocks.8.block.2.weight', 'blocks.8.block.2.bias', 'blocks.9.block.0.weight', 'blocks.9.block.0.bias', 'blocks.9.block.2.weight', 'blocks.9.block.2.bias', 'blocks.10.block.0.weight', 'blocks.10.block.0.bias', 'blocks.10.block.2.weight', 'blocks.10.block.2.bias', 'blocks.11.block.0.weight', 'blocks.11.block.0.bias', 'blocks.11.block.2.weight', 'blocks.11.block.2.bias', 'layernorms.0.weight', 'layernorms.0.bias', 'layernorms.1.weight', 'layernorms.1.bias', 'layernorms.2.weight', 'layernorms.2.bias', 'layernorms.3.weight', 'layernorms.3.bias', 'layernorms.4.weight', 'layernorms.4.bias', 'layernorms.5.weight', 'layernorms.5.bias', 'layernorms.6.weight', 'layernorms.6.bias', 'layernorms.7.weight', 'layernorms.7.bias', 'layernorms.8.weight', 'layernorms.8.bias', 'layernorms.9.weight', 'layernorms.9.bias', 'layernorms.10.weight', 'layernorms.10.bias', 'layernorms.11.weight', 'layernorms.11.bias'], unexpected_keys=['module.linear_in.weight', 'module.linear_in.bias', 'module.linear_out.weight', 'module.linear_out.bias', 'module.blocks.0.block.0.weight', 'module.blocks.0.block.0.bias', 'module.blocks.0.block.2.weight', 'module.blocks.0.block.2.bias', 'module.blocks.1.block.0.weight', 'module.blocks.1.block.0.bias', 'module.blocks.1.block.2.weight', 'module.blocks.1.block.2.bias', 'module.blocks.2.block.0.weight', 'module.blocks.2.block.0.bias', 'module.blocks.2.block.2.weight', 'module.blocks.2.block.2.bias', 'module.blocks.3.block.0.weight', 'module.blocks.3.block.0.bias', 'module.blocks.3.block.2.weight', 'module.blocks.3.block.2.bias', 'module.blocks.4.block.0.weight', 'module.blocks.4.block.0.bias', 'module.blocks.4.block.2.weight', 'module.blocks.4.block.2.bias', 'module.blocks.5.block.0.weight', 'module.blocks.5.block.0.bias', 'module.blocks.5.block.2.weight', 'module.blocks.5.block.2.bias', 'module.blocks.6.block.0.weight', 'module.blocks.6.block.0.bias', 'module.blocks.6.block.2.weight', 'module.blocks.6.block.2.bias', 'module.blocks.7.block.0.weight', 'module.blocks.7.block.0.bias', 'module.blocks.7.block.2.weight', 'module.blocks.7.block.2.bias', 'module.blocks.8.block.0.weight', 'module.blocks.8.block.0.bias', 'module.blocks.8.block.2.weight', 'module.blocks.8.block.2.bias', 'module.blocks.9.block.0.weight', 'module.blocks.9.block.0.bias', 'module.blocks.9.block.2.weight', 'module.blocks.9.block.2.bias', 'module.blocks.10.block.0.weight', 'module.blocks.10.block.0.bias', 'module.blocks.10.block.2.weight', 'module.blocks.10.block.2.bias', 'module.blocks.11.block.0.weight', 'module.blocks.11.block.0.bias', 'module.blocks.11.block.2.weight', 'module.blocks.11.block.2.bias', 'module.layernorms.0.weight', 'module.layernorms.0.bias', 'module.layernorms.1.weight', 'module.layernorms.1.bias', 'module.layernorms.2.weight', 'module.layernorms.2.bias', 'module.layernorms.3.weight', 'module.layernorms.3.bias', 'module.layernorms.4.weight', 'module.layernorms.4.bias', 'module.layernorms.5.weight', 'module.layernorms.5.bias', 'module.layernorms.6.weight', 'module.layernorms.6.bias', 'module.layernorms.7.weight', 'module.layernorms.7.bias', 'module.layernorms.8.weight', 'module.layernorms.8.bias', 'module.layernorms.9.weight', 'module.layernorms.9.bias', 'module.layernorms.10.weight', 'module.layernorms.10.bias', 'module.layernorms.11.weight', 'module.layernorms.11.bias'])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bypass\n",
      "Load_state output <All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP_Wrapper(\n",
       "  (model): BottleneckMLP(\n",
       "    (linear_in): Linear(in_features=12288, out_features=1024, bias=True)\n",
       "    (linear_out): Linear(in_features=1024, out_features=200, bias=True)\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x BottleneckBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorms): ModuleList(\n",
       "      (0-11): 12 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (resize): Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=warn)\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp = MLP_Wrapper(architecture, crop_resolution, num_classes, checkpoint)\n",
    "\n",
    "model_mlp = model_mlp.to(device)\n",
    "model_mlp.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cifar10_models.resnet import *\n",
    "model_resnet = resnet18()\n",
    "\n",
    "# Use the weights of the resnet18 model trained on CIFAR10\n",
    "weights = torch.load('state_dicts/resnet18.pt')\n",
    "model_resnet.load_state_dict(weights)\n",
    "\n",
    "model_resnet = model_resnet.to(device)\n",
    "model_resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(model_resnet.fc.weight.is_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "model_ViT_tmp = timm.create_model(\"vit_base_patch16_384\", pretrained=True)\n",
    "model_ViT_tmp.head = nn.Linear(model_ViT_tmp.head.in_features, 10) # CIFAR10\n",
    "checkpoint = torch.load(\"finetuned_models/ViT_CIFAR10.t7\", map_location=torch.device(\"cpu\"))\n",
    "model_ViT_tmp = torch.nn.DataParallel(model_ViT_tmp)\n",
    "model_ViT_tmp.load_state_dict(checkpoint['model'])\n",
    "print(\"model loaded\")\n",
    "# ViT uses 384x384 pixel image\n",
    "input_size = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_Wrapper(nn.Module):\n",
    "    def __init__(self, model, input_size):\n",
    "        super(ViT_Wrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.resize = transforms.Resize(input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resize(x)\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ViT = ViT_Wrapper(model_ViT_tmp, input_size)\n",
    "model_ViT.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, get_dataset_split_names\n",
    "import cv2\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "input_size = 64\n",
    "bs = 100\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "#batch_size = 100\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def collate_fn_test(batch):\n",
    "    return ((torch.stack([transform_test(Image.fromarray(cv2.cvtColor(np.array(x['image']),cv2.COLOR_GRAY2RGB))) if np.array(x['image']).ndim == 2 else transform_test(x['image']) for x in batch])), torch.tensor([x['label'] for x in batch]))\n",
    "    \n",
    "testset = load_dataset('Maysee/tiny-imagenet', split='valid')\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=0, collate_fn=collate_fn_test)\n",
    "\n",
    "#dataset = datasets.CIFAR10(root='./data',\n",
    "#                           train=False,\n",
    "#                           transform=transform,\n",
    "#                           download=True)\n",
    "\n",
    "# test_loader = DataLoader(dataset,\n",
    "#                                   batch_size=batch_size,\n",
    "#                                   shuffle=False,\n",
    "#                                   num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.insert(0, '..')\n",
    "import robustbench\n",
    "from robustbench.data import load_cifar10\n",
    "from robustbench.utils import load_model, clean_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data loaded]\n"
     ]
    }
   ],
   "source": [
    "print('[Data loaded]')\n",
    "\n",
    "acc_mlp_list = []\n",
    "acc_cnn_list = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    images, labels = batch\n",
    "    acc_mlp = clean_accuracy(model_mlp, images.to(device), labels.to(device))\n",
    "    acc_mlp_list.append(acc_mlp)\n",
    "    \n",
    "    acc_cnn = clean_accuracy(model_resnet, images.to(device), labels.to(device))\n",
    "    acc_cnn_list.append(acc_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP loaded]\n",
      "MLP Acc: 72.74 %\n",
      "[CNN loaded]\n",
      "CNN Acc: 0.60 %\n"
     ]
    }
   ],
   "source": [
    "acc_mlp = np.array(acc_mlp_list)\n",
    "acc = acc_mlp.mean()\n",
    "print('[MLP loaded]')\n",
    "print('MLP Acc: %2.2f %%'%(acc*100))\n",
    "\n",
    "acc_cnn = np.array(acc_cnn_list)\n",
    "acc = acc_cnn.mean()\n",
    "print('[CNN loaded]')\n",
    "print('CNN Acc: %2.2f %%'%(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_adv_attacks(model1, model2, atk_model1, images, labels, iterations=10, device='cpu'):\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    successful_adv_attacks_model1 = []\n",
    "    successful_adv_attacks_model2 = []\n",
    "\n",
    "    for it in range(iterations):\n",
    "        \n",
    "        adv_images = atk_model1(images, labels)\n",
    "        predictions = get_pred(model1, adv_images, device)\n",
    "        predictions = predictions.cuda()\n",
    "        \n",
    "        seccessful_adv_images = adv_images[predictions != labels]\n",
    "        true_labels_of_succ_adv_images = labels[predictions != labels]\n",
    "        successful_adv_attacks_model1.append(seccessful_adv_images.shape[0])\n",
    "\n",
    "\n",
    "        predictions = get_pred(model2, seccessful_adv_images, device)\n",
    "        predictions = predictions.cuda()\n",
    "        seccessful_adv_images = seccessful_adv_images[predictions != true_labels_of_succ_adv_images]\n",
    "        successful_adv_attacks_model2.append(seccessful_adv_images.shape[0])\n",
    "\n",
    "    return successful_adv_attacks_model1, successful_adv_attacks_model2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_exp = 20\n",
    "\n",
    "atk_mlp = PGD(model_mlp, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "# atk_mlp.set_normalization_used(mean=mean, std=std)\n",
    "\n",
    "\n",
    "l1_list_mlp = []\n",
    "l2_list_mlp = []\n",
    "# 100 batches\n",
    "for i,batch in enumerate(test_loader):\n",
    "    print(\"batch:\", i)\n",
    "    ll1_list = []\n",
    "    ll2_list = []\n",
    "    # N_exp times\n",
    "    for i in range(5):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        l1, l2 = compare_adv_attacks(model_mlp, model_resnet, atk_mlp, images, labels, 1, device)\n",
    "        ll1_list.append(l1)\n",
    "        ll2_list.append(l2)\n",
    "    \n",
    "    l1_list_mlp.append(ll1_list)\n",
    "    l2_list_mlp.append(ll2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.squeeze(mlp_adv,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_adv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on mlp Tested on resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_adv = np.array(l1_list_mlp)\n",
    "mlp_adv = np.squeeze(mlp_adv,2)\n",
    "mlp_adv = mlp_adv.sum(axis=0)\n",
    "\n",
    "mlp_adv_mean = mlp_adv.mean(axis=0)\n",
    "mlp_adv_std = mlp_adv.std(axis=0)\n",
    "print('mlp_adv_mean: ', mlp_adv_mean)\n",
    "print('mlp_adv_std: ', mlp_adv_std)\n",
    "\n",
    "cnn_adv = np.array(l2_list_mlp) \n",
    "cnn_adv = np.squeeze(cnn_adv,2)\n",
    "cnn_adv = cnn_adv.sum(axis=0)\n",
    "\n",
    "cnn_adv_mean = cnn_adv.mean(axis=0)\n",
    "cnn_adv_std = cnn_adv.std(axis=0)\n",
    "print('cnn_adv_mean: ', cnn_adv_mean)\n",
    "print('cnn_adv_std: ', cnn_adv_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ratio: \", (cnn_adv_mean/mlp_adv_mean)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_exp = 20\n",
    "\n",
    "atk_cnn = PGD(model_resnet, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "# atk_cnn.set_normalization_used(mean=mean, std=std)\n",
    "\n",
    "\n",
    "l1_list_cnn = []\n",
    "l2_list_cnn = []\n",
    "# 100 batches\n",
    "for i,batch in enumerate(test_loader):\n",
    "    print(\"batch:\", i)\n",
    "    ll1_list = []\n",
    "    ll2_list = []\n",
    "    # N_exp times\n",
    "    for i in range(5):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        l1, l2 = compare_adv_attacks(model_resnet, model_mlp, atk_cnn, images, labels, 1, device)\n",
    "        ll1_list.append(l1)\n",
    "        ll2_list.append(l2)\n",
    "    \n",
    "    l1_list_cnn.append(ll1_list)\n",
    "    l2_list_cnn.append(ll2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on resnet Tested on mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_adv = np.array(l1_list_cnn)\n",
    "cnn_adv = np.squeeze(cnn_adv,2)\n",
    "cnn_adv = cnn_adv.sum(axis=0)\n",
    "\n",
    "cnn_adv_mean = cnn_adv.mean(axis=0)\n",
    "cnn_adv_std = cnn_adv.std(axis=0)\n",
    "print('cnn_adv_mean: ', cnn_adv_mean)\n",
    "print('cnn_adv_std: ', cnn_adv_std)\n",
    "\n",
    "mlp_adv = np.array(l2_list_cnn) \n",
    "mlp_adv = np.squeeze(mlp_adv,2)\n",
    "mlp_adv = mlp_adv.sum(axis=0)\n",
    "\n",
    "mlp_adv_mean = mlp_adv.mean(axis=0)\n",
    "mlp_adv_std = mlp_adv.std(axis=0)\n",
    "print('mlp_adv_mean: ', mlp_adv_mean)\n",
    "print('mlp_adv_std: ', mlp_adv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"ratio: \", (mlp_adv_mean/cnn_adv_mean)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PGD: Inside normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((crop_resolution, crop_resolution)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "dataset = datasets.CIFAR10(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transform,\n",
    "                           download=True)\n",
    "\n",
    "test_loader = DataLoader(dataset,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_exp = 20\n",
    "\n",
    "atk_mlp = PGD(model_mlp, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "atk_mlp.set_normalization_used(mean=mean, std=std)\n",
    "\n",
    "\n",
    "l1_list_mlp = []\n",
    "l2_list_mlp = []\n",
    "# 100 batches\n",
    "for i,batch in enumerate(test_loader):\n",
    "    print(\"batch:\", i)\n",
    "    ll1_list = []\n",
    "    ll2_list = []\n",
    "    # N_exp times\n",
    "    for i in range(5):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        l1, l2 = compare_adv_attacks(model_mlp, model_resnet, atk_mlp, images, labels, 1, device)\n",
    "        ll1_list.append(l1)\n",
    "        ll2_list.append(l2)\n",
    "    \n",
    "    l1_list_mlp.append(ll1_list)\n",
    "    l2_list_mlp.append(ll2_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on cnn Tested on mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_adv = np.array(l1_list_mlp)\n",
    "mlp_adv = np.squeeze(mlp_adv,2)\n",
    "mlp_adv = mlp_adv.sum(axis=0)\n",
    "\n",
    "mlp_adv_mean = mlp_adv.mean(axis=0)\n",
    "mlp_adv_std = mlp_adv.std(axis=0)\n",
    "print('mlp_adv_mean: ', mlp_adv_mean)\n",
    "print('mlp_adv_std: ', mlp_adv_std)\n",
    "\n",
    "cnn_adv = np.array(l2_list_mlp) \n",
    "cnn_adv = np.squeeze(cnn_adv,2)\n",
    "cnn_adv = cnn_adv.sum(axis=0)\n",
    "\n",
    "cnn_adv_mean = cnn_adv.mean(axis=0)\n",
    "cnn_adv_std = cnn_adv.std(axis=0)\n",
    "print('cnn_adv_mean: ', cnn_adv_mean)\n",
    "print('cnn_adv_std: ', cnn_adv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ratio: \", (cnn_adv_mean/mlp_adv_mean)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_exp = 20\n",
    "\n",
    "atk_cnn = PGD(model_resnet, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "atk_cnn.set_normalization_used(mean=mean, std=std)\n",
    "\n",
    "\n",
    "l1_list_cnn = []\n",
    "l2_list_cnn = []\n",
    "# 100 batches\n",
    "for i,batch in enumerate(test_loader):\n",
    "    print(\"batch:\", i)\n",
    "    ll1_list = []\n",
    "    ll2_list = []\n",
    "    # N_exp times\n",
    "    for i in range(5):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        l1, l2 = compare_adv_attacks(model_resnet, model_mlp, atk_cnn, images, labels, 1, device)\n",
    "        ll1_list.append(l1)\n",
    "        ll2_list.append(l2)\n",
    "    \n",
    "    l1_list_cnn.append(ll1_list)\n",
    "    l2_list_cnn.append(ll2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained on cnn Tested on mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_adv = np.array(l1_list_cnn)\n",
    "cnn_adv = np.squeeze(cnn_adv,2)\n",
    "cnn_adv = cnn_adv.sum(axis=0)\n",
    "\n",
    "cnn_adv_mean = cnn_adv.mean(axis=0)\n",
    "cnn_adv_std = cnn_adv.std(axis=0)\n",
    "print('cnn_adv_mean: ', cnn_adv_mean)\n",
    "print('cnn_adv_std: ', cnn_adv_std)\n",
    "\n",
    "mlp_adv = np.array(l2_list_cnn) \n",
    "mlp_adv = np.squeeze(mlp_adv,2)\n",
    "mlp_adv = mlp_adv.sum(axis=0)\n",
    "\n",
    "mlp_adv_mean = mlp_adv.mean(axis=0)\n",
    "mlp_adv_std = mlp_adv.std(axis=0)\n",
    "print('mlp_adv_mean: ', mlp_adv_mean)\n",
    "print('mlp_adv_std: ', mlp_adv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ratio: \", (mlp_adv_mean/cnn_adv_mean)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = model_mlp\n",
    "model2 = model_resnet\n",
    "atk_model1 = atk\n",
    "iterations = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.eval()\n",
    "model2.eval()\n",
    "\n",
    "successful_adv_attacks_model1 = []\n",
    "successful_adv_attacks_model2 = []\n",
    "\n",
    "for it in range(iterations):\n",
    "    print(\"Iteration:\", it)\n",
    "    adv_images = atk_model1(images, labels)\n",
    "    predictions = get_pred(model1, adv_images, device)\n",
    "\n",
    "    seccessful_adv_images = adv_images[predictions.cuda() != labels]\n",
    "    true_labels_of_succ_adv_images = labels[predictions != labels]\n",
    "    successful_adv_attacks_model1.append(seccessful_adv_images.shape[0])\n",
    "\n",
    "    predictions = get_pred(model2, seccessful_adv_images, device)\n",
    "    seccessful_adv_images = seccessful_adv_images[predictions != true_labels_of_succ_adv_images]\n",
    "    successful_adv_attacks_model2.append(seccessful_adv_images.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Assuming a single-GPU setup\n",
    "    device_properties = torch.cuda.get_device_properties(device)\n",
    "    \n",
    "    total_memory = device_properties.total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    cached_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "    print(f\"Total GPU Memory: {total_memory / 1e9} GB\")\n",
    "    print(f\"Allocated Memory: {allocated_memory / 1e9} GB\")\n",
    "    print(f\"Cached Memory: {cached_memory / 1e9} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_list = []\n",
    "l2_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "atk = PGD(model_mlp, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "atk.set_normalization_used(mean=mean, std=std)\n",
    "l1, l2 = compare_adv_attcks(model_mlp, model_resnet, atk, images, labels, N_exp, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk = PGD(model_resnet, eps=8/255, alpha=2/225, steps=10, random_start=True)\n",
    "atk.set_normalization_used(mean=mean, std=std)\n",
    "l11, l22 = compare_adv_attcks(model_resnet, model_mlp, atk, images, labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(l11, l22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pre = get_pred(model_resnet, adv_images[idx:idx+1], device)\n",
    "imshow(adv_images[idx:idx+1], title=\"True:%d, Pre:%d\"%(labels[idx], pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "pre = get_pred(model, adv_images1[idx:idx+1].flatten(1), device)\n",
    "imshow(adv_images1[idx:idx+1], title=\"True:%d, Pre:%d\"%(labels[idx], pre))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ceb8aea646a0c712ed5db194d127de24ece80f87032283552cbe7de982c3798"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
