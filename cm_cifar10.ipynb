{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.datasets import ImageNet\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1109d82b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USE CNN that is pretrained on cifar10\n",
    "from cifar10_models.resnet import *\n",
    "\n",
    "model = resnet18()\n",
    "weights = torch.load('state_dicts/resnet18.pt')\n",
    "model.load_state_dict(weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check labels of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((crop_resolution, crop_resolution)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "cifar10_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "loader = DataLoader(cifar10_dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "\n",
    "cifar10_classes = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Classmaximization Method (with gaussian blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassMaxim(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassMaxim, self).__init__()\n",
    "        self.x_c = nn.parameter.Parameter(torch.randn(1, 3, 32, 32))  \n",
    "        self.clone = self.x_c.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import gaussian_blur\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsmax = ClassMaxim()\n",
    "clss = 9 # billard class\n",
    "\n",
    "# hyperparameters\n",
    "lamb = 0.1\n",
    "lr = 4\n",
    "sigma = 0.5\n",
    "kern_size = 3\n",
    "blur_frequency = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import gaussian_blur\n",
    "def gaussian_blur_channelwise(input_tensor, kernel_size, sigma):\n",
    "    channels = 3\n",
    "    blurred_channels = []\n",
    "    for c in range(channels):\n",
    "        blurred_channel = gaussian_blur(input_tensor[:, c:c+1, :, :], kernel_size=kernel_size, sigma=sigma)\n",
    "        blurred_channels.append(blurred_channel)\n",
    "    return torch.cat(blurred_channels, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_tens = torch.tensor(std).reshape(-1,1,1)\n",
    "mean_tens = torch.tensor(mean).reshape(-1,1,1)\n",
    "\n",
    "plt.figure()\n",
    "for i in range(40):\n",
    "    # lr-=2\n",
    "    # if lr <= 8:\n",
    "    #     lr = 8\n",
    "    plt.imshow((clsmax.x_c[0]*std_tens + mean_tens).detach().numpy().transpose(1,2,0))\n",
    "    plt.show()\n",
    "    for step in range(20):\n",
    "        xx = clsmax.x_c[0]\n",
    "        preds = model(xx.unsqueeze(0)) # cnn 1 x 3 x 32 x 32, expected dim\n",
    "        preds = preds.squeeze(0)\n",
    "\n",
    "        loss = (preds[clss] - lamb*(torch.norm(clsmax.x_c[0],p=2)))\n",
    "        (loss).backward()\n",
    "        \n",
    "        grad = clsmax.x_c.grad #/ (torch.norm(clsmax.x_c.grad)+ 1e-5) # l2 norm, grads\n",
    "        clsmax.x_c = nn.Parameter(clsmax.x_c + grad*lr) # update step, gradient ascent\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # normalize between 0 and 1\n",
    "            min_value = torch.min(clsmax.x_c)\n",
    "            max_value = torch.max(clsmax.x_c)\n",
    "            normalized_tensor = (clsmax.x_c - min_value) / (max_value - min_value)\n",
    "            blurred_tensor = normalized_tensor\n",
    "\n",
    "            # Gaussian blur\n",
    "            if step % blur_frequency == 0:\n",
    "                blurred_tensor = gaussian_blur_channelwise(normalized_tensor, kernel_size=kern_size, sigma=sigma)\n",
    "            \n",
    "            # clipping pixels with small norm\n",
    "            clipped_tensor = blurred_tensor\n",
    "            norms = torch.norm(blurred_tensor, dim=1, p=2)\n",
    "            below_threshold = norms < 0.001\n",
    "            blurred_tensor[:, :, below_threshold.squeeze(0)] = 0\n",
    "            clsmax.x_c = nn.Parameter(clipped_tensor  * (max_value - min_value) + min_value)\n",
    "    \n",
    "    if(i < 39):\n",
    "        clear_output(wait=True)\n",
    "        display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import gaussian_blur\n",
    "def gaussian_blur_channelwise(input_tensor, kernel_size, sigma):\n",
    "    channels = 3\n",
    "    blurred_channels = []\n",
    "    for c in range(channels):\n",
    "        blurred_channel = gaussian_blur(input_tensor[:, c:c+1, :, :], kernel_size=kernel_size, sigma=sigma)\n",
    "        blurred_channels.append(blurred_channel)\n",
    "    return torch.cat(blurred_channels, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean, std)\n",
    "\n",
    "def clamp_to_img(x):\n",
    "  return torch.clamp(x, min=0., max=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_maxim(model, label, lamb, alpha, iters, device=\"cpu\"):\n",
    "  sigma = 1.5\n",
    "  kernel_size=5\n",
    "  image = torch.randn(1, 3, 32, 32).to(device)\n",
    "  r = torch.rand_like(image).to(device)\n",
    "  r.requires_grad_()\n",
    "\n",
    "  for i in range(iters):\n",
    "      preds = model(normalize(r)) #clamp_to_img(r)\n",
    "      loss = preds[0,label] - lamb*(torch.norm(r,p=2))\n",
    "      (-loss).backward()\n",
    "      r.data -=  alpha * r.grad.data\n",
    "      \n",
    "      if i % 4 == 0:\n",
    "        with torch.no_grad():\n",
    "          r.data = gaussian_blur_channelwise(r, kernel_size, sigma)\n",
    "          sigma*=0.99\n",
    "          if sigma < 1.1:\n",
    "             sigma = 1.1\n",
    "          norms = torch.norm(r, dim=1, p=2)\n",
    "          below_threshold = norms < 0.0001\n",
    "          r.data[:, :, below_threshold.squeeze(0)] = 0\n",
    "        if i%20==0:\n",
    "          output_image = r.detach()\n",
    "          image_np = clamp_to_img(r.detach())[0].permute(1, 2, 0).cpu().numpy()\n",
    "          clear_output(wait=True)\n",
    "          display(plt.gcf())\n",
    "          plt.imshow(image_np)\n",
    "          plt.axis('off')\n",
    "          plt.show()\n",
    "\n",
    "\n",
    "  return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1177,  0.1299,  0.0008,  ...,  0.1568,  0.3667,  0.2051],\n",
       "          [ 0.2770, -0.0889, -0.0105,  ..., -0.0885,  0.2466,  0.3288],\n",
       "          [-0.0186, -0.0263, -0.0411,  ...,  0.6488,  0.1637,  0.1765],\n",
       "          ...,\n",
       "          [ 0.4125,  0.1455,  0.2086,  ...,  0.3890,  0.3295,  0.4336],\n",
       "          [ 0.3596,  0.4018, -0.0156,  ...,  0.3476,  0.3304,  0.4578],\n",
       "          [ 0.3432,  0.4103,  0.3088,  ...,  0.3172,  0.4260,  0.5393]],\n",
       "\n",
       "         [[-0.0147,  0.0642,  0.0235,  ..., -0.0519,  0.2859,  0.0369],\n",
       "          [ 0.1799, -0.0679,  0.0049,  ..., -0.3988, -0.0060,  0.1022],\n",
       "          [ 0.0227,  0.1336,  0.0537,  ...,  0.5635,  0.0423,  0.0413],\n",
       "          ...,\n",
       "          [ 0.4294,  0.2275,  0.2152,  ...,  0.3215,  0.1711,  0.3914],\n",
       "          [ 0.3465,  0.4953,  0.0783,  ...,  0.1377,  0.2134,  0.4176],\n",
       "          [ 0.3552,  0.4344,  0.3232,  ...,  0.2902,  0.3621,  0.4444]],\n",
       "\n",
       "         [[-0.1386, -0.0312, -0.0589,  ...,  0.0995,  0.4124,  0.1304],\n",
       "          [ 0.0046, -0.1556, -0.0777,  ..., -0.3633,  0.0905,  0.1794],\n",
       "          [-0.0361,  0.0993, -0.0792,  ...,  0.3301,  0.0568,  0.0928],\n",
       "          ...,\n",
       "          [ 0.1359, -0.0199,  0.0470,  ...,  0.2771,  0.1541,  0.2355],\n",
       "          [ 0.0472,  0.1743, -0.0515,  ..., -0.0016,  0.1672,  0.2595],\n",
       "          [ 0.0664,  0.1397,  0.1033,  ...,  0.1671,  0.2480,  0.3238]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_maxim(model, 1, lamb=0.0007, alpha=0.005, iters=1000, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
